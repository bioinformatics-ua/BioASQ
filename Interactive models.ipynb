{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import yaml_loader, load_model_weights\n",
    "from os.path import join\n",
    "# load config\n",
    "config = yaml_loader(\"config/attention_sw_split_6b.yaml\")\n",
    "\n",
    "cache_folder_name = config[\"cache_folder\"]\n",
    "prefix_name = config[\"corpora\"][\"name\"]\n",
    "# get deeprank config\n",
    "deeprank_config = config[\"pipeline\"][1][\"DeepRankDotSplit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD FROM CACHE] Load tokenizer from /backup/IR/cache/pubmed_2018_Regex3.json\n",
      "DEBUG created tokenizer pubmed_2018_Regex3\n",
      "True True\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/IR/cache/embedding_BioWordVec_PubMed_MIMICIII_d200_pubmed_2018_Regex3\n"
     ]
    }
   ],
   "source": [
    "from models.DeepRankDotSplit import DeepRankDotSplit\n",
    "\n",
    "deeprank = DeepRankDotSplit(cache_folder=cache_folder_name,\n",
    "                    prefix_name=prefix_name,\n",
    "                    **deeprank_config)\n",
    "\n",
    "deeprank.split_token = deeprank.tokenizer.texts_to_sequences([\".\"])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([Dimension(None), Dimension(13), Dimension(200)]), TensorShape([Dimension(None), Dimension(13), Dimension(58)])]\n"
     ]
    }
   ],
   "source": [
    "deeprank.build()\n",
    "deeprank.build_network(**deeprank_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "weight_file_name = \"0_last_weights_DeepRank_bioasq_Regex_embedding_BioWordVec_PubMed_MIMICIII_d200_bioasq_Regex_selu_False_58_100_3_3_256_adadelta_2_120_0.0001_5_2_3_selu_15_13_5_.h5\"\n",
    "load_model_weights(join(cache_folder_name,\"deeprank_xvalidation_weights\",weight_file_name), deeprank.deeprank_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "with open(\"validation_fold.p\",\"rb\") as f:\n",
    "    validation = pickle.load(f)\n",
    "    \n",
    "with open(\"/backup/IR/cache/full_data_validation_BM25_with_bioasq_stem_Bllip_2500_retrieved_results.p\",\"rb\") as f:\n",
    "    bm25_top = pickle.load(f)\n",
    "\n",
    "with open(\"/backup/BioASQ-training7b/full_data.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "    queries = {x[\"query_id\"]:{x[\"query\"]} for x in dataset}\n",
    "    gold_standard = {x[\"query_id\"]:{x[\"documents\"]} for x in dataset}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_keys = validation[0]\n",
    "\n",
    "k_fold_validation_data = {key: {\"documents\": training_data[\"train\"][key][\"positive_ids\"]+training_data[\"train\"][key][\"partially_positive_ids\"],\n",
    "                                \"query\": queries[key]} for key in validation_keys}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics on the full validation set\n",
      "544\n",
      "544\n",
      "2747\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-77fe22255e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold_validation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_standard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_standard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "k_fold_validation_data = {key : bm25_top[\"train\"][key] for key in validation_keys}\n",
    "validation_scores = deeprank.inference(data_to_infer=k_fold_validation_data, train=False, **deeprank_config)[\"retrieved\"]\n",
    "print(\"Metrics on the full validation set\")\n",
    "print(len(validation_scores))\n",
    "print(len(k_fold_validation_data))\n",
    "print(len(gold_standard))\n",
    "deeprank.show_evaluation(validation_scores, gold_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "query = \"sequences enhancers\"\n",
    "abstract = \"The mechanism of specific expression of glutathione transferase P gene during hepatocarcinogenesis of the rat has been investigated by cloning the gene and determining the upstream regulatory sequences. Two enhancers and a silencer are located within 3 kb upstream of the promoter. The stronger enhancer designated GPEI has two TPA (12-O-tetradecanoyl phorbol 13-acetate)-response element (TRE)-like sequences arranged in a palindrome at a 3 base pairs spacing. This special combination was found to form a very strong enhancer which could act efficiently even in F9 cells where the collagenase enhancer with a singlet TRE cannot work due to the low c-jun content. Whether this structure is operating with a very low concentration of c-jun/c-fos heterodimer or with any other proteins remains to be determined. These findings suggest that new and more efficient enhancers evolve by a combination of basic enhancer elements. The silencer region consists of several sequences that can bind specific protein(s) and works cooperatively.\"\n",
    "\n",
    "\n",
    "data = {0:{\"query\": query, \"documents\":[{\"id\":1,\"original\":abstract, \"title\":\"\"}]}}\n",
    "gen = deeprank.inference_generator(data,train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 13, 5, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = X[0][1][0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'silencer region consists sequences bind specific protein s works cooperatively'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeprank.tokenizer.sequences_to_texts([p])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tokenized = deeprank.tokenizer.tokenize_query(query)\n",
    "a_tokenized = deeprank.tokenizer.tokenize_article(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340 2\n",
      "282 2\n",
      "3755 2\n",
      "107 2\n",
      "1897 2\n",
      "3735 2\n",
      "1083 2\n",
      "823 2\n",
      "2 2\n",
      "5771 2\n",
      "374 2\n",
      "2287 2\n",
      "115 2\n",
      "1920 2\n",
      "20644 2\n",
      "386 2\n",
      "823 2\n",
      "7928 2\n",
      "39672 2\n",
      "35 2\n",
      "1224 2\n",
      "2153 2\n",
      "9511 2\n",
      "1366 2\n",
      "5362 2\n",
      "1328 2\n",
      "2 2\n",
      "29395 2\n",
      "390 2\n",
      "2922 2\n",
      "823 2\n",
      "2267 2\n",
      "117 2\n",
      "64 2\n",
      "188 2\n",
      "7079 2\n",
      "18288 2\n",
      "107 2\n",
      "1897 2\n",
      "3735 2\n",
      "1083 2\n",
      "823 2\n",
      "2 2\n",
      "44 2\n",
      "11634 2\n",
      "29395 2\n",
      "1236 2\n",
      "35 2\n",
      "3082 2\n",
      "3735 2\n",
      "1114 2\n",
      "302 2\n",
      "2 2\n",
      "192 2\n",
      "195 2\n",
      "133 2\n",
      "62 2\n",
      "1151 2\n",
      "11634 2\n",
      "8992 2\n",
      "514 2\n",
      "1366 2\n",
      "5362 2\n",
      "1328 2\n",
      "2 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[  340,   282,  3755,   107,  1897,  3735,  1083,   823,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [ 5771,   374,  2287,   115,  1920, 20644,   386,   823,  7928,\n",
       "          39672,    35,  1224,  2153,  9511,     0],\n",
       "         [29395,   390,  2922,   823,  2267,   117,    64,   188,  7079,\n",
       "          18288,     0,     0,     0,     0,     0],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0]], dtype=int32),\n",
       "  array([[   44, 11634, 29395,  1236,    35,  3082,  3735,  1114,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  192,   195,   133,    62,  1151, 11634,  8992,   514,  1366,\n",
       "           5362,  1328,     0,     0,     0,     0],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0]], dtype=int32)],\n",
       " [array([ 15,  42, 103,  -1,  -1], dtype=int32),\n",
       "  array([18, 93, -1, -1, -1], dtype=int32)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np \n",
    "\n",
    "def test_split(self, tokenized_query, tokenized_article, Q, P, S):\n",
    "\n",
    "    snippets = []\n",
    "    snippets_position = []\n",
    "\n",
    "    half_size = S//2\n",
    "\n",
    "    # O(n^2) complexity, probably can do better with better data struct TODO see if is worthit\n",
    "    for query_token in tokenized_query:\n",
    "        snippets_per_token = []\n",
    "        snippets_per_token_position = []\n",
    "        if query_token != 0:  # jump padded token\n",
    "            for i, article_token in enumerate(tokenized_article):\n",
    "                if article_token == query_token:\n",
    "\n",
    "                    lower_index = i-half_size\n",
    "                    lower_index = max(0, lower_index)\n",
    "\n",
    "                    higher_index = i+half_size\n",
    "                    higher_index = min(len(tokenized_article), higher_index)\n",
    "\n",
    "                    sentence = []\n",
    "\n",
    "                    for _i in range(lower_index, higher_index):\n",
    "                        token = tokenized_article[_i]\n",
    "                        print(token, self.split_token)\n",
    "                        if token == self.split_token:\n",
    "                            if _i < i:\n",
    "                                sentence = []\n",
    "                                continue\n",
    "                            else:\n",
    "                                break\n",
    "\n",
    "                        sentence.append(token)\n",
    "\n",
    "                    snippets_per_token.append(sentence)\n",
    "                    snippets_per_token_position.append(i)\n",
    "\n",
    "        if len(snippets_per_token) == 0:  # zero pad\n",
    "            snippets.append(np.zeros((P, S), dtype=np.int32))\n",
    "            snippets_position.append(np.zeros((P), dtype=np.int32) + self.SNIPPET_POSITION_PADDING_VALUE)\n",
    "            continue\n",
    "\n",
    "        max_snippets_len = min(P, len(snippets_per_token))\n",
    "\n",
    "        # snippets in matrix format\n",
    "        # pad\n",
    "        snippets_per_token = pad_sequences(snippets_per_token, maxlen=S, padding=\"post\")\n",
    "        # fill the gaps\n",
    "        _temp = np.zeros((P, S), dtype=np.int32)\n",
    "        _temp[:max_snippets_len] = snippets_per_token[:max_snippets_len]\n",
    "        snippets.append(_temp)\n",
    "\n",
    "        # snippets_position in matrix format\n",
    "        # pad\n",
    "        snippets_per_token_position = pad_sequences([snippets_per_token_position], maxlen=P, padding=\"post\", value=self.SNIPPET_POSITION_PADDING_VALUE)[0]\n",
    "        snippets_position.append(snippets_per_token_position)\n",
    "\n",
    "    return snippets, snippets_position\n",
    "\n",
    "deeprank.split_token = deeprank.tokenizer.texts_to_sequences([\".\"])[0][0]\n",
    "\n",
    "test_split(deeprank,q_tokenized, a_tokenized, 13,5,15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
