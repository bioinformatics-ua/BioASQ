{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pubmed_data.pubmed_helper as ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open /backup/pubmed_archive_json/pubmed_ready.tar.gz\n",
      "Creating generator\n"
     ]
    }
   ],
   "source": [
    "#Create a generator for pubmed data\n",
    "ONLY_TITLE = False\n",
    "\n",
    "if ONLY_TITLE:\n",
    "    map_function = lambda x:x[\"title\"]\n",
    "else:\n",
    "    map_function = lambda x:x[\"title\"] + \" \" + x[\"abstract\"] \n",
    "\n",
    "pubmed_generator = ph.create_pubmed_collection_generator(map_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_TOKENIZER = True\n",
    "FULL_TOKENS = False\n",
    "MODE = \"regex_less_700k_freq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load regex_less_700k_freq_tokenizer.p\n",
      "Num words: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if LOAD_TOKENIZER:\n",
    "            \n",
    "    if MODE == \"bag_of_trigrams\":\n",
    "        from keras_new_text import regex_alfanum_tokenizer\n",
    "        tk = ph.load_tokenizer(\"hashtrick_full_tokens\")\n",
    "        \n",
    "    else:\n",
    "        tk = ph.load_tokenizer(mode = MODE)\n",
    "        if FULL_TOKENS:\n",
    "            tk.num_words=None\n",
    "    \n",
    "    print(\"Num words:\",tk.num_words)\n",
    "else:\n",
    "    if MODE == \"bllip\":\n",
    "        from keras_new_text import Tokenizer, bllip_tokenizer\n",
    "        create_tokenizer = lambda : Tokenizer(tokenizer = bllip_tokenizer)\n",
    "\n",
    "    elif MODE == \"bllip_stem\":\n",
    "        from keras_new_text import Tokenizer,  bllip_stopW_stem_tokenizer\n",
    "        create_tokenizer = lambda : Tokenizer(tokenizer = bllip_stopW_stem_tokenizer)\n",
    "        \n",
    "    elif MODE == \"keras\":\n",
    "        from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "        create_tokenizer = lambda : Tokenizer()\n",
    "    elif MODE == \"regex_full_tokens\":\n",
    "        from keras_new_text import Tokenizer, regex_alfanum_tokenizer\n",
    "        create_tokenizer = lambda : Tokenizer(tokenizer = regex_alfanum_tokenizer)\n",
    "    elif MODE == \"hashtrick_full_tokens\":\n",
    "        base_tk = ph.load_tokenizer(\"regex_full_tokens\")\n",
    "        ##save some memory\n",
    "        del base_tk.word_index\n",
    "        del base_tk.word_counts\n",
    "        del base_tk.word_docs\n",
    "        del base_tk.index_docs\n",
    "        \n",
    "        pubmed_generator = ph.create_tokenized_pubmed_collection_generator(mode=\"regex_full_tokens\")\n",
    "        from keras_new_text import Tokenizer ,trigram_tokenizer, tri_gram\n",
    "        create_tokenizer = lambda : Tokenizer(tokenizer = trigram_tokenizer)\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi process functions\n",
    "if not LOAD_TOKENIZER:\n",
    "    \n",
    "    N = 0\n",
    "    output_tokenizer_file_name = MODE+'_file_{0:03}_tokenizer.p'\n",
    "    output_merged_tokenizer_file_name = MODE+(\"_N\"+str(N) if not FULL_TOKENS else \"\")+\"_tokenizer.p\"\n",
    "    \n",
    "    if MODE == \"hashtrick_full_tokens\":\n",
    "        \n",
    "        def fitTokenizeJob(proc_id, articles, base_tokenizer):\n",
    "            print(\"Process\",proc_id,\"Started\")\n",
    "            \n",
    "            token_mapping = lambda x:base_tokenizer.index_word[x]\n",
    "            \n",
    "            #ALL THREADS RUN THIS\n",
    "            tk = create_tokenizer()\n",
    "            \n",
    "            #slow! but only need to run 1 time so...\n",
    "            token_articles_mapping = map(lambda article: map(lambda x:token_mapping(x) ,article), articles)\n",
    "\n",
    "            tk.fit_on_texts(token_articles_mapping)\n",
    "            del articles\n",
    "\n",
    "            file_name = output_tokenizer_file_name.format(proc_id)\n",
    "            print(\"save:\",file_name)\n",
    "\n",
    "            pickle.dump(tk,open(os.path.join('/','backup','pubmed_tokenizers','mp_to_merge',file_name),\"wb\"))\n",
    "            del tk\n",
    "            print(\"Process\",proc_id,\"ENDED\")\n",
    "        \n",
    "        #initialization of the process\n",
    "        def fitTokenizer_process_init(proc_id, articles, **kwargs):\n",
    "            if \"base_tokenizer\" in kwargs:\n",
    "                base_tokenizer = kwargs.pop(\"base_tokenizer\")\n",
    "            else:\n",
    "                raise TypeError(\"base_tokenizer must be used for the hashtrick tokenization\")\n",
    "            \n",
    "            return Process(target=fitTokenizeJob, args=(proc_id, articles, base_tokenizer))\n",
    "        \n",
    "    else:\n",
    "       \n",
    "        #process main function\n",
    "        def fitTokenizeJob(proc_id, articles):\n",
    "            print(\"Process\",proc_id,\"Started\")\n",
    "\n",
    "            #ALL THREADS RUN THIS\n",
    "            tk = create_tokenizer()\n",
    "            tk.fit_on_texts(articles)\n",
    "            del articles\n",
    "\n",
    "            file_name = output_tokenizer_file_name.format(proc_id)\n",
    "            print(\"save:\",file_name)\n",
    "\n",
    "            pickle.dump(tk,open(os.path.join('/','backup','pubmed_tokenizers','mp_to_merge',file_name),\"wb\"))\n",
    "            del tk\n",
    "            print(\"Process\",proc_id,\"ENDED\")\n",
    "\n",
    "         \n",
    "        #initialization of the process\n",
    "        def fitTokenizer_process_init(proc_id, articles, **kwargs):\n",
    "            return Process(target=fitTokenizeJob, args=(proc_id, articles,))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multithread_loop(process_init, n_thread_per_file = [12,12,12,12,12,20],**kwargs):\n",
    "    \n",
    "    for i,texts in enumerate(pubmed_generator()):\n",
    "        process = []\n",
    "\n",
    "        t_len = len(texts)\n",
    "        t_itter = t_len//n_thread_per_file[i]\n",
    "\n",
    "        batch = list(range(0,t_len,t_itter))\n",
    "\n",
    "        if len(batch)==n_thread_per_file[i]:\n",
    "            batch.append(t_len)\n",
    "        else:\n",
    "            batch[n_thread_per_file[i]] = t_len\n",
    "\n",
    "        for j in range(n_thread_per_file[i]):\n",
    "            process.append(process_init(sum(n_thread_per_file[:i])+j,texts[batch[j]:batch[j+1]],**kwargs))\n",
    "\n",
    "        print(\"Start\",n_thread_per_file[i],\"working threads\")\n",
    "        for p in process:\n",
    "            p.start()\n",
    "\n",
    "        print(\"Wait\",n_thread_per_file[i],\"working threads\")\n",
    "        for p in process:\n",
    "            p.join()\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the tokenizer in multithread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#FIT TOKENIER MULTITHREAD\n",
    "if not LOAD_TOKENIZER:\n",
    "    \n",
    "    n_thread_per_file = [20]*6\n",
    "    if MODE == \"hashtrick_full_tokens\":\n",
    "        multithread_loop(fitTokenizer_process_init,n_thread_per_file,base_tokenizer=base_tk)\n",
    "    else:\n",
    "        multithread_loop(fitTokenizer_process_init,n_thread_per_file)\n",
    "\n",
    "    gc.collect() ##some clean up\n",
    "    \n",
    "    ## Merge the files\n",
    "    print(\"Start the MERGE!\")\n",
    "    files = sorted(filter(lambda x:MODE in x,os.listdir(\"/backup/pubmed_tokenizers/mp_to_merge/\")))\n",
    "    print(files)\n",
    "\n",
    "    tk = create_tokenizer()\n",
    "\n",
    "    for file in files:\n",
    "        print(\"load:\",file,end=\"\\r\")\n",
    "        with open(\"/backup/pubmed_tokenizers/mp_to_merge/\"+file,\"rb\") as f:\n",
    "            loaded_tk = pickle.load(f)\n",
    "\n",
    "        #manual merge\n",
    "        for w,c in loaded_tk.word_counts.items():\n",
    "            if w in tk.word_counts:\n",
    "                tk.word_counts[w] += c\n",
    "            else:\n",
    "                tk.word_counts[w] = c\n",
    "\n",
    "        for w,c in loaded_tk.word_docs.items():\n",
    "            if w in tk.word_docs:\n",
    "                tk.word_docs[w] += c\n",
    "            else:\n",
    "                tk.word_docs[w] = c\n",
    "        \n",
    "        tk.document_count += loaded_tk.document_count\n",
    "    #CODE FROM KERAS TOKENIZER\n",
    "    #\n",
    "\n",
    "    wcounts = list(tk.word_counts.items())\n",
    "    wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "    # forcing the oov_token to index 1 if it exists\n",
    "    if tk.oov_token is None:\n",
    "        sorted_voc = []\n",
    "    else:\n",
    "        sorted_voc = [tk.oov_token]\n",
    "    sorted_voc.extend(wc[0] for wc in wcounts)\n",
    "\n",
    "    # note that index 0 is reserved, never assigned to an existing word\n",
    "    tk.word_index = dict(\n",
    "        list(zip(sorted_voc, list(range(1, len(sorted_voc) + 1)))))\n",
    "\n",
    "    tk.index_word = dict((c, w) for w, c in tk.word_index.items())\n",
    "\n",
    "    for w, c in list(tk.word_docs.items()):\n",
    "        tk.index_docs[tk.word_index[w]] = c\n",
    "\n",
    "\n",
    "    print(\"Save:\",output_merged_tokenizer_file_name)\n",
    "    if not FULL_TOKENS:\n",
    "        max_vocabulary = len(list(filter(lambda x:x[1]>=N,tk.word_counts.items())))\n",
    "        tk.num_words = max_vocabulary + 1\n",
    "    print(\"Voc size\",tk.num_words)\n",
    "\n",
    "    with open(\"/backup/pubmed_tokenizers/\"+output_merged_tokenizer_file_name,\"wb\") as f:\n",
    "        pickle.dump(tk,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize in multithread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ONLY_TITLE:\n",
    "    output_file_name = MODE+'_file_{0:03}_title_pubmed.p'\n",
    "else:\n",
    "    output_file_name = MODE+'_file_{0:03}_title_abs_pubmed.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Open the file: pubmed_ready_00000000_to_02776362\n",
      "Returning: 2776363 articles\n",
      "Start 12 working threads\n",
      "Process 0 Started\n",
      "Process 1 Started\n",
      "Process 2 Started\n",
      "Process 3 Started\n",
      "Process 4 Started\n",
      "Process 5 Started\n",
      "Process 6 Started\n",
      "Process 7 Started\n",
      "Process 8 Started\n",
      "Process 9 Started\n",
      "Process 10 Started\n",
      "Process 11 Started\n",
      "Wait 12 working threads\n",
      "save: regex_less_700k_freq_file_000_title_abs_pubmed.p\n",
      "Process 0 ENDED\n",
      "save: regex_less_700k_freq_file_008_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_002_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_010_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_007_title_abs_pubmed.p\n",
      "Process 8 ENDED\n",
      "save: regex_less_700k_freq_file_011_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_009_title_abs_pubmed.p\n",
      "Process 2 ENDED\n",
      "save: regex_less_700k_freq_file_001_title_abs_pubmed.p\n",
      "Process 10 ENDED\n",
      "Process 7 ENDED\n",
      "save: regex_less_700k_freq_file_006_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_004_title_abs_pubmed.p\n",
      "Process 11 ENDED\n",
      "save: regex_less_700k_freq_file_005_title_abs_pubmed.p\n",
      "Process 9 ENDED\n",
      "Process 1 ENDED\n",
      "save: regex_less_700k_freq_file_003_title_abs_pubmed.p\n",
      "Process 6 ENDED\n",
      "Process 4 ENDED\n",
      "Process 5 ENDED\n",
      "Process 3 ENDED\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_02776363_to_05519968\n",
      "Returning: 2743606 articles\n",
      "Start 12 working threads\n",
      "Process 12 Started\n",
      "Process 13 Started\n",
      "Process 14 Started\n",
      "Process 15 Started\n",
      "Process 16 Started\n",
      "Process 17 Started\n",
      "Process 18 Started\n",
      "Process 19 Started\n",
      "Process 20 Started\n",
      "Process 21 Started\n",
      "Process 22 Started\n",
      "Process 23 Started\n",
      "Wait 12 working threads\n",
      "save: regex_less_700k_freq_file_015_title_abs_pubmed.p\n",
      "Process 15 ENDED\n",
      "save: regex_less_700k_freq_file_012_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_013_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_014_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_019_title_abs_pubmed.p\n",
      "Process 12 ENDED\n",
      "save: regex_less_700k_freq_file_023_title_abs_pubmed.p\n",
      "Process 13 ENDED\n",
      "Process 14 ENDED\n",
      "Process 19 ENDED\n",
      "Process 23 ENDED\n",
      "save: regex_less_700k_freq_file_018_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_021_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_022_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_016_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_020_title_abs_pubmed.p\n",
      "Process 18 ENDED\n",
      "save: regex_less_700k_freq_file_017_title_abs_pubmed.p\n",
      "Process 21 ENDED\n",
      "Process 22 ENDED\n",
      "Process 16 ENDED\n",
      "Process 20 ENDED\n",
      "Process 17 ENDED\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_05519969_to_08241071\n",
      "Returning: 2721103 articles\n",
      "Start 12 working threads\n",
      "Process 24 Started\n",
      "Process 25 Started\n",
      "Process 26 Started\n",
      "Process 27 Started\n",
      "Process 28 Started\n",
      "Process 29 Started\n",
      "Process 30 Started\n",
      "Process 31 Started\n",
      "Process 32 Started\n",
      "Process 33 Started\n",
      "Process 34 Started\n",
      "Process 35 Started\n",
      "Wait 12 working threads\n",
      "save: regex_less_700k_freq_file_025_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_027_title_abs_pubmed.p\n",
      "Process 25 ENDED\n",
      "save: regex_less_700k_freq_file_032_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_031_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_024_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_030_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_033_title_abs_pubmed.p\n",
      "Process 27 ENDED\n",
      "save: regex_less_700k_freq_file_035_title_abs_pubmed.p\n",
      "Process 31 ENDED\n",
      "Process 32 ENDED\n",
      "Process 24 ENDED\n",
      "save: regex_less_700k_freq_file_028_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_034_title_abs_pubmed.p\n",
      "Process 30 ENDED\n",
      "Process 33 ENDED\n",
      "save: regex_less_700k_freq_file_026_title_abs_pubmed.p\n",
      "Process 35 ENDED\n",
      "Process 28 ENDED\n",
      "save: regex_less_700k_freq_file_029_title_abs_pubmed.p\n",
      "Process 34 ENDED\n",
      "Process 26 ENDED\n",
      "Process 29 ENDED\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_08241072_to_11124313\n",
      "Returning: 2883242 articles\n",
      "Start 12 working threads\n",
      "Process 36 Started\n",
      "Process 37 Started\n",
      "Process 38 Started\n",
      "Process 39 Started\n",
      "Process 40 Started\n",
      "Process 41 Started\n",
      "Process 42 Started\n",
      "Process 43 Started\n",
      "Process 44 Started\n",
      "Process 45 Started\n",
      "Process 46 Started\n",
      "Process 47 Started\n",
      "Wait 12 working threads\n",
      "save: regex_less_700k_freq_file_046_title_abs_pubmed.p\n",
      "Process 46 ENDED\n",
      "save: regex_less_700k_freq_file_036_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_041_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_039_title_abs_pubmed.p\n",
      "Process 36 ENDED\n",
      "Process 41 ENDED\n",
      "save: regex_less_700k_freq_file_042_title_abs_pubmed.p\n",
      "Process 39 ENDED\n",
      "save: regex_less_700k_freq_file_047_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_045_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_038_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_044_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_040_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_037_title_abs_pubmed.p\n",
      "Process 42 ENDED\n",
      "Process 47 ENDED\n",
      "Process 45 ENDED\n",
      "Process 44 ENDED\n",
      "Process 38 ENDED\n",
      "Process 40 ENDED\n",
      "Process 37 ENDED\n",
      "save: regex_less_700k_freq_file_043_title_abs_pubmed.p\n",
      "Process 43 ENDED\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_11124314_to_13996815\n",
      "Returning: 2872502 articles\n",
      "Start 12 working threads\n",
      "Process 48 Started\n",
      "Process 49 Started\n",
      "Process 50 Started\n",
      "Process 51 Started\n",
      "Process 52 Started\n",
      "Process 53 Started\n",
      "Process 54 Started\n",
      "Process 55 Started\n",
      "Process 56 Started\n",
      "Process 57 Started\n",
      "Process 58 Started\n",
      "Process 59 Started\n",
      "Wait 12 working threads\n",
      "save: regex_less_700k_freq_file_051_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_049_title_abs_pubmed.p\n",
      "Process 51 ENDED\n",
      "save: regex_less_700k_freq_file_057_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_056_title_abs_pubmed.p\n",
      "Process 49 ENDED\n",
      "Process 57 ENDED\n",
      "Process 56 ENDED\n",
      "save: regex_less_700k_freq_file_048_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_054_title_abs_pubmed.p\n",
      "Process 48 ENDED\n",
      "save: regex_less_700k_freq_file_050_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_052_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_055_title_abs_pubmed.p\n",
      "Process 54 ENDED\n",
      "save: regex_less_700k_freq_file_059_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_053_title_abs_pubmed.p\n",
      "Process 50 ENDED\n",
      "Process 52 ENDED\n",
      "Process 55 ENDED\n",
      "save: regex_less_700k_freq_file_058_title_abs_pubmed.p\n",
      "Process 59 ENDED\n",
      "Process 53 ENDED\n",
      "Process 58 ENDED\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_13996816_to_18824354\n",
      "Returning: 4827539 articles\n",
      "Start 20 working threads\n",
      "Process 60 Started\n",
      "Process 61 Started\n",
      "Process 62 Started\n",
      "Process 63 Started\n",
      "Process 64 Started\n",
      "Process 65 Started\n",
      "Process 66 Started\n",
      "Process 67 Started\n",
      "Process 68 Started\n",
      "Process 69 Started\n",
      "Process 70 Started\n",
      "Process 71 Started\n",
      "Process 72 Started\n",
      "Process 73 Started\n",
      "Process 74 Started\n",
      "Process 75 Started\n",
      "Process 76 Started\n",
      "Process 77 Started\n",
      "Process 78 Started\n",
      "Process 79 Started\n",
      "Wait 20 working threads\n",
      "save: regex_less_700k_freq_file_068_title_abs_pubmed.p\n",
      "Process 68 ENDED\n",
      "save: regex_less_700k_freq_file_063_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_062_title_abs_pubmed.p\n",
      "Process 63 ENDED\n",
      "Process 62 ENDED\n",
      "save: regex_less_700k_freq_file_069_title_abs_pubmed.p\n",
      "Process 69 ENDED\n",
      "save: regex_less_700k_freq_file_061_title_abs_pubmed.p\n",
      "Process 61 ENDED\n",
      "save: regex_less_700k_freq_file_064_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_065_title_abs_pubmed.p\n",
      "Process 65 ENDED\n",
      "Process 64 ENDED\n",
      "save: regex_less_700k_freq_file_073_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_070_title_abs_pubmed.p\n",
      "Process 73 ENDED\n",
      "save: regex_less_700k_freq_file_076_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_078_title_abs_pubmed.p\n",
      "Process 70 ENDED\n",
      "save: regex_less_700k_freq_file_077_title_abs_pubmed.p\n",
      "Process 76 ENDED\n",
      "Process 78 ENDED\n",
      "Process 77 ENDED\n",
      "save: regex_less_700k_freq_file_066_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_071_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_060_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_072_title_abs_pubmed.p\n",
      "Process 66 ENDED\n",
      "Process 71 ENDED\n",
      "Process 60 ENDED\n",
      "save: regex_less_700k_freq_file_067_title_abs_pubmed.p\n",
      "Process 72 ENDED\n",
      "Process 67 ENDED\n",
      "save: regex_less_700k_freq_file_074_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_079_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_075_title_abs_pubmed.p\n",
      "Process 74 ENDED\n",
      "Process 79 ENDED\n",
      "Process 75 ENDED\n",
      "Force garbage collector 0\n",
      "['regex_less_700k_freq_file_000_title_abs_pubmed.p', 'regex_less_700k_freq_file_001_title_abs_pubmed.p', 'regex_less_700k_freq_file_002_title_abs_pubmed.p', 'regex_less_700k_freq_file_003_title_abs_pubmed.p', 'regex_less_700k_freq_file_004_title_abs_pubmed.p', 'regex_less_700k_freq_file_005_title_abs_pubmed.p', 'regex_less_700k_freq_file_006_title_abs_pubmed.p', 'regex_less_700k_freq_file_007_title_abs_pubmed.p', 'regex_less_700k_freq_file_008_title_abs_pubmed.p', 'regex_less_700k_freq_file_009_title_abs_pubmed.p', 'regex_less_700k_freq_file_010_title_abs_pubmed.p', 'regex_less_700k_freq_file_011_title_abs_pubmed.p', 'regex_less_700k_freq_file_012_title_abs_pubmed.p', 'regex_less_700k_freq_file_013_title_abs_pubmed.p', 'regex_less_700k_freq_file_014_title_abs_pubmed.p', 'regex_less_700k_freq_file_015_title_abs_pubmed.p', 'regex_less_700k_freq_file_016_title_abs_pubmed.p', 'regex_less_700k_freq_file_017_title_abs_pubmed.p', 'regex_less_700k_freq_file_018_title_abs_pubmed.p', 'regex_less_700k_freq_file_019_title_abs_pubmed.p', 'regex_less_700k_freq_file_020_title_abs_pubmed.p', 'regex_less_700k_freq_file_021_title_abs_pubmed.p', 'regex_less_700k_freq_file_022_title_abs_pubmed.p', 'regex_less_700k_freq_file_023_title_abs_pubmed.p', 'regex_less_700k_freq_file_024_title_abs_pubmed.p', 'regex_less_700k_freq_file_025_title_abs_pubmed.p', 'regex_less_700k_freq_file_026_title_abs_pubmed.p', 'regex_less_700k_freq_file_027_title_abs_pubmed.p', 'regex_less_700k_freq_file_028_title_abs_pubmed.p', 'regex_less_700k_freq_file_029_title_abs_pubmed.p', 'regex_less_700k_freq_file_030_title_abs_pubmed.p', 'regex_less_700k_freq_file_031_title_abs_pubmed.p', 'regex_less_700k_freq_file_032_title_abs_pubmed.p', 'regex_less_700k_freq_file_033_title_abs_pubmed.p', 'regex_less_700k_freq_file_034_title_abs_pubmed.p', 'regex_less_700k_freq_file_035_title_abs_pubmed.p', 'regex_less_700k_freq_file_036_title_abs_pubmed.p', 'regex_less_700k_freq_file_037_title_abs_pubmed.p', 'regex_less_700k_freq_file_038_title_abs_pubmed.p', 'regex_less_700k_freq_file_039_title_abs_pubmed.p', 'regex_less_700k_freq_file_040_title_abs_pubmed.p', 'regex_less_700k_freq_file_041_title_abs_pubmed.p', 'regex_less_700k_freq_file_042_title_abs_pubmed.p', 'regex_less_700k_freq_file_043_title_abs_pubmed.p', 'regex_less_700k_freq_file_044_title_abs_pubmed.p', 'regex_less_700k_freq_file_045_title_abs_pubmed.p', 'regex_less_700k_freq_file_046_title_abs_pubmed.p', 'regex_less_700k_freq_file_047_title_abs_pubmed.p', 'regex_less_700k_freq_file_048_title_abs_pubmed.p', 'regex_less_700k_freq_file_049_title_abs_pubmed.p', 'regex_less_700k_freq_file_050_title_abs_pubmed.p', 'regex_less_700k_freq_file_051_title_abs_pubmed.p', 'regex_less_700k_freq_file_052_title_abs_pubmed.p', 'regex_less_700k_freq_file_053_title_abs_pubmed.p', 'regex_less_700k_freq_file_054_title_abs_pubmed.p', 'regex_less_700k_freq_file_055_title_abs_pubmed.p', 'regex_less_700k_freq_file_056_title_abs_pubmed.p', 'regex_less_700k_freq_file_057_title_abs_pubmed.p', 'regex_less_700k_freq_file_058_title_abs_pubmed.p', 'regex_less_700k_freq_file_059_title_abs_pubmed.p', 'regex_less_700k_freq_file_060_title_abs_pubmed.p', 'regex_less_700k_freq_file_061_title_abs_pubmed.p', 'regex_less_700k_freq_file_062_title_abs_pubmed.p', 'regex_less_700k_freq_file_063_title_abs_pubmed.p', 'regex_less_700k_freq_file_064_title_abs_pubmed.p', 'regex_less_700k_freq_file_065_title_abs_pubmed.p', 'regex_less_700k_freq_file_066_title_abs_pubmed.p', 'regex_less_700k_freq_file_067_title_abs_pubmed.p', 'regex_less_700k_freq_file_068_title_abs_pubmed.p', 'regex_less_700k_freq_file_069_title_abs_pubmed.p', 'regex_less_700k_freq_file_070_title_abs_pubmed.p', 'regex_less_700k_freq_file_071_title_abs_pubmed.p', 'regex_less_700k_freq_file_072_title_abs_pubmed.p', 'regex_less_700k_freq_file_073_title_abs_pubmed.p', 'regex_less_700k_freq_file_074_title_abs_pubmed.p', 'regex_less_700k_freq_file_075_title_abs_pubmed.p', 'regex_less_700k_freq_file_076_title_abs_pubmed.p', 'regex_less_700k_freq_file_077_title_abs_pubmed.p', 'regex_less_700k_freq_file_078_title_abs_pubmed.p', 'regex_less_700k_freq_file_079_title_abs_pubmed.p']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save: regex_less_700k_freq_file_000_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_001_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_002_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_003_title_abs_pubmed.p\n",
      "save: regex_less_700k_freq_file_004_title_abs_pubmed.p\n"
     ]
    }
   ],
   "source": [
    "MERGE_FLAG = True\n",
    "\n",
    "if MODE == \"bag_of_trigrams\":\n",
    "    MERGE_FLAG = False\n",
    "    def tokenizeJob(proc_id, tokenizer, texts):\n",
    "        print(\"Process\",proc_id,\"Started\")\n",
    "\n",
    "        TRIGRAM_VOC = len(tokenizer.word_index) + 1 \n",
    "        \n",
    "        #ALL THREADS RUN THIS\n",
    "        _matrix = np.zeros((len(texts),TRIGRAM_VOC), dtype=np.int8)\n",
    "\n",
    "        for i,text in enumerate(texts):\n",
    "            bag_of_word = regex_alfanum_tokenizer(text)\n",
    "            for j in tokenizer.texts_to_sequences(bag_of_word):\n",
    "                _matrix[i][j] += 1\n",
    "        \n",
    "        file_name = output_file_name.format(proc_id)+\".npz\"\n",
    "        print(\"save:\",file_name)\n",
    "        sparse.save_npz(os.path.join('/','backup','pubmed_archive_tokenized','mp_to_merge',file_name), sparse.coo_matrix(_matrix))\n",
    "        #np.save(os.path.join('/','backup','pubmed_archive_tokenized','mp_to_merge',file_name),_matrix)\n",
    "        #pickle.dump(_matrix,open(os.path.join('/','backup','pubmed_archive_tokenized','mp_to_merge',file_name),\"wb\"),protocol=4)\n",
    "        del tokenizer\n",
    "        del texts\n",
    "        print(\"Process\",proc_id,\"ENDED\")\n",
    "    \n",
    "else:\n",
    "    def tokenizeJob(proc_id, tokenizer, articles):\n",
    "        print(\"Process\",proc_id,\"Started\")\n",
    "\n",
    "        #ALL THREADS RUN THIS\n",
    "        tokenized = tokenizer.texts_to_sequences(articles)\n",
    "\n",
    "        file_name = output_file_name.format(proc_id)\n",
    "        print(\"save:\",file_name)\n",
    "\n",
    "        pickle.dump(tokenized,open(os.path.join('/','backup','pubmed_archive_tokenized','mp_to_merge',file_name),\"wb\"))\n",
    "        del tokenized\n",
    "        del articles\n",
    "        print(\"Process\",proc_id,\"ENDED\")\n",
    "\n",
    "#same some memory\n",
    "del tk.index_docs\n",
    "del tk.index_word\n",
    "del tk.word_counts\n",
    "del tk.word_docs\n",
    "\n",
    "print(gc.collect())\n",
    "\n",
    "def tokenize_process_init(proc_id, articles):\n",
    "\n",
    "    \n",
    "    return Process(target=tokenizeJob, args=(proc_id, tk, articles,))\n",
    "\n",
    "\n",
    "multithread_loop(tokenize_process_init)\n",
    "        \n",
    "# Grouping the resulting files\n",
    "gc.collect() ##some clean up\n",
    "if MERGE_FLAG: \n",
    "    files = sorted(filter(lambda x:MODE in x,os.listdir(\"/backup/pubmed_archive_tokenized/mp_to_merge/\")))\n",
    "    print(files)\n",
    "    len_files = len(files)\n",
    "    n_output_files = 5\n",
    "    iter_files = len_files//n_output_files\n",
    "    batch = list(range(0,len_files,iter_files))\n",
    "\n",
    "    if len(batch)==n_output_files:\n",
    "        batch.append(len_files)\n",
    "    else:\n",
    "        batch[n_output_files] = len_files\n",
    "\n",
    "    for i in range(n_output_files):\n",
    "        tokenized = []\n",
    "        for file in files[batch[i]:batch[i+1]]:\n",
    "            print(\"load:\",file,end=\"\\r\")\n",
    "            with open(\"/backup/pubmed_archive_tokenized/mp_to_merge/\"+file,\"rb\") as f:\n",
    "                tokenized.extend(pickle.load(f))\n",
    "\n",
    "        file_name = output_file_name.format(i)\n",
    "        print(\"save:\",file_name)\n",
    "\n",
    "        with open(os.path.join('/','backup','pubmed_archive_tokenized',file_name),\"wb\") as f:\n",
    "            pickle.dump(tokenized,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.session.delete();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the tokenized articles\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open the pubmed tokenized tar.gz\n",
      "Creating generator\n"
     ]
    }
   ],
   "source": [
    "#Verify the tokenized articles\n",
    "\n",
    "tokenized_pubmed_generator = ph.create_tokenized_pubmed_collection_generator(path=\"/backup/pubmed_archive_tokenized/bllip_N10_title_abs.tar.gz\")#mode=\"bllip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open the file: 0\n",
      "Returning: 3690895 articles\n",
      "Force garbage collector 0\n",
      "Open the file: 1\n",
      "Returning: 3643138 articles\n",
      "Force garbage collector 0\n",
      "Open the file: 2\n",
      "Returning: 3790281 articles\n",
      "Force garbage collector 0\n",
      "Open the file: 3\n",
      "Returning: 3838006 articles\n",
      "Force garbage collector 0\n",
      "Open the file: 4\n",
      "Returning: 3862035 articles\n",
      "Force garbage collector 0\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "index_zero_len = []\n",
    "for collection in tokenized_pubmed_generator():\n",
    "    for doc in collection:\n",
    "        if len(doc)==0:\n",
    "            index_zero_len.append(index)\n",
    "        index+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18824355"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"bllip\"\n",
    "files = sorted(filter(lambda x:MODE in x,os.listdir(\"/backup/pubmed_archive_tokenized/mp_to_merge/\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 8, 12, 16, 19]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_files = len(files)\n",
    "n_output_files = 5\n",
    "iter_files = len_files//n_output_files\n",
    "batch = list(range(0,len_files,iter_files))\n",
    "\n",
    "if len(batch)==n_output_files:\n",
    "    batch.append(len_files)\n",
    "else:\n",
    "    batch[n_output_files] = len_files\n",
    "    \n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bllip_N10_file_00_title_abs_pubmed.p\n",
      "bllip_N10_file_01_title_abs_pubmed.p\n",
      "bllip_N10_file_02_title_abs_pubmed.p\n",
      "bllip_N10_file_03_title_abs_pubmed.p\n",
      "bllip_N10_file_04_title_abs_pubmed.p\n",
      "bllip_N10_file_05_title_abs_pubmed.p\n",
      "bllip_N10_file_06_title_abs_pubmed.p\n",
      "bllip_N10_file_07_title_abs_pubmed.p\n",
      "bllip_N10_file_08_title_abs_pubmed.p\n",
      "bllip_N10_file_09_title_abs_pubmed.p\n",
      "bllip_N10_file_10_title_abs_pubmed.p\n",
      "bllip_N10_file_11_title_abs_pubmed.p\n",
      "bllip_N10_file_12_title_abs_pubmed.p\n",
      "bllip_N10_file_13_title_abs_pubmed.p\n",
      "bllip_N10_file_14_title_abs_pubmed.p\n",
      "bllip_N10_file_15_title_abs_pubmed.p\n",
      "bllip_N10_file_16_title_abs_pubmed.p\n",
      "bllip_N10_file_17_title_abs_pubmed.p\n",
      "bllip_N10_file_18_title_abs_pubmed.p\n",
      "bllip_N10_file_19_title_abs_pubmed.p\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for name in files:\n",
    "    print(name)\n",
    "    with open(os.path.join(\"/backup/pubmed_archive_tokenized/mp_to_merge/\",name),\"rb\") as f:\n",
    "        count+=len(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18824349"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a index:pmid document mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open the pubmed tar.gz\n",
      "Creating generator\n",
      "Open the file: pubmed_ready_00000000_to_02776362\n",
      "Returning: 2776363 articles\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_02776363_to_05519968\n",
      "Returning: 2743606 articles\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_05519969_to_08241071\n",
      "Returning: 2721103 articles\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_08241072_to_11124313\n",
      "Returning: 2883242 articles\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_11124314_to_13996815\n",
      "Returning: 2872502 articles\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_13996816_to_18824354\n",
      "Returning: 4827539 articles\n",
      "Force garbage collector 0\n"
     ]
    }
   ],
   "source": [
    "#Create a generator for pubmed data\n",
    "from bidict import bidict\n",
    "\n",
    "map_function = lambda x:x[\"pmid\"]\n",
    "\n",
    "pubmed_generator = ph.create_pubmed_collection_generator(map_function,path=\"/backup/pubmed_archive_json/pubmed_ready.tar.gz\")\n",
    "\n",
    "index = 0\n",
    "#equal_pmid = [] #(anterior, novo)\n",
    "#bidirectional dictionary\n",
    "doc_pmid_index = bidict()\n",
    "for collection in pubmed_generator():\n",
    "    for doc in collection:\n",
    "        #if doc[0] in doc_pmid_index:\n",
    "            #equal_pmid.append((doc_pmid_index[doc[0]],doc))\n",
    "        doc_pmid_index[doc] = index\n",
    "        index+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20978152'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_pmid_index.inverse[18821518]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18824355"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REMOVE DUPLICATED ARTICLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open the pubmed tar.gz\n",
      "Creating generator\n",
      "Open the file: 0\n",
      "Returning: 3000000 articles\n",
      "Save file /backup/pubmed_archive_json/pubmed_ready_00000000_to_02776362 :Force garbage collector 32\n",
      "Open the file: 1\n",
      "Returning: 3000000 articles\n",
      "Save file /backup/pubmed_archive_json/pubmed_ready_02776363_to_05519968 :Force garbage collector 32\n",
      "Open the file: 2\n",
      "Returning: 3000000 articles\n",
      "Save file /backup/pubmed_archive_json/pubmed_ready_05519969_to_08241071 :Force garbage collector 32\n",
      "Open the file: 3\n",
      "Returning: 3000000 articles\n",
      "Save file /backup/pubmed_archive_json/pubmed_ready_08241072_to_11124313 :Force garbage collector 32\n",
      "Open the file: 4\n",
      "Returning: 3000000 articles\n",
      "Save file /backup/pubmed_archive_json/pubmed_ready_11124314_to_13996815 :Force garbage collector 32\n",
      "Open the file: 5\n",
      "Returning: 4885278 articles\n",
      "Save file /backup/pubmed_archive_json/pubmed_ready_13996816_to_18824354 :Force garbage collector 32\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "pubmed_generator = ph.create_pubmed_collection_generator()\n",
    "\n",
    "index = 0\n",
    "last_len = 0\n",
    "articles = []\n",
    "for collection in pubmed_generator():\n",
    "    for doc in collection:\n",
    "        if doc_pmid_index[doc['pmid']] == index :\n",
    "            articles.append(doc)\n",
    "        index+=1\n",
    "    \n",
    "    file_name = \"/backup/pubmed_archive_json/pubmed_ready_{0:08}_to_{1:08}\".format(last_len,(last_len+len(articles))-1)\n",
    "    print(\"Save file\",file_name,\":\",end=\"\")\n",
    "    json.dump(articles,open(file_name,\"w\"))\n",
    "    last_len = last_len+len(articles)\n",
    "    del articles\n",
    "    articles = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open the pubmed tar.gz\n",
      "Creating generator\n",
      "Open the file: pubmed_ready_00000000_to_02776362\n",
      "Returning: 2776363 articles\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_02776363_to_05519968\n",
      "Returning: 2743606 articles\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_05519969_to_08241071\n",
      "Returning: 2721103 articles\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_08241072_to_11124313\n",
      "Returning: 2883242 articles\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_11124314_to_13996815\n",
      "Returning: 2872502 articles\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_13996816_to_18824354\n",
      "Returning: 4827539 articles\n",
      "Force garbage collector 0\n"
     ]
    }
   ],
   "source": [
    "##count articles\n",
    "\n",
    "g = ph.create_pubmed_collection_generator()\n",
    "total_docs = 0\n",
    "for docs in g():\n",
    "    total_docs += len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open the file: pubmed_ready_00000000_to_02776362\n",
      "Returning: 2776363 articles\n"
     ]
    }
   ],
   "source": [
    "docs= next(g())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': 'The O protein of bacteriophage lambda is required for initiation of DNA replication at the lambda replicative origin designated ori lambda. The binding sites for O protein are four direct repeats, each of which is an inverted repeat. By means of electron microscopy, we have found that phage lambda O protein utilizes these multiple binding sites to form a specific nucleoprotein structure in which the origin DNA is inferred to be folded or wound. The phage lambda O and P proteins and host DnaB protein interact at ori lambda to generate a larger structure than that formed by O protein alone; P and DnaB proteins fail to form any observable complex when O protein is excluded from the reaction mixture. We conclude that the specialized nucleoprotein structure formed by phage lambda O protein and ori lambda provides for localized initiation of DNA replication by serving as the foundation for the assembly of the initial priming structure. Specialized nucleoprotein structures may be a general means to confer exceptional accuracy on DNA transactions requiring extraordinary precision.',\n",
       " 'pmid': '2991888',\n",
       " 'title': 'Specialized nucleoprotein structures at the origin of replication of bacteriophage lambda: complexes with lambda O protein and with lambda O, lambda P, and Escherichia coli DnaB proteins.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[853213]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/backup/pubmed_archive_tokenized/bllip_stem_file_04_title_abs_pubmed.p\",\"rb\") as f:\n",
    "    tok_articles = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tk.index_word[w] for w in tok_articles[853213]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
