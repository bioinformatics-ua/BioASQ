{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "import os\n",
    "os.chdir(\"/home/tiagoalmeida/bioASQ-taskb/\")\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from bisect import bisect\n",
    "\n",
    "\n",
    "##add keras to the modules\n",
    "module_path = os.path.abspath(os.path.join('pubmed_data'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from pubmed_data import pubmed_helper as ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepRank\n",
    "Reference PAPER :https://arxiv.org/pdf/1710.05649.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network structure\n",
    " - [General Network Configuration](#var_def)\n",
    " - [Input Network](#input_net)\n",
    " - [Measure Network](#measure_net)\n",
    " - [Aggregation Network](#aggreation_net)\n",
    " - [Final Network](#final_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load regex_full_tokens_tokenizer.p\n",
      "Load regex_full_tokens_word_embedding.p\n"
     ]
    }
   ],
   "source": [
    "#Load tokenizer and the embedding matrix\n",
    "\n",
    "MODE = \"regex_full_tokens\"\n",
    "tk = ph.load_tokenizer(mode=MODE)\n",
    "emb_dict = ph.load_embeddings(mode=MODE)\n",
    "\n",
    "assert len(tk.word_counts) == len(emb_dict)\n",
    "\n",
    "#Number of different words\n",
    "VOCAB_SIZE = len(tk.word_counts)+1\n",
    "\n",
    "#Dimension of embeddings\n",
    "EMB_DIM = emb_dict[1].shape[0]\n",
    "\n",
    "emb_matrix = np.zeros((VOCAB_SIZE, EMB_DIM))\n",
    "\n",
    "for i,vector in emb_dict.items():\n",
    "    emb_matrix[i] = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenized stopwords\n",
    "\n",
    "biomedical_stop_words = [\"a\", \"about\", \"again\", \"all\", \"almost\", \"also\", \"although\", \"always\", \"among\", \"an\", \"and\", \"another\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"between\", \"both\", \"but\", \"by\", \"can\", \"could\", \"did\", \"do\", \"does\", \"done\", \"due\", \"during\", \"each\", \"either\", \"enough\", \"especially\", \"etc\", \"for\", \"found\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"here\", \"how\", \"however\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"just\", \"kg\", \"km\", \"made\", \"mainly\", \"make\", \"may\", \"mg\", \"might\", \"ml\", \"mm\", \"most\", \"mostly\", \"must\", \"nearly\", \"neither\", \"no\", \"nor\", \"obtained\", \"of\", \"often\", \"on\", \"our\", \"overall\", \"perhaps\", \"pmid\", \"quite\", \"rather\", \"really\", \"regarding\", \"seem\", \"seen\", \"several\", \"should\", \"show\", \"showed\", \"shown\", \"shows\", \"significantly\", \"since\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"then\", \"there\", \"therefore\", \"these\", \"they\", \"this\", \"those\", \"through\", \"thus\", \"to\", \"upon\", \"use\", \"used\", \"using\", \"various\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"which\", \"while\", \"with\", \"within\", \"without\", \"would\"]\n",
    "biomedical_stop_words_tokens = set(tk.texts_to_sequences([biomedical_stop_words])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='var_def'></a>\n",
    "## General Network Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import unstack, stack\n",
    "##Test \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, activations\n",
    "from tensorflow.keras.initializers import Zeros, Ones, Constant\n",
    "from tensorflow.keras.layers import Lambda, Bidirectional, Dot,Masking,Reshape, Concatenate, Layer, Embedding, Input, Conv2D, GlobalMaxPooling2D, Flatten, TimeDistributed, GRU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.activations import tanh, sigmoid\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "#Number max of term per query\n",
    "MAX_Q_TERM = 15\n",
    "\n",
    "#Number max of the snippet terms\n",
    "QUERY_CENTRIC_CONTEX = 13\n",
    "\n",
    "#Number max of passages per query term\n",
    "MAX_PASSAGES_PER_QUERY = 3\n",
    "\n",
    "#Snippet position padding value\n",
    "SNIPPET_POSITION_PADDING_VALUE = -1\n",
    "\n",
    "#Mode for the creation of the S matrix\n",
    "S_MATRIX_MODE = 0\n",
    "#S_MATRIX_DIMENSION = EMB_DIM*2+1\n",
    "\n",
    "#Train embedding weights\n",
    "EMB_TRAINABLE = False\n",
    "\n",
    "#Number of filters in CNN\n",
    "CNN_FILTERS = 256\n",
    "CNN_KERNELS = (3,3)\n",
    "\n",
    "#RNN DIM\n",
    "USE_BIDIRECTIONAL = False\n",
    "GRU_REPRESENTATION_DIM = 56\n",
    "\n",
    "ACTIVATION_FUNCTION = \"selu\"\n",
    "\n",
    "#Term gating network mode\n",
    "TERM_GATING_MODE = 2 #0- weigt fixed per position, 1 - DRMM like term gating\n",
    "\n",
    "assert S_MATRIX_MODE in [0,1]\n",
    "assert TERM_GATING_MODE in [0,1,2]\n",
    "\n",
    "#MACRO STYLE\n",
    "def S_MATRIX_3D_DIMENSION():\n",
    "    if S_MATRIX_MODE==0:\n",
    "        return 1\n",
    "    elif S_MATRIX_MODE==1:\n",
    "        return EMB_DIM*2+1\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='input_net'></a>\n",
    "## Input Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "snippet_emb_model summary\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "snippet_token (InputLayer)   (None, 15, 3, 13)         0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  (None, 15, 3, 13, 200)    858359000 \n",
      "_________________________________________________________________\n",
      "snippet_transpose (Lambda)   (None, 15, 3, 200, 13)    0         \n",
      "=================================================================\n",
      "Total params: 858,359,000\n",
      "Trainable params: 0\n",
      "Non-trainable params: 858,359,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "  ---- Custom Layers ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "class SimilarityMatrix(Layer):\n",
    "    \n",
    "    def __init__(self, query_max_term, snippet_max_term, interaction_mode=0, **kwargs):\n",
    "        \"\"\"\n",
    "        interaction mode 0: only use similarity matrix\n",
    "                    mode 1: similarity matrix + query and snippet embeddings\n",
    "        \"\"\"\n",
    "        assert interaction_mode in [0,1] #only valid modes\n",
    "        \n",
    "        self.query_max_term = query_max_term\n",
    "        self.snippet_max_term = snippet_max_term\n",
    "        self.interaction_mode = interaction_mode\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self,x):\n",
    "        if self.interaction_mode==0:\n",
    "            #sim => dot product (None, MAX_Q_TERM, EMB_DIM) x (None, MAX_Q_TERM, MAX_PASSAGE_PER_Q, EMB_DIM, QUERY_CENTRIC_CONTEX)\n",
    "            query = K.expand_dims(x[0], axis=1) #(None, 1, MAX_Q_TERM, EMB_DIM)\n",
    "            query = K.expand_dims(query, axis=1) #(None, 1, 1, MAX_Q_TERM, EMB_DIM)\n",
    "            query = K.repeat_elements(query,x[1].shape[1],axis=1) #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, EMB_DIM)\n",
    "            query = K.repeat_elements(query,x[1].shape[2],axis=2)\n",
    "            s_matrix = K.batch_dot(query,x[1]) #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, EMB_DIM)\n",
    "            \n",
    "            s_matrix = K.expand_dims(s_matrix)\n",
    "            \n",
    "            return s_matrix #Add one more dimension #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, EMB_DIM, 1)\n",
    "        elif self.interaction_mode==1:\n",
    "            raise NotImplementedError(\"interaction mode of layer SimilarityMatrix is not implemented\")\n",
    "    \"\"\"                  \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.interaction_mode==0:\n",
    "            return (input_shape[0][0], input_shape[0][1], self.query_max_term, self.snippet_max_term, 1)\n",
    "        elif self.interaction_mode==1:\n",
    "            return (input_shape[0][0], input_shape[0][1], self.query_max_term, self.snippet_max_term, input_shape[0][2]*input_shape[1][2]+1) \n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "     ---- Layers ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "#Embedding Layer\n",
    "embedding = Embedding(VOCAB_SIZE,EMB_DIM, name=\"embedding_layer\",weights=[emb_matrix], trainable=EMB_TRAINABLE)\n",
    "\n",
    "#S matrix ref in the paper\n",
    "similarity_matrix = SimilarityMatrix(MAX_Q_TERM, QUERY_CENTRIC_CONTEX, interaction_mode=S_MATRIX_MODE, name=\"query_snippet_similarity\")\n",
    "\n",
    "#transpose (None, QUERY_CENTRIC_CONTEX, EMB_DIM) => (None, EMB_DIM, QUERY_CENTRIC_CONTEX) \n",
    "transpose_layer = Lambda(lambda x:K.permute_dimensions(x,[0,1,2,4,3]), name=\"snippet_transpose\") \n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    " ---- Auxiliar Models ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "#Snippet single embedding transformation\n",
    "snippet_token_input = Input(shape = (MAX_Q_TERM, MAX_PASSAGES_PER_QUERY, QUERY_CENTRIC_CONTEX,), name = \"snippet_token\")\n",
    "snippet_emb = embedding(snippet_token_input)\n",
    "snippet_emb_transpose = transpose_layer(snippet_emb)\n",
    "snippet_emb_model = Model(inputs = [snippet_token_input], outputs=[snippet_emb_transpose], name = \"snippet_emb_model\")\n",
    "print(\"\\n\\nsnippet_emb_model summary\")\n",
    "snippet_emb_model.summary()\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "  ---- Input Network ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "if DEBUG:\n",
    "    query_token_input = Input(shape=(MAX_Q_TERM,), name=\"query_tokens\")\n",
    "\n",
    "\n",
    "    snippets_tokens_input = Input(shape = (MAX_Q_TERM, MAX_PASSAGES_PER_QUERY, QUERY_CENTRIC_CONTEX), name = \"snippet_tokens_ipmodel\") \n",
    "    \n",
    "    query_emb = embedding(query_token_input)\n",
    "\n",
    "    snippet_emb = embedding(snippets_tokens_input)\n",
    "    snippet_emb_transpose = transpose_layer(snippet_emb)\n",
    "    \n",
    "    sim_matrix_layer = similarity_matrix([query_emb,snippet_emb_transpose])\n",
    "    \n",
    "    \n",
    "    \n",
    "    input_model = Model(inputs = [query_token_input,snippets_tokens_input], outputs=[sim_matrix_layer], name=\"input_model\")\n",
    "    print(\"\\n\\ninput_model summary\")\n",
    "    input_model.summary()\n",
    "    \n",
    "    \n",
    "    print(\"\\nOutput tensor\",sim_matrix_layer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='measure_net'></a>\n",
    "## Measure Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cnn_extraction_model summary\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 13, 11, 256)       2560      \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 256)               0         \n",
      "=================================================================\n",
      "Total params: 2,560\n",
      "Trainable params: 2,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri (None, 3, 256)            2560      \n",
      "=================================================================\n",
      "Total params: 2,560\n",
      "Trainable params: 2,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    " ---- Auxiliar Models ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "#Exctrate high-level features from query and snippet interactions with CNN\n",
    "cnn_extraction_model = Sequential(name=\"cnn_extraction_model\")\n",
    "cnn_extraction_model.add(Conv2D(input_shape = (MAX_Q_TERM, QUERY_CENTRIC_CONTEX, S_MATRIX_3D_DIMENSION()), filters = CNN_FILTERS, kernel_size=CNN_KERNELS, activation=ACTIVATION_FUNCTION ))\n",
    "cnn_extraction_model.add(GlobalMaxPooling2D())\n",
    "print(\"\\n\\ncnn_extraction_model summary\")\n",
    "cnn_extraction_model.summary()\n",
    "\n",
    "\n",
    "td_cnn_extraction_model = Sequential(name=\"TD_cnn_extraction_model\")\n",
    "td_cnn_extraction_model.add(TimeDistributed(cnn_extraction_model, input_shape=(MAX_PASSAGES_PER_QUERY, MAX_Q_TERM, QUERY_CENTRIC_CONTEX, S_MATRIX_3D_DIMENSION())))\n",
    "td_cnn_extraction_model.summary()\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "     ---- Layers ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "#concatenation layer over the last dimension\n",
    "concat_snippet_position = Concatenate( name = \"concat_snippet_position\")\n",
    "\n",
    "#RNN using GRU units\n",
    "if USE_BIDIRECTIONAL:\n",
    "    rnn_instance = GRU(GRU_REPRESENTATION_DIM, activation=ACTIVATION_FUNCTION, name=\"aggregation_snippet_by_q_term\")\n",
    "    gru = Bidirectional(rnn_instance, merge_mode=\"concat\")\n",
    "else:\n",
    "    gru = GRU(GRU_REPRESENTATION_DIM, activation=ACTIVATION_FUNCTION, name=\"aggregation_snippet_by_q_term\")\n",
    "    \n",
    "#add dimension Layer\n",
    "add_passage_dim = Lambda(lambda x:K.expand_dims(x,axis=1), name=\"add_passage_dim\")#Reshape(target_shape=(1,GRU_REPRESENTATION_DIM))\n",
    "\n",
    "#add last dimension Layer\n",
    "add_dim = Lambda(lambda x:K.expand_dims(x), name=\"add_dim\")\n",
    "\n",
    "#reciprocal function\n",
    "reciprocal_f = Lambda(lambda x:1/(x+2), name=\"reciprocal_function\")\n",
    "\n",
    "#concatenation layer over second dimension (passage dimension)\n",
    "concat_representation = Concatenate(axis = 1,name = \"concat_representation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='measure_net'></a>\n",
    "## Aggregation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "  ---- Custom Layers ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "class TermGating(Layer):\n",
    "    \n",
    "    def __init__(self, activation=None, initializer='glorot_normal', regularizer=None):\n",
    "        super(TermGating, self).__init__()\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        input_layer_size = input_shape[1].value\n",
    "        aggreation_dimension = input_shape[-1].value\n",
    "        \n",
    "        #term gating W\n",
    "        self.We = self.add_variable(name = \"term_gating_We\",\n",
    "                                   shape = [input_layer_size,1],\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        #self.ones = K.constant(np.ones((aggreation_dimension,1)))\n",
    "        \n",
    "        super(TermGating, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        #broadcast = self.We * self.ones\n",
    "        \n",
    "        return K.sum(K.sum(x * self.We, axis = -1),  axis = -1) # Replace with K.sum of all elements?\n",
    "    \n",
    "\n",
    "\n",
    "class TermGatingDRMM(Layer):\n",
    "    \n",
    "    def __init__(self, embedding_dim = EMB_DIM, activation=None, initializer='glorot_normal', regularizer=None):\n",
    "        super(TermGatingDRMM, self).__init__()\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "        self.emb_dim = embedding_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        #term gating W\n",
    "        self.We = self.add_variable(name = \"term_gating_We\",\n",
    "                                   shape = [self.emb_dim,1],\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        #self.ones = K.constant(np.ones((aggreation_dimension,1)))\n",
    "        \n",
    "        super(TermGatingDRMM, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        query_embeddings = x[0] #(None, MAX_Q_TERM, EMB_SIZE)\n",
    "        snippet_representation_per_query = x[1] #(None, MAX_Q_TERM, BI_GRU_DIM)\n",
    "        \n",
    "        #compute gated weights\n",
    "        gated_logits = K.dot(query_embeddings, self.We) \n",
    "        gated_distribution = K.softmax(gated_logits)\n",
    "        \n",
    "        #apply the gated weights\n",
    "        #print(snippet_representation_per_query)\n",
    "        #print(gated_distribution)\n",
    "        \n",
    "        #normalization?\n",
    "        \n",
    "        return K.sum(K.sum(snippet_representation_per_query * gated_distribution, axis = -1),  axis = -1) # Replace with K.sum of all elements?\n",
    "\n",
    "if USE_BIDIRECTIONAL:\n",
    "    snippet_rnn_rep_dim = GRU_REPRESENTATION_DIM*2\n",
    "else:\n",
    "    snippet_rnn_rep_dim = GRU_REPRESENTATION_DIM\n",
    "\n",
    "class TermGatingDRMM_Projection(Layer):\n",
    "    \n",
    "    def __init__(self, embedding_dim = EMB_DIM, rnn_dim = snippet_rnn_rep_dim ,activation=None, initializer='glorot_normal', regularizer=None):\n",
    "        super(TermGatingDRMM_Projection, self).__init__()\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "        self.emb_dim = embedding_dim\n",
    "        self.rnn_dim = rnn_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        #term gating W\n",
    "        self.W_query = self.add_variable(name = \"term_gating_We\",\n",
    "                                   shape = [self.emb_dim,1],\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.W_snippets_projection_score = self.add_variable(name = \"snippets_projection_score\",\n",
    "                                   shape = [self.rnn_dim,1],\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        #self.ones = K.constant(np.ones((aggreation_dimension,1)))\n",
    "        \n",
    "        super(TermGatingDRMM_Projection, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        query_embeddings = x[0] #(None, MAX_Q_TERM, EMB_SIZE)\n",
    "        snippet_representation_per_query = x[1] #(None, MAX_Q_TERM, BI_GRU_DIM)\n",
    "        \n",
    "        #compute gated weights\n",
    "        gated_logits = K.squeeze(K.dot(query_embeddings, self.W_query), axis = -1 )\n",
    "        #print(gated_logits)\n",
    "        gated_distribution = K.softmax(gated_logits)\n",
    "        #print(gated_distribution)\n",
    "        #snippet projection\n",
    "        snippets_score = K.squeeze(K.dot(snippet_representation_per_query, self.W_snippets_projection_score), axis = -1)\n",
    "        #snippets_score_activation = tanh(snippets_score)\n",
    "        \n",
    "        weighted_score = snippets_score * gated_distribution\n",
    "        \n",
    "        return K.sum(weighted_score,  axis = -1) # Replace with K.sum of all elements?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='final_net'></a>\n",
    "## Final Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ds_query_tokens (InputLayer)    (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ds_snippet_tokens (InputLayer)  (None, 15, 3, 13)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     multiple             858359000   ds_query_tokens[0][0]            \n",
      "                                                                 ds_snippet_tokens[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "snippet_transpose (Lambda)      (None, 15, 3, 200, 1 0           embedding_layer[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "query_snippet_similarity (Simil (None, 15, 3, 15, 13 0           embedding_layer[1][0]            \n",
      "                                                                 snippet_transpose[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ds_snippet_position_tokens (Inp (None, 15, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "unstack_query_term (Lambda)     multiple             0           query_snippet_similarity[0][0]   \n",
      "                                                                 ds_snippet_position_tokens[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reciprocal_function (Lambda)    (None, 3)            0           unstack_query_term[1][0]         \n",
      "                                                                 unstack_query_term[1][1]         \n",
      "                                                                 unstack_query_term[1][2]         \n",
      "                                                                 unstack_query_term[1][3]         \n",
      "                                                                 unstack_query_term[1][4]         \n",
      "                                                                 unstack_query_term[1][5]         \n",
      "                                                                 unstack_query_term[1][6]         \n",
      "                                                                 unstack_query_term[1][7]         \n",
      "                                                                 unstack_query_term[1][8]         \n",
      "                                                                 unstack_query_term[1][9]         \n",
      "                                                                 unstack_query_term[1][10]        \n",
      "                                                                 unstack_query_term[1][11]        \n",
      "                                                                 unstack_query_term[1][12]        \n",
      "                                                                 unstack_query_term[1][13]        \n",
      "                                                                 unstack_query_term[1][14]        \n",
      "__________________________________________________________________________________________________\n",
      "TD_cnn_extraction_model (Sequen (None, 3, 256)       2560        unstack_query_term[0][0]         \n",
      "                                                                 unstack_query_term[0][1]         \n",
      "                                                                 unstack_query_term[0][2]         \n",
      "                                                                 unstack_query_term[0][3]         \n",
      "                                                                 unstack_query_term[0][4]         \n",
      "                                                                 unstack_query_term[0][5]         \n",
      "                                                                 unstack_query_term[0][6]         \n",
      "                                                                 unstack_query_term[0][7]         \n",
      "                                                                 unstack_query_term[0][8]         \n",
      "                                                                 unstack_query_term[0][9]         \n",
      "                                                                 unstack_query_term[0][10]        \n",
      "                                                                 unstack_query_term[0][11]        \n",
      "                                                                 unstack_query_term[0][12]        \n",
      "                                                                 unstack_query_term[0][13]        \n",
      "                                                                 unstack_query_term[0][14]        \n",
      "__________________________________________________________________________________________________\n",
      "add_dim (Lambda)                (None, 3, 1)         0           reciprocal_function[0][0]        \n",
      "                                                                 reciprocal_function[1][0]        \n",
      "                                                                 reciprocal_function[2][0]        \n",
      "                                                                 reciprocal_function[3][0]        \n",
      "                                                                 reciprocal_function[4][0]        \n",
      "                                                                 reciprocal_function[5][0]        \n",
      "                                                                 reciprocal_function[6][0]        \n",
      "                                                                 reciprocal_function[7][0]        \n",
      "                                                                 reciprocal_function[8][0]        \n",
      "                                                                 reciprocal_function[9][0]        \n",
      "                                                                 reciprocal_function[10][0]       \n",
      "                                                                 reciprocal_function[11][0]       \n",
      "                                                                 reciprocal_function[12][0]       \n",
      "                                                                 reciprocal_function[13][0]       \n",
      "                                                                 reciprocal_function[14][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concat_snippet_position (Concat (None, 3, 257)       0           TD_cnn_extraction_model[1][0]    \n",
      "                                                                 add_dim[0][0]                    \n",
      "                                                                 TD_cnn_extraction_model[2][0]    \n",
      "                                                                 add_dim[1][0]                    \n",
      "                                                                 TD_cnn_extraction_model[3][0]    \n",
      "                                                                 add_dim[2][0]                    \n",
      "                                                                 TD_cnn_extraction_model[4][0]    \n",
      "                                                                 add_dim[3][0]                    \n",
      "                                                                 TD_cnn_extraction_model[5][0]    \n",
      "                                                                 add_dim[4][0]                    \n",
      "                                                                 TD_cnn_extraction_model[6][0]    \n",
      "                                                                 add_dim[5][0]                    \n",
      "                                                                 TD_cnn_extraction_model[7][0]    \n",
      "                                                                 add_dim[6][0]                    \n",
      "                                                                 TD_cnn_extraction_model[8][0]    \n",
      "                                                                 add_dim[7][0]                    \n",
      "                                                                 TD_cnn_extraction_model[9][0]    \n",
      "                                                                 add_dim[8][0]                    \n",
      "                                                                 TD_cnn_extraction_model[10][0]   \n",
      "                                                                 add_dim[9][0]                    \n",
      "                                                                 TD_cnn_extraction_model[11][0]   \n",
      "                                                                 add_dim[10][0]                   \n",
      "                                                                 TD_cnn_extraction_model[12][0]   \n",
      "                                                                 add_dim[11][0]                   \n",
      "                                                                 TD_cnn_extraction_model[13][0]   \n",
      "                                                                 add_dim[12][0]                   \n",
      "                                                                 TD_cnn_extraction_model[14][0]   \n",
      "                                                                 add_dim[13][0]                   \n",
      "                                                                 TD_cnn_extraction_model[15][0]   \n",
      "                                                                 add_dim[14][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "aggregation_snippet_by_q_term ( (None, 56)           52752       concat_snippet_position[0][0]    \n",
      "                                                                 concat_snippet_position[1][0]    \n",
      "                                                                 concat_snippet_position[2][0]    \n",
      "                                                                 concat_snippet_position[3][0]    \n",
      "                                                                 concat_snippet_position[4][0]    \n",
      "                                                                 concat_snippet_position[5][0]    \n",
      "                                                                 concat_snippet_position[6][0]    \n",
      "                                                                 concat_snippet_position[7][0]    \n",
      "                                                                 concat_snippet_position[8][0]    \n",
      "                                                                 concat_snippet_position[9][0]    \n",
      "                                                                 concat_snippet_position[10][0]   \n",
      "                                                                 concat_snippet_position[11][0]   \n",
      "                                                                 concat_snippet_position[12][0]   \n",
      "                                                                 concat_snippet_position[13][0]   \n",
      "                                                                 concat_snippet_position[14][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_passage_dim (Lambda)        (None, 1, 56)        0           aggregation_snippet_by_q_term[0][\n",
      "                                                                 aggregation_snippet_by_q_term[1][\n",
      "                                                                 aggregation_snippet_by_q_term[2][\n",
      "                                                                 aggregation_snippet_by_q_term[3][\n",
      "                                                                 aggregation_snippet_by_q_term[4][\n",
      "                                                                 aggregation_snippet_by_q_term[5][\n",
      "                                                                 aggregation_snippet_by_q_term[6][\n",
      "                                                                 aggregation_snippet_by_q_term[7][\n",
      "                                                                 aggregation_snippet_by_q_term[8][\n",
      "                                                                 aggregation_snippet_by_q_term[9][\n",
      "                                                                 aggregation_snippet_by_q_term[10]\n",
      "                                                                 aggregation_snippet_by_q_term[11]\n",
      "                                                                 aggregation_snippet_by_q_term[12]\n",
      "                                                                 aggregation_snippet_by_q_term[13]\n",
      "                                                                 aggregation_snippet_by_q_term[14]\n",
      "__________________________________________________________________________________________________\n",
      "concat_representation (Concaten (None, 15, 56)       0           add_passage_dim[0][0]            \n",
      "                                                                 add_passage_dim[1][0]            \n",
      "                                                                 add_passage_dim[2][0]            \n",
      "                                                                 add_passage_dim[3][0]            \n",
      "                                                                 add_passage_dim[4][0]            \n",
      "                                                                 add_passage_dim[5][0]            \n",
      "                                                                 add_passage_dim[6][0]            \n",
      "                                                                 add_passage_dim[7][0]            \n",
      "                                                                 add_passage_dim[8][0]            \n",
      "                                                                 add_passage_dim[9][0]            \n",
      "                                                                 add_passage_dim[10][0]           \n",
      "                                                                 add_passage_dim[11][0]           \n",
      "                                                                 add_passage_dim[12][0]           \n",
      "                                                                 add_passage_dim[13][0]           \n",
      "                                                                 add_passage_dim[14][0]           \n",
      "__________________________________________________________________________________________________\n",
      "term_gating_drmm__projection (T (None,)              256         embedding_layer[1][0]            \n",
      "                                                                 concat_representation[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 858,414,568\n",
      "Trainable params: 55,568\n",
      "Non-trainable params: 858,359,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "  ---- Final Network ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "query_token_input = Input(shape=(MAX_Q_TERM,), name=\"ds_query_tokens\")\n",
    "doc_score_snippet_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), name = \"ds_snippet_tokens\")\n",
    "doc_score_snippet_position_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY), name = \"ds_snippet_position_tokens\")\n",
    "\n",
    "\n",
    "unstack_by_q_term = Lambda(lambda x:unstack(x,axis=1), name=\"unstack_query_term\")\n",
    "\n",
    "#doc_score_snippet_by_q_term = unstack_by_q_term(doc_score_snippet_input)\n",
    "#doc_score_snippet_position_by_q_term = unstack_by_q_term(doc_score_snippet_position_input)\n",
    "\n",
    "#INPUT in token format\n",
    "#query_token_input = Input(shape=(MAX_Q_TERM,), name=\"query_tokens\")\n",
    "#snippets_tokens_input = [Input(shape = (MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), name = \"snippet_tokens_\"+str(q)) for q in range(MAX_Q_TERM)]\n",
    "#inputs_contex_position = [Input(shape = (MAX_PASSAGES_PER_QUERY,), name = \"q_context_position_\"+str(q)) for q in range(MAX_Q_TERM)]\n",
    "\n",
    "query_emb = embedding(query_token_input)\n",
    "\n",
    "doc_score_snippet_emb = embedding(doc_score_snippet_input)\n",
    "doc_score_snippet_emb_transpose = transpose_layer(doc_score_snippet_emb)\n",
    "\n",
    "query_snippets_s_matrix = similarity_matrix([query_emb,doc_score_snippet_emb_transpose])\n",
    "\n",
    "list_of_s_matrix_by_q_term = unstack_by_q_term(query_snippets_s_matrix)\n",
    "list_of_snippet_postion_by_q_term = unstack_by_q_term(doc_score_snippet_position_input)\n",
    "\n",
    "relevance_representation = []\n",
    "for i in range(MAX_Q_TERM):\n",
    "    \n",
    "    snippet_relative_position = reciprocal_f(list_of_snippet_postion_by_q_term[i])\n",
    "    \n",
    "    local_relevance = td_cnn_extraction_model(list_of_s_matrix_by_q_term[i])\n",
    "    \n",
    "    local_relevance_position = concat_snippet_position([local_relevance,add_dim(snippet_relative_position)])\n",
    "    \n",
    "    relevance_representation.append(add_passage_dim(gru(local_relevance_position)))\n",
    "\n",
    "concat_relevance = concat_representation(relevance_representation)\n",
    "\n",
    "if TERM_GATING_MODE==0:\n",
    "    term_gating = TermGating()\n",
    "    document_score = term_gating(concat_relevance)\n",
    "    \n",
    "elif TERM_GATING_MODE==1:\n",
    "    term_gating = TermGatingDRMM()\n",
    "    document_score = term_gating([query_emb,concat_relevance])\n",
    "\n",
    "elif TERM_GATING_MODE==2:\n",
    "    term_gating = TermGatingDRMM_Projection()\n",
    "    document_score = term_gating([query_emb,concat_relevance])\n",
    "\n",
    "\n",
    "document_score_model = Model(inputs = [query_token_input, doc_score_snippet_input, doc_score_snippet_position_input], outputs = [document_score], name=\"query_document_score\")\n",
    "document_score_model.summary()      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL Trainable arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dr_query_tokens (InputLayer)    (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_snippet_tokens (InputL (None, 15, 3, 13)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_snippet_position_token (None, 15, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_snippet_tokens (InputL (None, 15, 3, 13)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_snippet_position_token (None, 15, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "query_document_score (Model)    (None,)              858414568   dr_query_tokens[0][0]            \n",
      "                                                                 positive_snippet_tokens[0][0]    \n",
      "                                                                 positive_snippet_position_tokens[\n",
      "                                                                 dr_query_tokens[0][0]            \n",
      "                                                                 negative_snippet_tokens[0][0]    \n",
      "                                                                 negative_snippet_position_tokens[\n",
      "==================================================================================================\n",
      "Total params: 858,414,568\n",
      "Trainable params: 55,568\n",
      "Non-trainable params: 858,359,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "query_token_input = Input(shape=(MAX_Q_TERM,), name=\"dr_query_tokens\")\n",
    "positive_snippet_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), name = \"positive_snippet_tokens\")\n",
    "positive_snippet_position_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY), name = \"positive_snippet_position_tokens\")\n",
    "negative_snippet_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), name = \"negative_snippet_tokens\")\n",
    "negative_snippet_position_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY), name = \"negative_snippet_position_tokens\")\n",
    "\n",
    "positive_documents_score = document_score_model([query_token_input, positive_snippet_input, positive_snippet_position_input])\n",
    "negative_documents_score = document_score_model([query_token_input, negative_snippet_input, negative_snippet_position_input])\n",
    "\n",
    "#stack_socres = stack_scores_layer([positive_documents_score,negative_documents_score])\n",
    "\n",
    "\n",
    "\n",
    "#pairwise_loss_layer = Lambda(pairwise_hinge_loss, name=\"pairwise_hinge\")\n",
    "#pairwise_loss = pairwise_loss_layer([positive_documents_score,negative_documents_score])\n",
    "\n",
    "\n",
    "\n",
    "inputs = [query_token_input, positive_snippet_input, positive_snippet_position_input, negative_snippet_input, negative_snippet_position_input]\n",
    "\n",
    "deepRank_model = Model(inputs = inputs, outputs = [positive_documents_score, negative_documents_score], name=\"deep_rank\")\n",
    "\n",
    "\n",
    "p_loss = K.mean(K.maximum(0.0, 1.0 - positive_documents_score + negative_documents_score))\n",
    "\n",
    "deepRank_model.add_loss(p_loss)\n",
    "\n",
    "deepRank_model.summary() \n",
    "#m.predict([Q, Q_t1_passage, Q_t2_passage, Q_t3_passage, Q_t1_passage_pos, Q_t2_passage_pos, Q_t3_passage_pos])\n",
    "\n",
    "#deepRank_model.compile(loss=pairwise_hinge_loss, optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITE_GRAPH = False\n",
    "if WRITE_GRAPH:\n",
    "    from tensorflow.summary import FileWriter\n",
    "\n",
    "    graph = K.get_session().graph\n",
    "     # Your model implementation\n",
    "    #with graph.as_default():\n",
    "      # compile method actually creates the model in the graph.\n",
    "      #deepRank_model.compile(loss=identity_loss, optimizer='adam', metrics=['accuracy'])\n",
    "    writer = FileWriter(logdir='tensorboard/deepRank', graph=graph)\n",
    "    writer.flush()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_score.predict([Q, Q_t1_passage, Q_t2_passage, Q_t3_passage, Q_t1_passage_pos, Q_t2_passage_pos, Q_t3_passage_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open /backup/results/fast_method_relevant_results/train_data_deep_models.tar.gz\n"
     ]
    }
   ],
   "source": [
    "path_dl_train = \"/backup/results/fast_method_relevant_results/train_data_deep_models.tar.gz\"\n",
    "\n",
    "\n",
    "\n",
    "tar = tarfile.open(path_dl_train)\n",
    "#open\n",
    "print(\"Open\",path_dl_train)\n",
    "m = tar.getmembers()[0]\n",
    "f = tar.extractfile(m)\n",
    "train_articles_collection = pickle.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrainDataGenerator(object):\n",
    "    def __init__(self, article_collection, tokenizer, batch_queries_size):\n",
    "        \n",
    "        self.batch_size = batch_queries_size\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.train_data = article_collection[\"bioasq_data\"]\n",
    "        self.articles = article_collection[\"collection\"]\n",
    "        \n",
    "        #cumulative calculation of the batch\n",
    "        self.num_steps = 0\n",
    "        counter = 0\n",
    "        for sample in self.train_data:\n",
    "            counter += len(sample[\"positive_pmid\"])\n",
    "            \n",
    "            if counter >= self.batch_size:\n",
    "                self.num_steps += 1\n",
    "                counter = 0\n",
    "        \n",
    "        if counter != 0:\n",
    "            self.num_steps += 1\n",
    "        \n",
    "        self.last_query_index = len(self.train_data)-1\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        \n",
    "        query = []\n",
    "        query_positive_doc = []\n",
    "        query_positive_doc_position = []\n",
    "        query_negative_doc = []\n",
    "        query_negative_doc_position = []\n",
    "        \n",
    "        while True:\n",
    "            for n_queries,query_data in enumerate(self.train_data):\n",
    "\n",
    "                #list with negatives documents returned by the fast method\n",
    "                negative_pmid_docs = query_data[\"negative_pmid\"]\n",
    "\n",
    "                tokenized_query = query_data[\"query\"]\n",
    "\n",
    "                for doc_pmid in query_data[\"positive_pmid\"]:\n",
    "                    #positive\n",
    "                    tokenized_positive_doc = self.articles[doc_pmid]\n",
    "                    positive_snippets, positive_snippets_position = self.__snippet_interaction(tokenized_query, tokenized_positive_doc)\n",
    "\n",
    "                    #negative\n",
    "                    random_ind = bisect(query_data[\"negative_cumulative_prob\"],random.random())\n",
    "                    random_negative_doc_pmid = query_data[\"negative_pmid\"][random_ind]\n",
    "                    #print(self.__get_article(random_negative_doc_pmid))\n",
    "                    tokenized_negative_doc = self.articles[random_negative_doc_pmid]\n",
    "                    negative_snippets, negative_snippets_position = self.__snippet_interaction(tokenized_query, tokenized_negative_doc)\n",
    "\n",
    "                    ### add ###\n",
    "\n",
    "                    #not efficient\n",
    "                    query.append(tokenized_query)\n",
    "\n",
    "                    #positive doc\n",
    "                    query_positive_doc.append(positive_snippets)\n",
    "                    query_positive_doc_position.append(positive_snippets_position)\n",
    "\n",
    "                    #negative doc\n",
    "                    query_negative_doc.append(negative_snippets)\n",
    "                    query_negative_doc_position.append(negative_snippets_position)\n",
    "\n",
    "\n",
    "                if len(query)>=self.batch_size or n_queries==self.last_query_index:\n",
    "                    #missing fill the gap for the missing query_terms\n",
    "                    X = [np.array(query), np.array(query_positive_doc), np.array(query_positive_doc_position), np.array(query_negative_doc), np.array(query_negative_doc_position)]\n",
    "                    Y = [np.zeros((len(query))),np.zeros((len(query)))]\n",
    "                    yield X, Y\n",
    "\n",
    "\n",
    "                    #reset\n",
    "                    query = []\n",
    "                    query_positive_doc = []\n",
    "                    query_positive_doc_position = []\n",
    "                    query_negative_doc = []\n",
    "                    query_negative_doc_position = []\n",
    "                \n",
    "    def __snippet_interaction(self, tokenized_query, tokenized_doc, snippet_length=QUERY_CENTRIC_CONTEX):\n",
    "        \n",
    "        snippets = []\n",
    "        snippets_position = [] \n",
    "\n",
    "        half_size = snippet_length//2\n",
    "        \n",
    "        #O(n^2) complexity, probably can do better with better data struct TODO see if is worthit\n",
    "        for query_token in tokenized_query:\n",
    "            \n",
    "            snippets_per_token = []\n",
    "            snippets_per_token_position = []\n",
    "            \n",
    "            if query_token != 0: #jump padded token\n",
    "            \n",
    "                for i,doc_token in enumerate(tokenized_doc):\n",
    "\n",
    "                    if doc_token==query_token:\n",
    "\n",
    "                        lower_index = i-half_size\n",
    "                        lower_index = max(0,lower_index)\n",
    "\n",
    "                        higher_index = i+half_size\n",
    "                        higher_index = min(len(tokenized_doc),higher_index)\n",
    "\n",
    "                        snippets_per_token.append(tokenized_doc[lower_index:higher_index])\n",
    "                        snippets_per_token_position.append(i)\n",
    "            \n",
    "            if len(snippets_per_token)==0:\n",
    "                snippets.append(np.zeros((MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), dtype=np.int32))\n",
    "                snippets_position.append(np.zeros((MAX_PASSAGES_PER_QUERY), dtype=np.int32)+SNIPPET_POSITION_PADDING_VALUE)\n",
    "                continue\n",
    "                \n",
    "            max_snippets_len = min(MAX_PASSAGES_PER_QUERY, len(snippets_per_token))\n",
    "            \n",
    "            ### snippets in matrix format\n",
    "            #pad\n",
    "            snippets_per_token = pad_sequences(snippets_per_token, maxlen = QUERY_CENTRIC_CONTEX, padding=\"post\")\n",
    "            #fill the gaps\n",
    "            _temp = np.zeros((MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), dtype=np.int32)\n",
    "            _temp[:max_snippets_len] = snippets_per_token[:max_snippets_len]\n",
    "            snippets.append(_temp)\n",
    "            \n",
    "            ### snippets_position in matrix format\n",
    "            #pad\n",
    "            snippets_per_token_position = pad_sequences([snippets_per_token_position], maxlen = MAX_PASSAGES_PER_QUERY, padding=\"post\", value=SNIPPET_POSITION_PADDING_VALUE)[0]\n",
    "            snippets_position.append(snippets_per_token_position)\n",
    "            \n",
    "        return snippets, snippets_position\n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,Y = next(data_generator)\n",
    "\n",
    "TESTE = False\n",
    "if TESTE:\n",
    "    data_generator = TrainDataGenerator(train_articles_collection, tk, 1)\n",
    "    data_generator = iter(data_generator)\n",
    "    X,Y = next(data_generator)\n",
    "    \n",
    "    print(X[0].shape)\n",
    "    print(X[1].shape)\n",
    "    print(X[2].shape)\n",
    "    print(X[3].shape)\n",
    "    print(X[4].shape)\n",
    "    print(Y[0].shape)\n",
    "    print(Y[1].shape)\n",
    "    deepRank_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"query_document_score\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"query_document_score\" during training.\n",
      "WARNING:tensorflow:Output \"query_document_score\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"query_document_score\" during training.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam,  Adadelta\n",
    "\n",
    "#sgd = SGD(lr=0.001)\n",
    "#adam = Adam(lr=0.001)\n",
    "adadelta = Adadelta(lr=2)\n",
    "\n",
    "\n",
    "deepRank_model.compile( optimizer=adadelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoach: 0 | avg loss: 0.7224702 | max loss: 1.0177375 | min loss: 0.44888818 0.448888188\n",
      "Epoach: 1 | avg loss: 0.5898626 | max loss: 0.9366206 | min loss: 0.3592888: 0.359288888\n",
      "Epoach: 2 | avg loss: 0.55806553 | max loss: 0.84803396 | min loss: 0.373519420.373519422\n",
      "Epoach: 3 | avg loss: 0.55275893 | max loss: 0.8559794 | min loss: 0.31425488.31425488882\n",
      "Epoach: 4 | avg loss: 0.5409845 | max loss: 0.83039 | min loss: 0.35330114: 0.35330114923\n",
      "Epoach: 5 | avg loss: 0.5432016 | max loss: 0.809358 | min loss: 0.3603817s: 0.3603817\n",
      "Epoach: 6 | avg loss: 0.5261551 | max loss: 0.8495721 | min loss: 0.34632874: 0.346328744\n",
      "Epoach: 7 | avg loss: 0.5296195 | max loss: 0.8167981 | min loss: 0.34094095: 0.340940955\n",
      "Epoach: 8 | avg loss: 0.5222418 | max loss: 0.8210781 | min loss: 0.3166805: 0.316680555\n",
      "Epoach: 9 | avg loss: 0.5189543 | max loss: 0.753664 | min loss: 0.30178967: 0.301789671\n",
      "Epoach: 10 | avg loss: 0.5127486 | max loss: 0.7321558 | min loss: 0.302211820.302211822\n",
      "Epoach: 11 | avg loss: 0.5126846 | max loss: 0.7558213 | min loss: 0.345866680.3458666888\n",
      "Epoach: 12 | avg loss: 0.5108083 | max loss: 0.76863295 | min loss: 0.3220913: 0.32209134\n",
      "Epoach: 13 | avg loss: 0.5161023 | max loss: 0.75324243 | min loss: 0.33475178 0.33475178\n",
      "Step: 17 | loss: 0.4068142 | current max loss: 0.6772238 | current min loss: 0.366094833\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8cb6d1911937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss_per_epoach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepRank_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"| loss:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_per_epoach\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"| current max loss:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_per_epoach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"| current min loss:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_per_epoach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen = TrainDataGenerator(train_articles_collection, tk, 256)\n",
    "\n",
    "gen_iter = iter(gen)\n",
    "\n",
    "# SE DER ERRO POR CAUSA DA SHAPE A 1 AMOSTRA DAS QUERIES NAO TEM PADDING BURRO\n",
    "# MUDA ISSO\n",
    "loss = []\n",
    "\n",
    "for epoach in range(50):\n",
    "    loss_per_epoach = []\n",
    "    for step in range(len(gen)):\n",
    "        X,Y = next(gen_iter)\n",
    "        loss_per_epoach.append(deepRank_model.train_on_batch(X))\n",
    "        \n",
    "        print(\"Step:\",step,\"| loss:\",loss_per_epoach[-1],\"| current max loss:\",np.max(loss_per_epoach),\"| current min loss:\",np.min(loss_per_epoach),end=\"\\r\")\n",
    "    \n",
    "    loss.append(loss_per_epoach)\n",
    "    print(\"\",end=\"\\r\")#clear the line\n",
    "    print(\"Epoach:\",epoach,\"| avg loss:\",np.mean(loss[-1]),\"| max loss:\",np.max(loss[-1]),\"| min loss:\",np.min(loss[-1]))\n",
    "#deepRank_model.fit_generator(gen_iter, steps_per_epoch=len(gen), verbose=1, epochs=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = TrainDataGenerator(train_articles_collection, tk, 1)\n",
    "data_generator = iter(data_generator)\n",
    "X,Y = next(data_generator)\n",
    "X,Y = next(data_generator)\n",
    "X,Y = next(data_generator)\n",
    "X,Y = next(data_generator)\n",
    "pos, neg = deepRank_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4.5924015, 3.160991 , 4.8889594, 5.548592 , 4.4002953, 4.5258055,\n",
       "        4.139955 ], dtype=float32),\n",
       " array([4.383961 , 3.1046154, 4.238362 , 2.9524093, 3.2229958, 2.530694 ,\n",
       "        2.618923 ], dtype=float32)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepRank_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test (validation) data generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open /backup/results/fast_method_relevant_results/test_data_deep_models.tar.gz\n"
     ]
    }
   ],
   "source": [
    "path_dl_test = \"/backup/results/fast_method_relevant_results/test_data_deep_models.tar.gz\"\n",
    "\n",
    "\n",
    "\n",
    "tar = tarfile.open(path_dl_test)\n",
    "#open\n",
    "print(\"Open\",path_dl_test)\n",
    "m = tar.getmembers()[0]\n",
    "f = tar.extractfile(m)\n",
    "test_articles_collection = pickle.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TestDataGenerator(object):\n",
    "    def __init__(self, article_collection, tokenizer, article_map = article_map):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.test_data = article_collection[\"bioasq_data\"] \n",
    "        self.articles = article_collection[\"collection\"]\n",
    "        \n",
    "        self.num_steps = len(self.test_data)\n",
    "        \n",
    "        self.article_map = article_map\n",
    "    \n",
    "    def __get_article(self, pmid):\n",
    "        return self.article_map(self.articles[pmid])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        \n",
    "        query = []\n",
    "        query_doc = []\n",
    "        query_doc_position = []\n",
    "        \n",
    "\n",
    "        for query_data in self.test_data:\n",
    "\n",
    "            #tokenized_query = self.tokenizer.texts_to_sequences([query_data[\"query\"]])[0]\n",
    "            tokenized_query = query_data[\"query\"]\n",
    "            #manualy remove the stopwords\n",
    "            #tokenized_query = [ token for token in tokenized_query if token not in biomedical_stop_words_tokens]\n",
    "\n",
    "            #tokenized_query = pad_sequences([tokenized_query], maxlen = MAX_Q_TERM, padding=\"post\")[0]\n",
    "\n",
    "            for doc_pmid in query_data[\"documents\"]:\n",
    "                #positive\n",
    "\n",
    "                tokenized_doc = self.articles[doc_pmid]\n",
    "                doc_snippets, doc_snippets_position = self.__snippet_interaction(tokenized_query, tokenized_doc)\n",
    "\n",
    "                ### add ###\n",
    "\n",
    "                query.append(tokenized_query)\n",
    "\n",
    "                #positive doc\n",
    "                query_doc.append(doc_snippets)\n",
    "                query_doc_position.append(doc_snippets_position)\n",
    "\n",
    "\n",
    "            #missing fill the gap for the missing query_terms\n",
    "\n",
    "            X = [np.array(query), np.array(query_doc), np.array(query_doc_position)]\n",
    "\n",
    "            yield X\n",
    "\n",
    "            #reset\n",
    "            query = []\n",
    "            query_doc = []\n",
    "            query_doc_position = []\n",
    "\n",
    "                \n",
    "    def __snippet_interaction(self, tokenized_query, tokenized_doc, snippet_length=QUERY_CENTRIC_CONTEX):\n",
    "        \n",
    "        snippets = []\n",
    "        snippets_position = [] \n",
    "\n",
    "        half_size = snippet_length//2\n",
    "        \n",
    "        #O(n^2) complexity, probably can do better with better data struct TODO see if is worthit\n",
    "        for query_token in tokenized_query:\n",
    "            \n",
    "            snippets_per_token = []\n",
    "            snippets_per_token_position = []\n",
    "            \n",
    "            if query_token != 0: #jump padded token\n",
    "                \n",
    "                for i,doc_token in enumerate(tokenized_doc):\n",
    "\n",
    "                    if doc_token==query_token:\n",
    "\n",
    "                        lower_index = i-half_size\n",
    "                        lower_index = max(0,lower_index)\n",
    "\n",
    "                        higher_index = i+half_size\n",
    "                        higher_index = min(len(tokenized_doc),higher_index)\n",
    "\n",
    "                        snippets_per_token.append(tokenized_doc[lower_index:higher_index])\n",
    "                        snippets_per_token_position.append(i)\n",
    "\n",
    "            if len(snippets_per_token)==0:\n",
    "                snippets.append(np.zeros((MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), dtype=np.int32))\n",
    "                snippets_position.append(np.zeros((MAX_PASSAGES_PER_QUERY), dtype=np.int32)+SNIPPET_POSITION_PADDING_VALUE)\n",
    "                continue\n",
    "                \n",
    "            max_snippets_len = min(MAX_PASSAGES_PER_QUERY, len(snippets_per_token))\n",
    "            \n",
    "            ### snippets in matrix format\n",
    "            #pad\n",
    "            snippets_per_token = pad_sequences(snippets_per_token, maxlen = QUERY_CENTRIC_CONTEX, padding=\"post\")\n",
    "            #fill the gaps\n",
    "            _temp = np.zeros((MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), dtype=np.int32)\n",
    "            _temp[:max_snippets_len] = snippets_per_token[:max_snippets_len]\n",
    "            snippets.append(_temp)\n",
    "            \n",
    "            ### snippets_position in matrix format\n",
    "            #pad\n",
    "            snippets_per_token_position = pad_sequences([snippets_per_token_position], maxlen = MAX_PASSAGES_PER_QUERY, padding=\"post\",value=SNIPPET_POSITION_PADDING_VALUE)[0]\n",
    "            snippets_position.append(snippets_per_token_position)\n",
    "            \n",
    "        return snippets, snippets_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save query results: 2183\r"
     ]
    }
   ],
   "source": [
    "\n",
    "data_generator = TestDataGenerator(train_articles_collection, tk)\n",
    "data_generator = iter(data_generator)\n",
    "\n",
    "query_results = {}\n",
    "\n",
    "for i,X in enumerate(data_generator):\n",
    "    print(\"Predict query:\",i,end=\"\\r\")\n",
    "    deep_ranking = document_score_model.predict(X)\n",
    "    bm25_results = train_articles_collection[\"bioasq_data\"][i][\"documents\"]\n",
    "    deep_ranking_pmid = list(zip(bm25_results,deep_ranking.tolist()))\n",
    "    deep_ranking_pmid.sort(key=lambda x:-x[1])\n",
    "    query_results[train_articles_collection[\"bioasq_data\"][i][\"id\"]] = deep_ranking_pmid\n",
    "    print(\"save query results:\",i,end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = \"/backup/results/deep_rank\"\n",
    "path_save = os.path.join(path_save, \"deep_rank_v1_train_data.p\")\n",
    "\n",
    "with open(path_save, \"wb\") as f:\n",
    "    pickle.dump(query_results,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_to_test_index = 0\n",
    "\n",
    "data_generator = TestDataGenerator(test_articles_collection, tk)\n",
    "data_generator = iter(data_generator)\n",
    "for _ in range(query_to_test_index+1):\n",
    "    X = next(data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_ranking = document_score_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_results = test_articles_collection[\"bioasq_data\"][query_to_test_index][\"documents\"]\n",
    "positive_docs = test_articles_collection[\"bioasq_data\"][query_to_test_index][\"positive_pmid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_ranking_pmid = list(zip(bm25_results,re_ranking.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_ranking_pmid.sort(key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('21060967', 7.931817054748535),\n",
       " ('29597032', 7.902041435241699),\n",
       " ('30284483', 7.8478264808654785),\n",
       " ('29947303', 7.842199802398682),\n",
       " ('24794627', 7.818233013153076),\n",
       " ('30114722', 7.759010314941406),\n",
       " ('30251567', 7.714381217956543),\n",
       " ('24469711', 7.696432590484619),\n",
       " ('29433332', 7.648404598236084),\n",
       " ('29990710', 7.3919854164123535)]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_ranking_pmid[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13502,    43,   478,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_articles_collection[\"bioasq_data\"][query_to_test_index][\"query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['24554704',\n",
       " '24784583',\n",
       " '24577791',\n",
       " '23197849',\n",
       " '24035588',\n",
       " '21060967',\n",
       " '25479728',\n",
       " '21755313',\n",
       " '24469711',\n",
       " '22512788',\n",
       " '24911883',\n",
       " '24794627',\n",
       " '21464439',\n",
       " '25059784']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '21060967', 7.931817054748535), (4, '24794627', 7.818233013153076), (7, '24469711', 7.696432590484619), (16, '24554704', 7.1962456703186035), (18, '23197849', 7.1423821449279785), (23, '24577791', 7.079183578491211), (35, '25059784', 6.679515361785889), (41, '25479728', 6.609160423278809), (44, '24784583', 6.542194366455078), (50, '21755313', 6.423967361450195), (67, '21464439', 6.2153167724609375), (68, '24035588', 6.175839424133301), (110, '22512788', 5.816000938415527), (541, '24911883', 5.157998085021973)]\n",
      "[(3, '23197849'), (4, '21755313'), (6, '25479728'), (7, '24784583'), (8, '24577791'), (9, '24035588'), (15, '22512788'), (17, '24911883'), (19, '21464439'), (22, '24794627'), (126, '25059784'), (134, '24554704'), (466, '21060967'), (2793, '24469711')]\n"
     ]
    }
   ],
   "source": [
    "positive_docs_ranked = []\n",
    "for i,result in enumerate(re_ranking_pmid):\n",
    "    if result[0] in set(positive_docs):\n",
    "        positive_docs_ranked.append((i,result[0],result[1]))\n",
    "        \n",
    "true_ranked = []\n",
    "for i,pmid in enumerate(bm25_results):\n",
    "    if pmid in set(positive_docs):\n",
    "        true_ranked.append((i,pmid))\n",
    "\n",
    "print(positive_docs_ranked)\n",
    "print(true_ranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with train set, check overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokens = np.array([X[0][0]])\n",
    "snippet_list = np.array([X[1][0]])\n",
    "\n",
    "query_tokens = X[0][:2]\n",
    "snippet_list = X[1][:2]\n",
    "\n",
    "model_input = [query_tokens, snippet_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 15)\n",
      "(2, 15, 3, 15)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(query_tokens.shape)\n",
    "print(snippet_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 15, 3, 15, 15, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = input_model.predict(model_input)\n",
    "np.array(matrix).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[0][4][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(matrix[0][7][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  165, 13502,    26,    61,     8,     1,    43,     2,   478],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  363,     5,  2386,    97,  7598,   774,    32, 13502,    18,\n",
       "          15,     7, 11695,   117,    17,     0], dtype=int32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_list[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05703647,  0.19594026,  0.03365219,  0.15514491,  0.00540348,\n",
       "       -0.02335026, -0.06095085,  0.0226689 , -0.05668721,  0.01571985,\n",
       "       -0.09896637,  0.13836679,  0.02710932,  0.06420047, -0.03692323,\n",
       "        0.03899341,  0.00553868, -0.08639584, -0.05358738, -0.02609682,\n",
       "        0.06495432, -0.00129713, -0.01882407, -0.10850747, -0.02421302,\n",
       "        0.05556208,  0.00291283, -0.04882976,  0.01770345,  0.0035051 ,\n",
       "        0.07192209, -0.00432884, -0.15161929, -0.07024549, -0.04793473,\n",
       "        0.01823143,  0.10337584, -0.04076301,  0.01026187,  0.12004871,\n",
       "        0.03939956, -0.03548966, -0.10689223, -0.16337523,  0.10883316,\n",
       "        0.01135785,  0.03041399,  0.06011688, -0.09919181,  0.01741308,\n",
       "       -0.04328503, -0.00256405, -0.11370766,  0.0522779 ,  0.0702537 ,\n",
       "        0.01021139,  0.06773005,  0.01114117, -0.05878652,  0.0720681 ,\n",
       "        0.05551391,  0.08731035,  0.07339004,  0.0031227 ,  0.10792159,\n",
       "        0.12050318, -0.05851915, -0.08350374, -0.03341928,  0.12355518,\n",
       "        0.11631501,  0.09690028, -0.02127477,  0.05462002, -0.03942551,\n",
       "        0.0049153 , -0.12599072,  0.04783105, -0.00842254,  0.06550868,\n",
       "       -0.06309314, -0.04500391, -0.08562729,  0.0129618 ,  0.05385976,\n",
       "        0.004466  ,  0.0030678 ,  0.02940105, -0.01520511,  0.04611286,\n",
       "        0.03223389,  0.01480899,  0.06665637, -0.00282597, -0.14461097,\n",
       "       -0.16931795,  0.01149627,  0.09099851, -0.06041984, -0.04964583,\n",
       "        0.04263903, -0.02741712,  0.04248934, -0.04117119,  0.1337102 ,\n",
       "       -0.01596204,  0.05762576,  0.00730769,  0.06105822,  0.02038225,\n",
       "        0.06695575,  0.03489211,  0.01739459, -0.00652035, -0.05772134,\n",
       "        0.07827891,  0.03879946,  0.13483463,  0.00304335, -0.07165361,\n",
       "        0.06038161, -0.05664064, -0.10519326, -0.00272405, -0.07754581,\n",
       "        0.06773265,  0.04561247,  0.07964371, -0.07700854,  0.08914506,\n",
       "        0.17518474,  0.01715964,  0.02536894, -0.02523981, -0.10770504,\n",
       "       -0.15260579, -0.00970635,  0.02915863,  0.06111136,  0.00900377,\n",
       "        0.04059591,  0.07735578, -0.04844462,  0.14468352,  0.03376452,\n",
       "       -0.06587124,  0.08225945,  0.02598596,  0.03487838,  0.04218515,\n",
       "        0.07036549, -0.01025326, -0.02166397, -0.06253622,  0.06407471,\n",
       "       -0.123892  , -0.01470529,  0.00293271,  0.05741141, -0.0694873 ,\n",
       "       -0.11187772, -0.07452565,  0.04860114, -0.06068123, -0.03620556,\n",
       "       -0.00046228, -0.05690295, -0.04587905,  0.04772501, -0.04207376,\n",
       "       -0.04783105,  0.05724974, -0.2321706 ,  0.08884832,  0.00071572,\n",
       "        0.05809747, -0.10940351,  0.01475556,  0.01292174, -0.07808141,\n",
       "        0.02383626,  0.06215472,  0.01264787,  0.04288376,  0.06052897,\n",
       "       -0.02552988,  0.05216181,  0.0164598 , -0.03271155, -0.05829453,\n",
       "       -0.09264817,  0.16744114, -0.01112105,  0.07499924,  0.05138046,\n",
       "       -0.00072611, -0.03267605, -0.00081905, -0.02593878, -0.10709276],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dict[13502]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\r"
     ]
    }
   ],
   "source": [
    "data_generator = TrainDataGenerator(train_articles_collection, tk, 256)\n",
    "data_generator = iter(data_generator)\n",
    "for i in range(9):\n",
    "    print(i,end=\"\\r\")\n",
    "    X,Y = next(data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: len(list(filter(lambda y:y!=0,x))),X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 988,  988,  279, 1208,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  1075,      2,    986,      4,      6,    248,    988,   2250,\n",
       "             44,    713,   6300,   6300,      0],\n",
       "        [  3974,      2,    986,      4,      6,    248,    988,   2250,\n",
       "             18,    713,   6300,   6300,      0],\n",
       "        [    17,     17,  46298,  59753,   9143,   2106,    988,   2250,\n",
       "             16,      6,    200,    174,      0]],\n",
       "\n",
       "       [[  1075,      2,    986,      4,      6,    248,    988,   2250,\n",
       "             44,    713,   6300,   6300,      0],\n",
       "        [  3974,      2,    986,      4,      6,    248,    988,   2250,\n",
       "             18,    713,   6300,   6300,      0],\n",
       "        [    17,     17,  46298,  59753,   9143,   2106,    988,   2250,\n",
       "             16,      6,    200,    174,      0]],\n",
       "\n",
       "       [[     2,    248,    986,      4,      1,  10169,    279,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[  1063,   2870,   1361,      7,      6,   6336,   1208,    544,\n",
       "          22931,    137,     27,    587,      0],\n",
       "        [     1,    102,      2,      6,    134,   6336,   1208,    544,\n",
       "          22931, 551603,    137,      8,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]]], dtype=int32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3][26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos,neg = deepRank_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'bioasq_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-10cc45d13ac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery_to_test_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_articles_collection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_to_test_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-219-bffc59abe895>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, article_collection, tokenizer, article_map)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_collection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bioasq_data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_collection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"collection\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'bioasq_data'"
     ]
    }
   ],
   "source": [
    "query_to_test_index = 1\n",
    "\n",
    "data_generator = TestDataGenerator(train_articles_collection, tk)\n",
    "data_generator = iter(data_generator)\n",
    "for _ in range(query_to_test_index+1):\n",
    "    X = next(data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('26671317', 7.931817054748535),\n",
       " ('20975159', 7.902041435241699),\n",
       " ('20650709', 7.8478264808654785),\n",
       " ('19805301', 7.842199802398682),\n",
       " ('21731768', 7.818233013153076),\n",
       " ('24681619', 7.759010314941406),\n",
       " ('26631348', 7.714381217956543),\n",
       " ('22196114', 7.696432590484619),\n",
       " ('23817568', 7.648404598236084),\n",
       " ('26410599', 7.3919854164123535)]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_ranking = document_score_model.predict(X)\n",
    "\n",
    "bm25_results = train_articles_collection[\"test_data\"][query_to_test_index][\"documents\"]\n",
    "positive_docs = train_articles_collection[\"test_data\"][query_to_test_index][\"positive_pmid\"]\n",
    "\n",
    "re_ranking_pmid = list(zip(bm25_results,re_ranking.tolist()))\n",
    "\n",
    "re_ranking_pmid.sort(key=lambda x:-x[1])\n",
    "\n",
    "re_ranking_pmid[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kind enzyme encoded proto oncogene abl1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['21435002',\n",
       " '20841568',\n",
       " '9500553',\n",
       " '24012954',\n",
       " '18796434',\n",
       " '23842646',\n",
       " '18528425']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tk.sequences_to_texts([train_articles_collection[\"test_data\"][query_to_test_index][\"query\"]]))\n",
    "positive_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, '24012954', 5.5485920906066895), (29, '9500553', 4.8889594078063965), (87, '21435002', 4.592401504516602), (112, '23842646', 4.525805473327637), (155, '18796434', 4.400295257568359), (342, '18528425', 4.139955043792725), (2244, '20841568', 3.1609909534454346)]\n",
      "[(1, '9500553'), (29, '21435002'), (187, '24012954'), (309, '18528425'), (333, '23842646'), (610, '20841568'), (2354, '18796434')]\n"
     ]
    }
   ],
   "source": [
    "positive_docs_ranked = []\n",
    "for i,result in enumerate(re_ranking_pmid):\n",
    "    if result[0] in set(positive_docs):\n",
    "        positive_docs_ranked.append((i,result[0],result[1]))\n",
    "        \n",
    "true_ranked = []\n",
    "for i,pmid in enumerate(bm25_results):\n",
    "    if pmid in set(positive_docs):\n",
    "        true_ranked.append((i,pmid))\n",
    "\n",
    "print(positive_docs_ranked)\n",
    "print(true_ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
