{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "import os\n",
    "os.chdir(\"/home/tiagoalmeida/bioASQ-taskb/\")\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from bisect import bisect\n",
    "import math\n",
    "\n",
    "##add keras to the modules\n",
    "module_path = os.path.abspath(os.path.join('pubmed_data'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from pubmed_data import pubmed_helper as ph\n",
    "\n",
    "EMB_DIM = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAR\n",
    "\n",
    "Reference PAPER:http://dmkd.cs.vt.edu/papers/WWW19.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network structure\n",
    " - [General Network Configuration](#var_def)\n",
    " - [Self Attention Layer](#self_attn)\n",
    " - [Cross Attention Layer](#cross_attn)\n",
    " - [Final Network](#final_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load regex_full_tokens_tokenizer.p\n",
      "Load regex_full_tokens_word_embedding.p\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load tokenizer and the embedding matrix\n",
    "\n",
    "MODE = \"regex_full_tokens\"\n",
    "tk = ph.load_tokenizer(mode=MODE)\n",
    "emb_dict = ph.load_embeddings(mode=MODE)\n",
    "\n",
    "assert len(tk.word_counts) == len(emb_dict)\n",
    "\n",
    "#Number of different words\n",
    "VOCAB_SIZE = len(tk.word_counts)+1\n",
    "\n",
    "#Dimension of embeddings\n",
    "EMB_DIM = emb_dict[1].shape[0]\n",
    "\n",
    "emb_matrix = np.zeros((VOCAB_SIZE, EMB_DIM))\n",
    "\n",
    "for i,vector in emb_dict.items():\n",
    "    emb_matrix[i] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='var_def'></a>\n",
    "## General Network Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import unstack, stack\n",
    "##Test \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, activations\n",
    "from tensorflow.keras.initializers import Zeros, Ones, Constant\n",
    "from tensorflow.keras.layers import Dense, Lambda, Bidirectional, Dot,Masking,Reshape, Concatenate, Layer, Embedding, Input, Conv2D, GlobalMaxPooling2D, Flatten, TimeDistributed, GRU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.activations import tanh, sigmoid\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "#Number max of term per query\n",
    "MAX_Q_TERM = 13\n",
    "\n",
    "#Number max of the snippet terms\n",
    "MAX_SNIPPET_LENGTH = 13\n",
    "\n",
    "MAX_NUMBER_SNIPPETS = 20\n",
    "\n",
    "MAX_DOCUMENT_TOKENS = MAX_SNIPPET_LENGTH*MAX_NUMBER_SNIPPETS\n",
    "\n",
    "ATTENTION_DIMENSION = 100\n",
    "\n",
    "assert ATTENTION_DIMENSION%2==0\n",
    "\n",
    "GRU_DIM = ATTENTION_DIMENSION//2\n",
    "\n",
    "NUM_OF_SELF_ATTENTION = 1\n",
    "#Train embedding weights\n",
    "EMB_TRAINABLE = False\n",
    "\n",
    "ACTIVATION_FUNCTION = \"selu\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='self_attn'></a>\n",
    "### Self Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(Layer):\n",
    "    \n",
    "    def __init__(self, initializer='glorot_normal', regularizer=None, **kargs):\n",
    "        super(SelfAttention, self).__init__(**kargs)\n",
    "\n",
    "        self.initializer = initializer\n",
    "        \n",
    "        if regularizer is None or isinstance(regularizer,str):\n",
    "            self.regularizer = regularizers.get(regularizer)\n",
    "        else:\n",
    "            self.regularizer = regularizer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        emb_dim = int(input_shape[2])\n",
    "\n",
    "        self.W_attn_project = self.add_variable(name = \"self_attention_projection\",\n",
    "                                   shape = [emb_dim, ATTENTION_DIMENSION],\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.W_attn_score = self.add_variable(name = \"self_attention_score\",\n",
    "                                   shape = [ATTENTION_DIMENSION, NUM_OF_SELF_ATTENTION],\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        print(\"x\",x)\n",
    "        #x_transpose = K.permute_dimensions(x,[0,2,1]) # (NONE, 100,15)\n",
    "        #print(\"x_transpose\",x_transpose)\n",
    "        condition = K.all(x,keepdims=True,axis=-1)\n",
    "        print(\"condition\",condition)\n",
    "        inv_condition = (1-K.cast(condition, K.floatx()))\n",
    "        print(\"inv_condition\",inv_condition)\n",
    "        \n",
    "        condition_all_zero = K.all(x,keepdims=True)\n",
    "        print(\"condition_all_zero\",condition_all_zero)\n",
    "        inv_condition_all_zero = (1-K.cast(condition_all_zero, K.floatx()))\n",
    "        print(\"inv_condition_all_zero\",inv_condition_all_zero)\n",
    "        \n",
    "        \n",
    "        x_projection = K.dot(x, self.W_attn_project) # (NONE, 300, 300)\n",
    "        print(\"x_projection\",x_projection)\n",
    "        x_tanh = K.tanh(x_projection) # (NONE, 300, 15)\n",
    "        print(\"x_tanh\",x_tanh)\n",
    "        x_attention = K.dot(x_tanh, self.W_attn_score)\n",
    "        print(\"x_attention\",x_attention)\n",
    "        x_attention_maked = x_attention + (1.0 - inv_condition) * -100000.0\n",
    "        print(\"x_attention_maked\",x_attention_maked)\n",
    "        #x_attention_squeeze = K.squeeze(x_score, axis=1)\n",
    "        x_attention_softmax = K.softmax(x_attention_maked,axis = 1)\n",
    "        print(\"x_attention_softmax\",x_attention_softmax)\n",
    "        x_attention_softmax_masked = x_attention_softmax * inv_condition_all_zero\n",
    "        print(\"x_attention_softmax_masked\",x_attention_softmax_masked)\n",
    "        \n",
    "        if (NUM_OF_SELF_ATTENTION>1):\n",
    "            x_attention_softmax_transpose =  K.permute_dimensions(x_attention_softmax_masked,[0,2,1])\n",
    "            print(\"x_attention_softmax_transpose\",x_attention_softmax_transpose)\n",
    "\n",
    "            x_attention_softmax_transpose_expand = K.expand_dims(x_attention_softmax_transpose)\n",
    "            print(\"x_attention_softmax_transpose_expand\",x_attention_softmax_transpose_expand)\n",
    "\n",
    "            x_scored_emb = x_attention_softmax_transpose_expand * x\n",
    "            print(\"x_scored_emb\",x_scored_emb)\n",
    "            x_attention_rep = K.sum(x_scored_emb, axis=2)\n",
    "        else:\n",
    "            x_scored_emb = x_attention_softmax_masked * x\n",
    "            print(\"x_scored_emb\",x_scored_emb)\n",
    "            x_attention_rep = K.sum(x_scored_emb, axis=1)\n",
    "            \n",
    "        print(\"x_attention_rep\",x_attention_rep)\n",
    "        return x_attention_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Tensor(\"embedding_2/embedding_lookup/Identity_1:0\", shape=(?, 13, 300), dtype=float32)\n",
      "condition Tensor(\"self_attention_2/All:0\", shape=(?, 13, 1), dtype=bool)\n",
      "inv_condition Tensor(\"self_attention_2/sub:0\", shape=(?, 13, 1), dtype=float32)\n",
      "condition_all_zero Tensor(\"self_attention_2/All_1:0\", shape=(1, 1, 1), dtype=bool)\n",
      "inv_condition_all_zero Tensor(\"self_attention_2/sub_1:0\", shape=(1, 1, 1), dtype=float32)\n",
      "x_projection Tensor(\"self_attention_2/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_2/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_2/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"self_attention_2/add:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_2/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax_masked Tensor(\"self_attention_2/mul_1:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_2/mul_2:0\", shape=(?, 13, 300), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_2/Sum:0\", shape=(?, 300), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 13, 300)           300000    \n",
      "_________________________________________________________________\n",
      "self_attention_2 (SelfAttent (None, 300)               30100     \n",
      "=================================================================\n",
      "Total params: 330,100\n",
      "Trainable params: 330,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#test self attention\n",
    "visualize_self_attention = True\n",
    "if visualize_self_attention:\n",
    "    self_test = Sequential()\n",
    "    self_test.add(Embedding(1000,300, input_length = MAX_Q_TERM))\n",
    "    self_test.add(SelfAttention())\n",
    "\n",
    "    self_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cross_attn'></a>\n",
    "### Cross Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(Layer):\n",
    "    \n",
    "    def __init__(self, initializer='glorot_normal', regularizer=None, **kargs):\n",
    "        super(CrossAttention, self).__init__(**kargs)\n",
    "\n",
    "        self.initializer = initializer\n",
    "        \n",
    "        if regularizer is None or isinstance(regularizer,str):\n",
    "            self.regularizer = regularizers.get(regularizer)\n",
    "        else:\n",
    "            self.regularizer = regularizer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        \"\"\"\n",
    "        input: [0] - query context embedding\n",
    "               [1] - document context embedding\n",
    "        \"\"\"\n",
    "        doc_embedding = input_shape[1]\n",
    "        query_embedding = input_shape[0]\n",
    "        \n",
    "        self.query_len = query_embedding[1]\n",
    "        self.doc_len = doc_embedding[1]\n",
    "        \n",
    "        print(\"query_len\", self.query_len)\n",
    "        print(\"doc_len\", self.doc_len)\n",
    "        \n",
    "        assert int(query_embedding[2]) == int(doc_embedding[2])\n",
    "        \n",
    "        self.embedding_dim = int(query_embedding[2])\n",
    "        \n",
    "        self.W_sim_projection = self.add_variable(name = \"similarity_projection\",\n",
    "                                   shape = [self.embedding_dim*3,1],\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        super(CrossAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        input: [0] - query context embedding\n",
    "               [1] - document context embedding\n",
    "        \"\"\"\n",
    "        doc_embedding = x[1]\n",
    "        print(\"doc_embedding\",doc_embedding.shape)\n",
    "        query_embedding = x[0]\n",
    "        print(\"query_embedding\",query_embedding.shape)\n",
    "        \n",
    "        #build similarity matrix\n",
    "        #row document token\n",
    "        #colum query token\n",
    "        doc_q_matrix = K.expand_dims(doc_embedding, axis=2)\n",
    "        print(\"doc_q_matrix\",doc_q_matrix.shape)\n",
    "        doc_q_matrix = K.repeat_elements(doc_q_matrix, self.query_len, axis=2)\n",
    "        print(\"doc_q_matrix\",doc_q_matrix.shape)\n",
    "        \n",
    "        q_doc_matrix = K.expand_dims(query_embedding, axis=1)\n",
    "        print(\"q_doc_matrix\",q_doc_matrix.shape)\n",
    "        q_doc_matrix = K.repeat_elements(q_doc_matrix, self.doc_len, axis=1)\n",
    "        print(\"q_doc_matrix\",q_doc_matrix.shape)\n",
    "        \n",
    "        element_mult = doc_q_matrix * q_doc_matrix\n",
    "        print(\"element_mult\",element_mult.shape)\n",
    "        \n",
    "        #concatenation\n",
    "        S = K.concatenate([doc_q_matrix, q_doc_matrix, element_mult])\n",
    "        print(\"S\",S.shape)\n",
    "        print(\"Wc\",self.W_sim_projection.shape)\n",
    "        S = K.dot(S, self.W_sim_projection)\n",
    "        print(\"S\",S.shape)\n",
    "        \n",
    "        S = K.squeeze(S,axis=-1)\n",
    "        print(\"S\",S.shape)\n",
    "        \n",
    "        S_D2Q = K.softmax(S, axis=1)\n",
    "        print(\"S_D2Q\",S_D2Q.shape)\n",
    "        \n",
    "        S_Q2D = K.softmax(S, axis=2)\n",
    "        print(\"S_Q2D\",S_Q2D.shape)\n",
    "        \n",
    "        A_D2Q = K.batch_dot(S_D2Q, query_embedding)\n",
    "        print(\"A_D2Q\",A_D2Q.shape)\n",
    "        \n",
    "        S_Q2D_transpose = K.permute_dimensions(S_Q2D,[0,2,1])\n",
    "        print(\"S_Q2D_transpose\",S_Q2D_transpose.shape)\n",
    "        \n",
    "        A_D2Q_Q2D = K.batch_dot(S_D2Q, S_Q2D_transpose)\n",
    "        print(\"A_D2Q_Q2D\",A_D2Q_Q2D.shape)\n",
    "        \n",
    "        A_Q2D = K.batch_dot(A_D2Q_Q2D, doc_embedding)\n",
    "        print(\"A_Q2D\",A_Q2D.shape)\n",
    "        \n",
    "        #concat\n",
    "        doc_attn = doc_embedding * A_D2Q\n",
    "        print(\"doc_attn\",doc_attn.shape)\n",
    "        doc_q_attn = doc_embedding * A_Q2D\n",
    "        print(\"doc_q_attn\",doc_q_attn.shape)\n",
    "        \n",
    "        V = K.concatenate([doc_embedding, A_D2Q, doc_attn, doc_q_attn])\n",
    "        \n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "visualize_cross_attention = False\n",
    "if visualize_cross_attention:\n",
    "    input_doc = Input(shape=(MAX_Q_TERM,))\n",
    "    input_query = Input(shape=(MAX_SNIPPET_LENGTH,))\n",
    "\n",
    "    emb_layer = Embedding(1000,300)\n",
    "\n",
    "    cross_layer = CrossAttention()\n",
    "\n",
    "    emb_doc = emb_layer(input_doc)\n",
    "    emb_query = emb_layer(input_query)\n",
    "\n",
    "    cross_attention = cross_layer([emb_query, emb_doc])\n",
    "\n",
    "    model = Model(inputs=[input_query,input_doc], outputs = [cross_attention])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='final_net'></a>\n",
    "### Final Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_projection Tensor(\"self_attention/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention/mul:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention/Sum:0\", shape=(?, 100), dtype=float32)\n",
      "query_len 13\n",
      "doc_len 13\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_2/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_2/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_2/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_2/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_2/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_3/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_3/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_3/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_3/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_3/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_3/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_4/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_4/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_4/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_4/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_4/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_4/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_5/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_5/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_5/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_5/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_5/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_5/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_6/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_6/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_6/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_6/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_6/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_6/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_7/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_7/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_7/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_7/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_7/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_7/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_8/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_8/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_8/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_8/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_8/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_8/Sum:0\", shape=(?, 400), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_9/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_9/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_9/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_9/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_9/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_9/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_10/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_10/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_10/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_10/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_10/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_10/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_11/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_11/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_11/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_11/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_11/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_11/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_12/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_12/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_12/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_12/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_12/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_12/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_13/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_13/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_13/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_13/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_13/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_13/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_14/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_14/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_14/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_14/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_14/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_14/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_15/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_15/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_15/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_15/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_15/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_15/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_16/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_16/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_16/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_16/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_16/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_16/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_17/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_17/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_17/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_17/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_17/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_17/Sum:0\", shape=(?, 400), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_18/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_18/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_18/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_18/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_18/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_18/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"self_attention_1_19/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_1_19/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_1_19/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_1_19/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_1_19/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_1_19/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"self_attention_2/Reshape_2:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_tanh Tensor(\"self_attention_2/Tanh:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_attention Tensor(\"self_attention_2/Reshape_5:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"self_attention_2/transpose_3:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"self_attention_2/mul:0\", shape=(?, 20, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"self_attention_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "document_input (InputLayer)     (None, 20, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "query_input (InputLayer)        (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "unstack_snippets (Lambda)       [(None, 13), (None,  0           document_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     (None, 13, 200)      858359000   query_input[0][0]                \n",
      "                                                                 unstack_snippets[0][0]           \n",
      "                                                                 unstack_snippets[0][1]           \n",
      "                                                                 unstack_snippets[0][2]           \n",
      "                                                                 unstack_snippets[0][3]           \n",
      "                                                                 unstack_snippets[0][4]           \n",
      "                                                                 unstack_snippets[0][5]           \n",
      "                                                                 unstack_snippets[0][6]           \n",
      "                                                                 unstack_snippets[0][7]           \n",
      "                                                                 unstack_snippets[0][8]           \n",
      "                                                                 unstack_snippets[0][9]           \n",
      "                                                                 unstack_snippets[0][10]          \n",
      "                                                                 unstack_snippets[0][11]          \n",
      "                                                                 unstack_snippets[0][12]          \n",
      "                                                                 unstack_snippets[0][13]          \n",
      "                                                                 unstack_snippets[0][14]          \n",
      "                                                                 unstack_snippets[0][15]          \n",
      "                                                                 unstack_snippets[0][16]          \n",
      "                                                                 unstack_snippets[0][17]          \n",
      "                                                                 unstack_snippets[0][18]          \n",
      "                                                                 unstack_snippets[0][19]          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 13, 100)      75300       embedding_layer[0][0]            \n",
      "                                                                 embedding_layer[1][0]            \n",
      "                                                                 embedding_layer[2][0]            \n",
      "                                                                 embedding_layer[3][0]            \n",
      "                                                                 embedding_layer[4][0]            \n",
      "                                                                 embedding_layer[5][0]            \n",
      "                                                                 embedding_layer[6][0]            \n",
      "                                                                 embedding_layer[7][0]            \n",
      "                                                                 embedding_layer[8][0]            \n",
      "                                                                 embedding_layer[9][0]            \n",
      "                                                                 embedding_layer[10][0]           \n",
      "                                                                 embedding_layer[11][0]           \n",
      "                                                                 embedding_layer[12][0]           \n",
      "                                                                 embedding_layer[13][0]           \n",
      "                                                                 embedding_layer[14][0]           \n",
      "                                                                 embedding_layer[15][0]           \n",
      "                                                                 embedding_layer[16][0]           \n",
      "                                                                 embedding_layer[17][0]           \n",
      "                                                                 embedding_layer[18][0]           \n",
      "                                                                 embedding_layer[19][0]           \n",
      "                                                                 embedding_layer[20][0]           \n",
      "__________________________________________________________________________________________________\n",
      "cross_attention (CrossAttention (None, 13, 400)      300         bidirectional[0][0]              \n",
      "                                                                 bidirectional[1][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[2][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[3][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[4][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[5][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[6][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[7][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[8][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[9][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[10][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[11][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[12][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[13][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[14][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[15][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[16][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[17][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[18][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[19][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional[20][0]             \n",
      "__________________________________________________________________________________________________\n",
      "self_attention_1 (SelfAttention (None, 400)          40100       cross_attention[0][0]            \n",
      "                                                                 cross_attention[1][0]            \n",
      "                                                                 cross_attention[2][0]            \n",
      "                                                                 cross_attention[3][0]            \n",
      "                                                                 cross_attention[4][0]            \n",
      "                                                                 cross_attention[5][0]            \n",
      "                                                                 cross_attention[6][0]            \n",
      "                                                                 cross_attention[7][0]            \n",
      "                                                                 cross_attention[8][0]            \n",
      "                                                                 cross_attention[9][0]            \n",
      "                                                                 cross_attention[10][0]           \n",
      "                                                                 cross_attention[11][0]           \n",
      "                                                                 cross_attention[12][0]           \n",
      "                                                                 cross_attention[13][0]           \n",
      "                                                                 cross_attention[14][0]           \n",
      "                                                                 cross_attention[15][0]           \n",
      "                                                                 cross_attention[16][0]           \n",
      "                                                                 cross_attention[17][0]           \n",
      "                                                                 cross_attention[18][0]           \n",
      "                                                                 cross_attention[19][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1, 400)       0           self_attention_1[0][0]           \n",
      "                                                                 self_attention_1[1][0]           \n",
      "                                                                 self_attention_1[2][0]           \n",
      "                                                                 self_attention_1[3][0]           \n",
      "                                                                 self_attention_1[4][0]           \n",
      "                                                                 self_attention_1[5][0]           \n",
      "                                                                 self_attention_1[6][0]           \n",
      "                                                                 self_attention_1[7][0]           \n",
      "                                                                 self_attention_1[8][0]           \n",
      "                                                                 self_attention_1[9][0]           \n",
      "                                                                 self_attention_1[10][0]          \n",
      "                                                                 self_attention_1[11][0]          \n",
      "                                                                 self_attention_1[12][0]          \n",
      "                                                                 self_attention_1[13][0]          \n",
      "                                                                 self_attention_1[14][0]          \n",
      "                                                                 self_attention_1[15][0]          \n",
      "                                                                 self_attention_1[16][0]          \n",
      "                                                                 self_attention_1[17][0]          \n",
      "                                                                 self_attention_1[18][0]          \n",
      "                                                                 self_attention_1[19][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 20, 400)      0           lambda[0][0]                     \n",
      "                                                                 lambda[1][0]                     \n",
      "                                                                 lambda[2][0]                     \n",
      "                                                                 lambda[3][0]                     \n",
      "                                                                 lambda[4][0]                     \n",
      "                                                                 lambda[5][0]                     \n",
      "                                                                 lambda[6][0]                     \n",
      "                                                                 lambda[7][0]                     \n",
      "                                                                 lambda[8][0]                     \n",
      "                                                                 lambda[9][0]                     \n",
      "                                                                 lambda[10][0]                    \n",
      "                                                                 lambda[11][0]                    \n",
      "                                                                 lambda[12][0]                    \n",
      "                                                                 lambda[13][0]                    \n",
      "                                                                 lambda[14][0]                    \n",
      "                                                                 lambda[15][0]                    \n",
      "                                                                 lambda[16][0]                    \n",
      "                                                                 lambda[17][0]                    \n",
      "                                                                 lambda[18][0]                    \n",
      "                                                                 lambda[19][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self_attention_2 (SelfAttention (None, 400)          40100       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "self_attention (SelfAttention)  (None, 100)          10100       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          40100       self_attention_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 100)          0           self_attention[0][0]             \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           5050        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            51          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 858,580,201\n",
      "Trainable params: 221,201\n",
      "Non-trainable params: 858,359,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Layer Defenition\n",
    "\n",
    "input_query = Input(shape=(MAX_Q_TERM,), name = \"query_input\")\n",
    "input_doc = Input(shape=(MAX_NUMBER_SNIPPETS, MAX_SNIPPET_LENGTH), name = \"document_input\")\n",
    "#input_doc = [Input(shape=(MAX_SNIPPET_LENGTH,), name = \"document_input_\"+j) for j in range(MAX_NUMBER_SNIPPETS)]\n",
    "\n",
    "unstack_snippets = Lambda(lambda x:unstack(x,axis=1), name=\"unstack_snippets\")\n",
    "\n",
    "input_docs = unstack_snippets(input_doc)\n",
    "\n",
    "emb_layer = Embedding(VOCAB_SIZE,EMB_DIM, weights=[emb_matrix],name=\"embedding_layer\", trainable=EMB_TRAINABLE)\n",
    "\n",
    "context_encoder = Bidirectional(GRU(GRU_DIM, return_sequences=True))\n",
    "\n",
    "query_self_attention = SelfAttention()\n",
    "cross_layer = CrossAttention()\n",
    "snippet_lvl_1_self_attention = SelfAttention()\n",
    "snippet_lvl_2_self_attention = SelfAttention()\n",
    "\n",
    "snippet_expand_dim = Lambda(lambda x:K.expand_dims(x,axis=1))\n",
    "snippet_concat = Lambda(lambda x:K.concatenate(x,axis=1))\n",
    "\n",
    "snippets_projection = Dense(ATTENTION_DIMENSION)\n",
    "\n",
    "q_doc_mult = Lambda(lambda x:x[0]*x[1])\n",
    "\n",
    "#NO ACTIVATION??\n",
    "fnn_h1 = Dense(ATTENTION_DIMENSION)\n",
    "fnn_h2 = Dense(ATTENTION_DIMENSION//2)\n",
    "fnn_h3 = Dense(1)\n",
    "\n",
    "\"\"\"\n",
    "ASSEMBLE\n",
    "\"\"\"\n",
    "\n",
    "query_emb = emb_layer(input_query)\n",
    "query_context_aware = context_encoder(query_emb)\n",
    "query_weighted = query_self_attention(query_context_aware)\n",
    "\n",
    "snippets = []\n",
    "for input_snippet in input_docs:\n",
    "    snippet_emb = emb_layer(input_snippet)\n",
    "    snippet_context_aware = context_encoder(snippet_emb)\n",
    "    snippet_query = cross_layer([query_context_aware, snippet_context_aware])\n",
    "    snippet_weighted = snippet_lvl_1_self_attention(snippet_query)\n",
    "    snippets.append(snippet_expand_dim(snippet_weighted))\n",
    "\n",
    "snippets = snippet_concat(snippets)\n",
    "snippets = snippet_lvl_2_self_attention(snippets)\n",
    "snippets = snippets_projection(snippets)\n",
    "q_snippets_rep = q_doc_mult([query_weighted, snippets])\n",
    "score = fnn_h1(q_snippets_rep)\n",
    "score = fnn_h2(score)\n",
    "score = fnn_h3(score)\n",
    "\n",
    "inputs = [input_query, input_doc]\n",
    "\n",
    "har_model = Model(inputs=inputs, outputs = [score])\n",
    "\n",
    "har_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build training Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_of_documents[0] Tensor(\"unstack_documents/unstack:0\", shape=(?, 20, 13), dtype=float32)\n",
      "input_query Tensor(\"input_1:0\", shape=(?, 13), dtype=float32)\n",
      "doc_tensor Tensor(\"unstack_documents/unstack:0\", shape=(?, 20, 13), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"model/self_attention_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tanh Tensor(\"model/self_attention_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_2/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_2/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_2/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_2/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_2/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_3/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_3/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_3/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_3/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_3/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_3/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_4/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_4/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_4/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_4/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_4/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_4/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_5/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_5/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_5/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_5/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_5/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_5/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_6/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_6/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_6/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_6/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_6/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_6/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_7/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_7/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_7/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_7/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_7/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_7/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_8/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_8/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_8/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_8/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_8/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_8/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_9/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_9/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_9/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_9/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_9/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_9/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_10/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_10/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_10/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_10/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_10/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_10/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_11/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_11/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_11/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_11/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_11/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_11/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_12/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_12/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_12/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_12/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_12/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_12/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_13/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_13/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_13/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_13/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_13/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_13/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_14/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_14/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_14/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_14/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_14/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_14/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_15/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_15/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_15/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_15/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_15/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_15/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_16/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_16/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_16/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_16/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_16/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_16/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_17/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_17/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_17/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_attention_softmax Tensor(\"model/self_attention_1_17/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_17/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_17/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_18/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_18/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_18/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_18/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_18/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_18/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_1_19/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_1_19/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_1_19/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_1_19/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_1_19/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_1_19/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention_2/Reshape_2:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention_2/Tanh:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention_2/Reshape_5:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention_2/transpose_3:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention_2/mul:0\", shape=(?, 20, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model/self_attention/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model/self_attention/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model/self_attention/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model/self_attention/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model/self_attention/mul:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention_rep Tensor(\"model/self_attention/Sum:0\", shape=(?, 100), dtype=float32)\n",
      "input_query Tensor(\"input_1:0\", shape=(?, 13), dtype=float32)\n",
      "doc_tensor Tensor(\"unstack_documents/unstack:1\", shape=(?, 20, 13), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"model_1/self_attention_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_2/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_2/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_2/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_2/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_2/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_3/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_3/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_3/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_3/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_3/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_3/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_4/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_4/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_4/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_4/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_4/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_4/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_5/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_5/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_5/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_5/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_5/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_5/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_6/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_6/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_6/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_6/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_6/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_6/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_7/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_7/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_7/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_7/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_7/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_7/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_8/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_8/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_8/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_8/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_8/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_8/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_9/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_9/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_9/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_9/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_9/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_9/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_10/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_10/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_10/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_10/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_10/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_10/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_11/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_11/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_11/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_11/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_11/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_11/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_12/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_12/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_12/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_12/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_12/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_12/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_13/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_13/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_attention Tensor(\"model_1/self_attention_1_13/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_13/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_13/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_13/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_14/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_14/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_14/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_14/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_14/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_14/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_15/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_15/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_15/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_15/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_15/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_15/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_16/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_16/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_16/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_16/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_16/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_16/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_17/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_17/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_17/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_17/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_17/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_17/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_18/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_18/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_18/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_18/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_18/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_18/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_1_19/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_1_19/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_1_19/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_1_19/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_1_19/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_1_19/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention_2/Reshape_2:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention_2/Tanh:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention_2/Reshape_5:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention_2/transpose_3:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention_2/mul:0\", shape=(?, 20, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_1/self_attention/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_1/self_attention/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_1/self_attention/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_1/self_attention/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_1/self_attention/mul:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_1/self_attention/Sum:0\", shape=(?, 100), dtype=float32)\n",
      "input_query Tensor(\"input_1:0\", shape=(?, 13), dtype=float32)\n",
      "doc_tensor Tensor(\"unstack_documents/unstack:2\", shape=(?, 20, 13), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"model_2/self_attention_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_2/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_2/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_2/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_2/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_2/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_3/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_3/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_3/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_3/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_3/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_3/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_4/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_4/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_4/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_4/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_4/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_4/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_5/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_5/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_5/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_5/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_5/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_5/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_6/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_6/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_6/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_attention_softmax Tensor(\"model_2/self_attention_1_6/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_6/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_6/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_7/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_7/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_7/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_7/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_7/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_7/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_8/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_8/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_8/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_8/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_8/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_8/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_9/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_9/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_9/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_9/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_9/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_9/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_10/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_10/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_10/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_10/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_10/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_10/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_11/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_11/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_11/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_11/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_11/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_11/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_12/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_12/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_12/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_12/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_12/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_12/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_13/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_13/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_13/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_13/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_13/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_13/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_14/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_14/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_14/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_14/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_14/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_14/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_15/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_15/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_15/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_15/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_15/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_15/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_16/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_16/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_16/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_16/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_16/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_16/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_17/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_17/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_17/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_17/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_17/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_17/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_18/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_18/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_18/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_18/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_18/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_18/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_1_19/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_1_19/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_1_19/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_1_19/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_1_19/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_1_19/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_2/self_attention_2/Reshape_2:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention_2/Tanh:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention_2/Reshape_5:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention_2/transpose_3:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention_2/mul:0\", shape=(?, 20, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention_2/Sum:0\", shape=(?, 400), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_projection Tensor(\"model_2/self_attention/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_2/self_attention/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_2/self_attention/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_2/self_attention/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_2/self_attention/mul:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_2/self_attention/Sum:0\", shape=(?, 100), dtype=float32)\n",
      "input_query Tensor(\"input_1:0\", shape=(?, 13), dtype=float32)\n",
      "doc_tensor Tensor(\"unstack_documents/unstack:3\", shape=(?, 20, 13), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"model_3/self_attention_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_2/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_2/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_2/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_2/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_2/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_3/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_3/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_3/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_3/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_3/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_3/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_4/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_4/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_4/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_4/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_4/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_4/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_5/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_5/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_5/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_5/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_5/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_5/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_6/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_6/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_6/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_6/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_6/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_6/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_7/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_7/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_7/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_7/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_7/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_7/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_8/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_8/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_8/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_8/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_8/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_8/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_9/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_9/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_9/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_9/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_9/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_9/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_10/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_10/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_10/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_10/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_10/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_10/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_11/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_11/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_11/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_11/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_11/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_11/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_12/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_12/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_12/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_12/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_12/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_12/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_13/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_13/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_13/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_13/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_13/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_13/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_14/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_14/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_14/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_14/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_14/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_14/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_15/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_15/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_15/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_attention_softmax Tensor(\"model_3/self_attention_1_15/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_15/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_15/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_16/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_16/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_16/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_16/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_16/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_16/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_17/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_17/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_17/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_17/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_17/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_17/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_18/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_18/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_18/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_18/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_18/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_18/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_1_19/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_1_19/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_1_19/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_1_19/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_1_19/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_1_19/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention_2/Reshape_2:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention_2/Tanh:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention_2/Reshape_5:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention_2/transpose_3:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention_2/mul:0\", shape=(?, 20, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_3/self_attention/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_3/self_attention/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_3/self_attention/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_3/self_attention/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_3/self_attention/mul:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_3/self_attention/Sum:0\", shape=(?, 100), dtype=float32)\n",
      "input_query Tensor(\"input_1:0\", shape=(?, 13), dtype=float32)\n",
      "doc_tensor Tensor(\"unstack_documents/unstack:4\", shape=(?, 20, 13), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"model_4/self_attention_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_2/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_2/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_2/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_2/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_2/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_3/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_3/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_3/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_3/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_3/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_3/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_4/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_4/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_4/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_4/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_4/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_4/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_5/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_5/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_5/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_5/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_5/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_5/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_6/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_6/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_6/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_6/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_6/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_6/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_7/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_7/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_7/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_7/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_7/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_7/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_8/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_8/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_8/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_8/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_8/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_8/Sum:0\", shape=(?, 400), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_projection Tensor(\"model_4/self_attention_1_9/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_9/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_9/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_9/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_9/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_9/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_10/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_10/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_10/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_10/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_10/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_10/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_11/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_11/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_11/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_11/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_11/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_11/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_12/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_12/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_12/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_12/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_12/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_12/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_13/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_13/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_13/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_13/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_13/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_13/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_14/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_14/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_14/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_14/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_14/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_14/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_15/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_15/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_15/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_15/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_15/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_15/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_16/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_16/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_16/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_16/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_16/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_16/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_17/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_17/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_17/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_17/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_17/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_17/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_18/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_18/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_18/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_18/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_18/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_18/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_1_19/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_1_19/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_1_19/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_1_19/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_1_19/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_1_19/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention_2/Reshape_2:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention_2/Tanh:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention_2/Reshape_5:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention_2/transpose_3:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention_2/mul:0\", shape=(?, 20, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_4/self_attention/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_4/self_attention/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_4/self_attention/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_4/self_attention/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_4/self_attention/mul:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_4/self_attention/Sum:0\", shape=(?, 100), dtype=float32)\n",
      "input_query Tensor(\"input_1:0\", shape=(?, 13), dtype=float32)\n",
      "doc_tensor Tensor(\"unstack_documents/unstack:5\", shape=(?, 20, 13), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"model_5/self_attention_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1/Sum:0\", shape=(?, 400), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_projection Tensor(\"model_5/self_attention_1_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_2/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_2/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_2/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_2/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_2/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_3/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_3/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_3/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_3/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_3/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_3/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_4/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_4/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_4/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_4/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_4/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_4/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_5/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_5/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_5/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_5/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_5/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_5/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_6/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_6/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_6/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_6/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_6/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_6/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_7/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_7/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_7/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_7/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_7/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_7/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_8/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_8/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_8/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_8/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_8/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_8/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_9/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_9/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_9/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_9/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_9/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_9/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_10/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_10/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_10/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_10/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_10/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_10/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_11/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_11/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_11/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_11/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_11/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_11/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_12/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_12/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_12/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_12/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_12/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_12/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_13/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_13/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_13/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_13/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_13/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_13/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_14/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_14/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_14/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_14/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_14/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_14/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_15/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_15/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_15/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_15/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_15/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_15/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_16/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_16/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_16/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_16/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_16/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_16/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_17/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_17/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_17/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_17/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_17/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_17/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_18/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_18/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_18/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_attention_softmax Tensor(\"model_5/self_attention_1_18/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_18/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_18/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_1_19/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_1_19/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_1_19/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_1_19/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_1_19/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_1_19/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention_2/Reshape_2:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention_2/Tanh:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention_2/Reshape_5:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention_2/transpose_3:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention_2/mul:0\", shape=(?, 20, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_5/self_attention/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_5/self_attention/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_5/self_attention/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_5/self_attention/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_5/self_attention/mul:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_5/self_attention/Sum:0\", shape=(?, 100), dtype=float32)\n",
      "input_query Tensor(\"input_1:0\", shape=(?, 13), dtype=float32)\n",
      "doc_tensor Tensor(\"unstack_documents/unstack:6\", shape=(?, 20, 13), dtype=float32)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "doc_embedding (?, 13, 100)\n",
      "query_embedding (?, 13, 100)\n",
      "doc_q_matrix (?, 13, 1, 100)\n",
      "doc_q_matrix (?, 13, 13, 100)\n",
      "q_doc_matrix (?, 1, 13, 100)\n",
      "q_doc_matrix (?, 13, 13, 100)\n",
      "element_mult (?, 13, 13, 100)\n",
      "S (?, 13, 13, 300)\n",
      "Wc (300, 1)\n",
      "S (?, 13, 13, 1)\n",
      "S (?, 13, 13)\n",
      "S_D2Q (?, 13, 13)\n",
      "S_Q2D (?, 13, 13)\n",
      "A_D2Q (?, 13, 100)\n",
      "S_Q2D_transpose (?, 13, 13)\n",
      "A_D2Q_Q2D (?, 13, 13)\n",
      "A_Q2D (?, 13, 100)\n",
      "doc_attn (?, 13, 100)\n",
      "doc_q_attn (?, 13, 100)\n",
      "x_projection Tensor(\"model_6/self_attention_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_1/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_1/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_1/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_1/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_1/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_1/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_2/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_2/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_2/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_2/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_2/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_3/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_3/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_3/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_3/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_3/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_3/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_4/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_4/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_4/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_4/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_4/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_4/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_5/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_5/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_5/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_5/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_5/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_5/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_6/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_6/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_6/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_6/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_6/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_6/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_7/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_7/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_7/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_7/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_7/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_7/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_8/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_8/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_8/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_8/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_8/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_8/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_9/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_9/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_9/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_9/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_9/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_9/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_10/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_10/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_10/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_10/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_10/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_10/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_11/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_11/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_11/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_11/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_11/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_11/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_12/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_12/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_12/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_attention_softmax Tensor(\"model_6/self_attention_1_12/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_12/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_12/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_13/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_13/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_13/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_13/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_13/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_13/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_14/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_14/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_14/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_14/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_14/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_14/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_15/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_15/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_15/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_15/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_15/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_15/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_16/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_16/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_16/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_16/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_16/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_16/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_17/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_17/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_17/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_17/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_17/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_17/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_18/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_18/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_18/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_18/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_18/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_18/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_1_19/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_1_19/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_1_19/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_1_19/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_1_19/mul:0\", shape=(?, 13, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_1_19/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention_2/Reshape_2:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention_2/Tanh:0\", shape=(?, 20, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention_2/Reshape_5:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention_2/transpose_3:0\", shape=(?, 20, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention_2/mul:0\", shape=(?, 20, 400), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention_2/Sum:0\", shape=(?, 400), dtype=float32)\n",
      "x_projection Tensor(\"model_6/self_attention/Reshape_2:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_tanh Tensor(\"model_6/self_attention/Tanh:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention Tensor(\"model_6/self_attention/Reshape_5:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"model_6/self_attention/transpose_3:0\", shape=(?, 13, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"model_6/self_attention/mul:0\", shape=(?, 13, 100), dtype=float32)\n",
      "x_attention_rep Tensor(\"model_6/self_attention/Sum:0\", shape=(?, 100), dtype=float32)\n",
      "concat Tensor(\"concat:0\", shape=(?, 6), dtype=float32)\n",
      "loss Tensor(\"concat:0\", shape=(?, 6), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 7, 20, 13)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "unstack_documents (Lambda)      [(None, 20, 13), (No 0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model (Model)                   (None, 1)            858580201   input_1[0][0]                    \n",
      "                                                                 unstack_documents[0][0]          \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 unstack_documents[0][1]          \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 unstack_documents[0][2]          \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 unstack_documents[0][3]          \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 unstack_documents[0][4]          \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 unstack_documents[0][5]          \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 unstack_documents[0][6]          \n",
      "==================================================================================================\n",
      "Total params: 858,580,201\n",
      "Trainable params: 221,201\n",
      "Non-trainable params: 858,359,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# total number of negative samples\n",
    "\n",
    "NUM_NEG_SAMPLES = 6\n",
    "TOTAL_NUM_SAMPLES = NUM_NEG_SAMPLES+1\n",
    "#input\n",
    "\n",
    "input_query = Input(shape=(MAX_Q_TERM,))\n",
    "input_docs = Input(shape=(TOTAL_NUM_SAMPLES, MAX_NUMBER_SNIPPETS, MAX_SNIPPET_LENGTH,))\n",
    "\n",
    "unstack_documents = Lambda(lambda x:unstack(x,axis=1), name=\"unstack_documents\")\n",
    "\n",
    "list_of_documents = unstack_documents(input_docs)\n",
    "print(\"list_of_documents[0]\",list_of_documents[0])\n",
    "scores = []\n",
    "for doc_tensor in list_of_documents:\n",
    "    print(\"input_query\", input_query)\n",
    "    print(\"doc_tensor\", doc_tensor)\n",
    "    scores.append(har_model([input_query, doc_tensor]))\n",
    "    \n",
    "har_trainable_model = Model(inputs=[input_query, input_docs], outputs=scores)\n",
    "\n",
    "#write tensor operation loss function\n",
    "\n",
    "\n",
    "pos_score = scores[0]\n",
    "maximum = [K.maximum(0.0,1.0 - pos_score + neg_score) for neg_score in scores[1:]]\n",
    "concat = K.concatenate(maximum)\n",
    "print(\"concat\",concat)\n",
    "\n",
    "loss = K.mean(K.sum(concat,axis=-1))\n",
    "\n",
    "print(\"loss\",concat)\n",
    "\n",
    "    \n",
    "har_trainable_model.add_loss(loss)\n",
    "\n",
    "\n",
    "har_trainable_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITE_GRAPH = False\n",
    "if WRITE_GRAPH:\n",
    "    from tensorflow.summary import FileWriter\n",
    "\n",
    "    graph = K.get_session().graph\n",
    "     # Your model implementation\n",
    "    #with graph.as_default():\n",
    "      # compile method actually creates the model in the graph.\n",
    "      #deepRank_model.compile(loss=identity_loss, optimizer='adam', metrics=['accuracy'])\n",
    "    writer = FileWriter(logdir='tensorboard/har', graph=graph)\n",
    "    writer.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open /backup/results/fast_method_relevant_results/train_data_deep_models_v2.tar.gz\n"
     ]
    }
   ],
   "source": [
    "path_dl_train = \"/backup/results/fast_method_relevant_results/train_data_deep_models_v2.tar.gz\"\n",
    "\n",
    "\n",
    "\n",
    "tar = tarfile.open(path_dl_train)\n",
    "#open\n",
    "print(\"Open\",path_dl_train)\n",
    "m = tar.getmembers()[0]\n",
    "f = tar.extractfile(m)\n",
    "train_articles_collection = pickle.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTILLY_POSITIVE_SAMPLES = 2\n",
    "NEGATIVE_SAMPLES = 4\n",
    "\n",
    "class TrainDataGenerator(object):\n",
    "    def __init__(self, article_collection, tokenizer, batch_queries_size):\n",
    "        \n",
    "        self.batch_size = batch_queries_size\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.train_data = article_collection[\"bioasq_data\"]\n",
    "        self.articles = article_collection[\"collection\"]\n",
    "        self.irrelevant_pmid = article_collection[\"irrelevant_pmid\"]\n",
    "        \n",
    "        self.num_steps = len(self.train_data)//self.batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        \n",
    "        query = []\n",
    "        query_doc = [] #(pos,neg,...,neg)\n",
    "        query_negative_doc = []\n",
    "\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            #stop condition\n",
    "            if len(query)>=self.batch_size:\n",
    "                #missing fill the gap for the missing query_terms\n",
    "                query = np.array(query)\n",
    "                #p=np.random.permutation(query.shape[0])\n",
    "                #query = query[p]\n",
    "                query_doc = np.array(query_doc)#[p]\n",
    "\n",
    "                \n",
    "                X = [query, query_doc]\n",
    "                #Y = [np.zeros((len(query))),np.zeros((len(query)))]\n",
    "                yield X\n",
    "\n",
    "\n",
    "                #reset\n",
    "                query = []\n",
    "                query_doc = []\n",
    "                query_negative_doc = []\n",
    "            \n",
    "            docs = []\n",
    "            \n",
    "            #select a random question\n",
    "            random_query_index = random.randint(0, len(self.train_data)-1) \n",
    "            query_data = self.train_data[random_query_index]\n",
    "            \n",
    "            #list of partilly relevant documents\n",
    "            partilly_positive_pmid_docs = query_data[\"partilly_positive_pmid\"]\n",
    "            tokenized_query = query_data[\"query\"][:MAX_Q_TERM]\n",
    "            \n",
    "            query.append(tokenized_query)\n",
    "            \n",
    "            #positive doc\n",
    "            #select a random positive\n",
    "            random_doc_index = random.randint(0, len(query_data[\"positive_pmid\"])-1) \n",
    "            doc_pmid = query_data[\"positive_pmid\"][random_doc_index]\n",
    "\n",
    "            tokenized_positive_doc = self.articles[doc_pmid]\n",
    "            positive_snippets = self.__snippet_split(tokenized_positive_doc)\n",
    "            \n",
    "            docs.append(positive_snippets)\n",
    "            \n",
    "            for j in range(PARTILLY_POSITIVE_SAMPLES+NEGATIVE_SAMPLES):\n",
    "                \n",
    "                \n",
    "                if j<PARTILLY_POSITIVE_SAMPLES:\n",
    "                    #select the partilly posivite doc\n",
    "                    random_ind = bisect(query_data[\"partially_positive_cumulative_prob\"],random.random())\n",
    "                    random_negative_doc_pmid = query_data[\"partilly_positive_pmid\"][random_ind]\n",
    "                    #print(self.__get_article(random_negative_doc_pmid))\n",
    "                    tokenized_negative_doc = self.articles[random_negative_doc_pmid]\n",
    "                    negative_snippets = self.__snippet_split( tokenized_negative_doc)\n",
    "                else:\n",
    "                    #select a random negative\n",
    "                    random_doc_index = random.randint(0, len(self.irrelevant_pmid)-1) \n",
    "                    doc_pmid = self.irrelevant_pmid[random_doc_index]\n",
    "                    \n",
    "                    tokenized_negative_doc = self.articles[doc_pmid]\n",
    "                    negative_snippets = self.__snippet_split( tokenized_negative_doc)\n",
    "                \n",
    "                #negative doc\n",
    "                docs.append(negative_snippets)\n",
    "\n",
    "            query_doc.append(docs)\n",
    "\n",
    "            \n",
    "    def __snippet_split(self, tokenized_doc, snippet_length=MAX_SNIPPET_LENGTH):\n",
    "        \n",
    "\n",
    "        if len(tokenized_doc) < MAX_DOCUMENT_TOKENS:\n",
    "            #pad\n",
    "            tokenized_doc += [0]*(MAX_DOCUMENT_TOKENS-len(tokenized_doc))\n",
    "            \n",
    "        else:\n",
    "            tokenized_doc = tokenized_doc[:MAX_DOCUMENT_TOKENS] #cut\n",
    "        \n",
    "        index_list = list(range(0, MAX_DOCUMENT_TOKENS, MAX_SNIPPET_LENGTH))+[MAX_DOCUMENT_TOKENS]\n",
    "        #print(tokenized_doc)\n",
    "        snippets = [ tokenized_doc[index_list[i]:index_list[i+1]] for i in range(len(index_list)-1)  ]\n",
    "        #print(snippets)\n",
    "        return snippets\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = TrainDataGenerator(train_articles_collection, tk, 128)\n",
    "data_generator = iter(data_generator)\n",
    "X = next(data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 13)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test (validation) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open /backup/results/fast_method_relevant_results/test_data_deep_models_v2.tar.gz\n"
     ]
    }
   ],
   "source": [
    "path_dl_test = \"/backup/results/fast_method_relevant_results/test_data_deep_models_v2.tar.gz\"\n",
    "\n",
    "\n",
    "\n",
    "tar = tarfile.open(path_dl_test)\n",
    "#open\n",
    "print(\"Open\",path_dl_test)\n",
    "m = tar.getmembers()[0]\n",
    "f = tar.extractfile(m)\n",
    "test_articles_collection = pickle.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataGenerator(object):\n",
    "    def __init__(self, article_collection, tokenizer):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.test_data = article_collection[\"bioasq_data\"] \n",
    "        self.articles = article_collection[\"collection\"]\n",
    "        \n",
    "        self.num_steps = len(self.test_data)\n",
    "        \n",
    "    \n",
    "    def __get_article(self, pmid):\n",
    "        return self.article_map(self.articles[pmid])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        \n",
    "        query = []\n",
    "        query_doc = []\n",
    "        \n",
    "\n",
    "        for query_data in self.test_data:\n",
    "\n",
    "\n",
    "            tokenized_query = query_data[\"query\"][:MAX_Q_TERM]\n",
    "\n",
    "            for doc_pmid in query_data[\"documents\"]:\n",
    "\n",
    "                tokenized_doc = self.articles[doc_pmid]\n",
    "                doc_snippets = self.__snippet_split(tokenized_doc)\n",
    "\n",
    "                ### add ###\n",
    "                query.append(tokenized_query)\n",
    "\n",
    "                #positive doc\n",
    "                query_doc.append(doc_snippets)\n",
    "\n",
    "\n",
    "            #missing fill the gap for the missing query_terms\n",
    "\n",
    "            X = [np.array(query), np.array(query_doc)]\n",
    "\n",
    "            yield X\n",
    "\n",
    "            #reset\n",
    "            query = []\n",
    "            query_doc = []\n",
    "\n",
    "                \n",
    "    def __snippet_split(self, tokenized_doc, snippet_length=MAX_SNIPPET_LENGTH):\n",
    "        \n",
    "\n",
    "        if len(tokenized_doc) < MAX_DOCUMENT_TOKENS:\n",
    "            #pad\n",
    "            tokenized_doc += [0]*(MAX_DOCUMENT_TOKENS-len(tokenized_doc))\n",
    "            \n",
    "        else:\n",
    "            tokenized_doc = tokenized_doc[:MAX_DOCUMENT_TOKENS] #cut\n",
    "        \n",
    "        index_list = list(range(0, MAX_DOCUMENT_TOKENS, MAX_SNIPPET_LENGTH))+[MAX_DOCUMENT_TOKENS]\n",
    "        #print(tokenized_doc)\n",
    "        snippets = [ tokenized_doc[index_list[i]:index_list[i+1]] for i in range(len(index_list)-1)  ]\n",
    "        #print(snippets)\n",
    "        return snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549\n",
      "82\n",
      "validation size 82 test size 549\n"
     ]
    }
   ],
   "source": [
    "validation_articles_collection = {\"bioasq_data\":[],\"collection\":test_articles_collection[\"collection\"]}\n",
    "\n",
    "_temp = test_articles_collection[\"bioasq_data\"][:]\n",
    "random.shuffle(_temp)\n",
    "print(len(_temp))\n",
    "\n",
    "validation_percentage = 0.15\n",
    "\n",
    "split_index = int(len(_temp)*validation_percentage)\n",
    "print(split_index)\n",
    "\n",
    "validation_articles_collection[\"bioasq_data\"] = _temp[:split_index]\n",
    "#test_articles_collection[\"bioasq_data\"] = _temp[split_index:]\n",
    "\n",
    "print(\"validation size\",len(validation_articles_collection[\"bioasq_data\"]),\"test size\",len(test_articles_collection[\"bioasq_data\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_test_data(data):\n",
    "    data_generator = TestDataGenerator(data, tk)\n",
    "    data_generator = iter(data_generator)\n",
    "\n",
    "    query_results = {}\n",
    "\n",
    "    for i,X in enumerate(data_generator):\n",
    "        print(\"Predict query:\",i,end=\"\\r\")\n",
    "        har_ranking = har_model.predict(X)\n",
    "        har_ranking = map(lambda x:x[0],har_ranking.tolist())\n",
    "        bm25_results = data[\"bioasq_data\"][i][\"documents\"]\n",
    "        har_ranking_pmid = list(zip(bm25_results,har_ranking))\n",
    "        har_ranking_pmid.sort(key=lambda x:-x[1])\n",
    "        query_results[data[\"bioasq_data\"][i][\"id\"]] = {\"result\":har_ranking_pmid,\"goldstandard\":data[\"bioasq_data\"][i][\"positive_pmid\"]}\n",
    "        #print(\"save query results:\",i,end=\"\\r\")\n",
    "        \n",
    "    return query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.generic_model import ModelAPI, f_recall, f_map\n",
    "\n",
    "def validation_score(deep_rank_test_query_results):\n",
    "    id_to_remove = []\n",
    "    for k,v in deep_rank_test_query_results.items():\n",
    "        if len(v[\"goldstandard\"]) == 0:\n",
    "            id_to_remove.append(k)\n",
    "\n",
    "    for k in id_to_remove:\n",
    "        del deep_rank_test_query_results[k]\n",
    "\n",
    "    deep_rank_test_query_results = list(deep_rank_test_query_results.values())\n",
    "\n",
    "    print(\"TEST set, len \",len(deep_rank_test_query_results))\n",
    "\n",
    "    expectations = list(map(lambda x:x[\"goldstandard\"],deep_rank_test_query_results))\n",
    "    predictions = list(map(lambda x:x[\"result\"],deep_rank_test_query_results))\n",
    "\n",
    "    #print(\"Recall:\",f_recall(predictions,expectations,at=1000))\n",
    "    bioasq_map = f_map(predictions,expectations,bioASQ=True)\n",
    "    print(\"MAP @10 bioASQ:\", bioasq_map)\n",
    "    print(\"MAP @25:\",f_map(predictions,expectations, bioASQ=True, at=25))\n",
    "    print(\"MAP @50:\",f_map(predictions,expectations, bioASQ=True, at=50))\n",
    "    print(\"MAP @100:\",f_map(predictions,expectations, bioASQ=True, at=100))\n",
    "    print(\"MAP @200:\",f_map(predictions,expectations, bioASQ=True, at=200))\n",
    "    print(\"MAP @300:\",f_map(predictions,expectations, bioASQ=True, at=300))\n",
    "\n",
    "    print(\"MAP:\",f_map(predictions,expectations, use_len=True))\n",
    "    \n",
    "    print(\"RECALL@10:\",f_recall(predictions,expectations, at=10))\n",
    "    print(\"RECALL@50:\",f_recall(predictions,expectations, at=50))\n",
    "    print(\"RECALL@100:\",f_recall(predictions,expectations, at=100))\n",
    "    return bioasq_map, predictions, expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"model\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"model\" during training.\n",
      "WARNING:tensorflow:Output \"model\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"model\" during training.\n",
      "WARNING:tensorflow:Output \"model\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"model\" during training.\n",
      "WARNING:tensorflow:Output \"model\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"model\" during training.\n",
      "WARNING:tensorflow:Output \"model\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"model\" during training.\n",
      "WARNING:tensorflow:Output \"model\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"model\" during training.\n",
      "WARNING:tensorflow:Output \"model\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"model\" during training.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam,  Adadelta\n",
    "\n",
    "#sgd = SGD(lr=0.001)\n",
    "#adam = Adam(lr=0.001)\n",
    "adadelta = Adadelta(lr=2)\n",
    "\n",
    "har_trainable_model.compile(optimizer=adadelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation = validate_test_data(validation_articles_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = validation_score(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoach: 0 | avg loss: 5.9780874 | max loss: 5.99463 | min loss: 5.9432344\n",
      "Epoach: 1 | avg loss: 4.905095 | max loss: 5.8984766 | min loss: 3.7576256\n",
      "Epoach: 2 | avg loss: 2.9906387 | max loss: 3.3897157 | min loss: 2.704164\n",
      "Epoach: 3 | avg loss: 2.8194482 | max loss: 2.949489 | min loss: 2.4600127\n",
      "Epoach: 4 | avg loss: 2.628375 | max loss: 2.8297503 | min loss: 2.4962373\n",
      "Epoach: 5 | avg loss: 2.685218 | max loss: 3.0441256 | min loss: 2.4424398\n",
      "Epoach: 6 | avg loss: 2.5781336 | max loss: 2.8284454 | min loss: 2.2068458\n",
      "Epoach: 7 | avg loss: 2.6436124 | max loss: 2.9304729 | min loss: 2.389176\n",
      "Epoach: 8 | avg loss: 2.4605103 | max loss: 2.64789 | min loss: 2.194899\n",
      "Epoach: 9 | avg loss: 2.491694 | max loss: 2.780827 | min loss: 2.0749748\n",
      "Epoach: 10 | avg loss: 2.4773917 | max loss: 2.5970063 | min loss: 2.3449244\n",
      "Epoach: 11 | avg loss: 2.595004 | max loss: 2.7311952 | min loss: 2.4138684\n",
      "Epoach: 12 | avg loss: 2.4845245 | max loss: 2.8401804 | min loss: 2.1962109\n",
      "Epoach: 13 | avg loss: 2.5415878 | max loss: 2.6870608 | min loss: 2.2921324\n",
      "Epoach: 14 | avg loss: 2.609613 | max loss: 2.9268003 | min loss: 2.3237607\n",
      "Epoach: 15 | avg loss: 2.3937035 | max loss: 2.5742588 | min loss: 2.0738935\n",
      "Epoach: 16 | avg loss: 2.3443718 | max loss: 2.6599321 | min loss: 2.1325612\n",
      "Epoach: 17 | avg loss: 2.3581395 | max loss: 2.7093124 | min loss: 2.047425\n",
      "Epoach: 18 | avg loss: 2.4524407 | max loss: 2.7854288 | min loss: 2.2114556\n",
      "Epoach: 19 | avg loss: 2.2969356 | max loss: 2.447682 | min loss: 2.0373378\n",
      "Epoach: 20 | avg loss: 2.3743665 | max loss: 2.6197007 | min loss: 2.1028738\n",
      "Epoach: 21 | avg loss: 2.4389815 | max loss: 2.6824157 | min loss: 2.2653582\n",
      "Epoach: 22 | avg loss: 2.210689 | max loss: 2.3620617 | min loss: 1.973933\n",
      "Epoach: 23 | avg loss: 2.1204925 | max loss: 2.4318852 | min loss: 1.8974464\n",
      "Epoach: 24 | avg loss: 1.9643753 | max loss: 2.28724 | min loss: 1.7591301\n",
      "Epoach: 25 | avg loss: 1.8595172 | max loss: 1.9433187 | min loss: 1.697963\n",
      "Epoach: 26 | avg loss: 1.8917596 | max loss: 2.022117 | min loss: 1.6858494\n",
      "Epoach: 27 | avg loss: 1.8547233 | max loss: 2.234895 | min loss: 1.6477046\n",
      "Epoach: 28 | avg loss: 1.7123307 | max loss: 1.9328023 | min loss: 1.6036886\n",
      "Epoach: 29 | avg loss: 1.6655715 | max loss: 1.869956 | min loss: 1.5220041\n",
      "Epoach: 30 | avg loss: 1.542407 | max loss: 1.7981727 | min loss: 1.3796129\n",
      "Epoach: 31 | avg loss: 1.5624098 | max loss: 1.689928 | min loss: 1.3875535\n",
      "Epoach: 32 | avg loss: 1.4992206 | max loss: 1.8673114 | min loss: 1.3302169\n",
      "Epoach: 33 | avg loss: 1.521507 | max loss: 1.5974046 | min loss: 1.4263021\n",
      "Epoach: 34 | avg loss: 1.5623419 | max loss: 1.7277248 | min loss: 1.3802528\n",
      "Epoach: 35 | avg loss: 1.4016501 | max loss: 1.5257232 | min loss: 1.3073474\n",
      "Epoach: 36 | avg loss: 1.4622712 | max loss: 1.5484133 | min loss: 1.3463483\n",
      "Epoach: 37 | avg loss: 1.3487068 | max loss: 1.4104111 | min loss: 1.233626\n",
      "Epoach: 38 | avg loss: 1.3614056 | max loss: 1.4372945 | min loss: 1.215651\n",
      "Epoach: 39 | avg loss: 1.2809007 | max loss: 1.4131112 | min loss: 1.1193931\n",
      "Epoach: 40 | avg loss: 1.2810105 | max loss: 1.3845927 | min loss: 1.1450512\n",
      "Epoach: 41 | avg loss: 1.233505 | max loss: 1.3913293 | min loss: 1.1187295\n",
      "Epoach: 42 | avg loss: 1.2277634 | max loss: 1.3219963 | min loss: 1.1337305\n",
      "Epoach: 43 | avg loss: 1.2544527 | max loss: 1.3728124 | min loss: 1.1267716\n",
      "Epoach: 44 | avg loss: 1.1713707 | max loss: 1.2868026 | min loss: 1.0845788\n",
      "Epoach: 45 | avg loss: 1.142194 | max loss: 1.257983 | min loss: 1.0047557\n",
      "Epoach: 46 | avg loss: 1.2142966 | max loss: 1.3223985 | min loss: 1.0618107\n",
      "Epoach: 47 | avg loss: 1.1592314 | max loss: 1.2579349 | min loss: 1.0720134\n",
      "Epoach: 48 | avg loss: 1.162452 | max loss: 1.2797453 | min loss: 0.99012506\n",
      "Epoach: 49 | avg loss: 1.0726779 | max loss: 1.1475031 | min loss: 0.9413074\n",
      "Epoach: 50 | avg loss: 1.1409993 | max loss: 1.2475965 | min loss: 0.9649783\n",
      "Epoach: 51 | avg loss: 1.097041 | max loss: 1.1795175 | min loss: 0.98717856\n",
      "Epoach: 52 | avg loss: 1.0393006 | max loss: 1.2230012 | min loss: 0.9345807\n",
      "Epoach: 53 | avg loss: 1.026203 | max loss: 1.1489805 | min loss: 0.94752\n",
      "Epoach: 54 | avg loss: 1.0768747 | max loss: 1.1743994 | min loss: 1.0113237\n",
      "Epoach: 55 | avg loss: 0.994049 | max loss: 1.1065917 | min loss: 0.9092121\n",
      "Epoach: 56 | avg loss: 1.0797826 | max loss: 1.2142751 | min loss: 1.0102161\n",
      "Epoach: 57 | avg loss: 1.0828724 | max loss: 1.2176023 | min loss: 0.99580914\n",
      "Epoach: 58 | avg loss: 1.0678916 | max loss: 1.2262983 | min loss: 0.8534225\n",
      "Epoach: 59 | avg loss: 1.0420332 | max loss: 1.1406393 | min loss: 0.85044384\n",
      "Epoach: 60 | avg loss: 0.98210657 | max loss: 1.1366241 | min loss: 0.8608227\n",
      "Epoach: 61 | avg loss: 0.9709005 | max loss: 1.0382161 | min loss: 0.887081\n",
      "Epoach: 62 | avg loss: 0.99226457 | max loss: 1.1369065 | min loss: 0.8357794\n",
      "Epoach: 63 | avg loss: 1.0187327 | max loss: 1.2073808 | min loss: 0.83997464\n",
      "Epoach: 64 | avg loss: 0.935206 | max loss: 1.0543458 | min loss: 0.89059633\n",
      "Epoach: 65 | avg loss: 0.9408858 | max loss: 1.0260777 | min loss: 0.79333544\n",
      "Epoach: 66 | avg loss: 0.88664097 | max loss: 0.9892197 | min loss: 0.7692667\n",
      "Epoach: 67 | avg loss: 0.9093869 | max loss: 1.1026584 | min loss: 0.7474739\n",
      "Epoach: 68 | avg loss: 0.89839363 | max loss: 1.0409017 | min loss: 0.74789596\n",
      "Epoach: 69 | avg loss: 0.859374 | max loss: 0.916062 | min loss: 0.7466327\n",
      "Epoach: 70 | avg loss: 0.93024033 | max loss: 1.111994 | min loss: 0.7667709\n",
      "Epoach: 71 | avg loss: 0.87595624 | max loss: 0.94238764 | min loss: 0.7962942\n",
      "Epoach: 72 | avg loss: 0.8944758 | max loss: 1.0149765 | min loss: 0.82197195\n",
      "Epoach: 73 | avg loss: 0.9097617 | max loss: 0.95777917 | min loss: 0.81043905\n",
      "Epoach: 74 | avg loss: 0.8929982 | max loss: 1.0212762 | min loss: 0.81749666\n",
      "Epoach: 75 | avg loss: 0.8922342 | max loss: 1.1215779 | min loss: 0.7880794\n",
      "Epoach: 76 | avg loss: 0.80893993 | max loss: 1.0845686 | min loss: 0.72912276\n",
      "Epoach: 77 | avg loss: 0.84086037 | max loss: 0.95950973 | min loss: 0.7236538\n",
      "Epoach: 78 | avg loss: 0.797345 | max loss: 0.8927901 | min loss: 0.7270044\n",
      "Epoach: 79 | avg loss: 0.84175575 | max loss: 0.94526994 | min loss: 0.66063255\n",
      "Epoach: 80 | avg loss: 0.82117873 | max loss: 0.96518 | min loss: 0.7443757\n",
      "Epoach: 81 | avg loss: 0.81779945 | max loss: 0.9784576 | min loss: 0.70948863\n",
      "Epoach: 82 | avg loss: 0.8270156 | max loss: 0.89822006 | min loss: 0.73044336\n",
      "Epoach: 83 | avg loss: 0.80012155 | max loss: 0.9032526 | min loss: 0.7598572\n",
      "Epoach: 84 | avg loss: 0.7958628 | max loss: 0.9893826 | min loss: 0.72271574\n",
      "Epoach: 85 | avg loss: 0.75286 | max loss: 0.8795844 | min loss: 0.6792761\n",
      "Epoach: 86 | avg loss: 0.7902687 | max loss: 0.9439776 | min loss: 0.65372205\n",
      "Epoach: 87 | avg loss: 0.780462 | max loss: 0.93893087 | min loss: 0.57058024\n",
      "Epoach: 88 | avg loss: 0.7943232 | max loss: 0.90674776 | min loss: 0.66871125\n",
      "Epoach: 89 | avg loss: 0.7847024 | max loss: 0.9160532 | min loss: 0.71545666\n",
      "Epoach: 90 | avg loss: 0.74923956 | max loss: 0.9082886 | min loss: 0.61277723\n",
      "Epoach: 91 | avg loss: 0.7235343 | max loss: 0.8050476 | min loss: 0.6407075\n",
      "Epoach: 92 | avg loss: 0.7746182 | max loss: 0.94909203 | min loss: 0.58679223\n",
      "Epoach: 93 | avg loss: 0.73757476 | max loss: 0.8835714 | min loss: 0.62064016\n",
      "Epoach: 94 | avg loss: 0.7006309 | max loss: 0.785855 | min loss: 0.6117137\n",
      "Epoach: 95 | avg loss: 0.7440672 | max loss: 0.84527266 | min loss: 0.6198202\n",
      "Epoach: 96 | avg loss: 0.7043189 | max loss: 0.8621309 | min loss: 0.5808015\n",
      "Epoach: 97 | avg loss: 0.6943765 | max loss: 0.7872144 | min loss: 0.6031387\n",
      "Epoach: 98 | avg loss: 0.7417793 | max loss: 0.86565435 | min loss: 0.61115026\n",
      "Epoach: 99 | avg loss: 0.69181657 | max loss: 0.71915054 | min loss: 0.6201602\n",
      "Epoach: 100 | avg loss: 0.71226454 | max loss: 0.78602016 | min loss: 0.6568879\n",
      "Epoach: 101 | avg loss: 0.6689999 | max loss: 0.72149825 | min loss: 0.60936654\n",
      "Epoach: 102 | avg loss: 0.70250547 | max loss: 0.76522005 | min loss: 0.63780963\n",
      "Epoach: 103 | avg loss: 0.72398424 | max loss: 0.8080021 | min loss: 0.6022655\n",
      "Epoach: 104 | avg loss: 0.73618597 | max loss: 0.83172405 | min loss: 0.6589365\n",
      "Epoach: 105 | avg loss: 0.6873716 | max loss: 0.8492228 | min loss: 0.5415654\n",
      "Epoach: 106 | avg loss: 0.6856278 | max loss: 0.87941945 | min loss: 0.5426867\n",
      "Epoach: 107 | avg loss: 0.64947534 | max loss: 0.7555987 | min loss: 0.5392301\n",
      "Epoach: 108 | avg loss: 0.69048536 | max loss: 0.79947674 | min loss: 0.6011533\n",
      "Epoach: 109 | avg loss: 0.66354394 | max loss: 0.7981707 | min loss: 0.6113767\n",
      "Epoach: 110 | avg loss: 0.65681493 | max loss: 0.7570534 | min loss: 0.5644398\n",
      "Epoach: 111 | avg loss: 0.698164 | max loss: 0.95473 | min loss: 0.6128936\n",
      "Epoach: 112 | avg loss: 0.6463368 | max loss: 0.70897 | min loss: 0.5272298\n",
      "Epoach: 113 | avg loss: 0.6985053 | max loss: 0.7742896 | min loss: 0.63962483\n",
      "Epoach: 114 | avg loss: 0.664688 | max loss: 0.8138097 | min loss: 0.48496664\n",
      "Epoach: 115 | avg loss: 0.65019935 | max loss: 0.7502313 | min loss: 0.5700055\n",
      "Epoach: 116 | avg loss: 0.6387339 | max loss: 0.75847256 | min loss: 0.5497521\n",
      "Epoach: 117 | avg loss: 0.6313432 | max loss: 0.7699226 | min loss: 0.53464603\n",
      "Epoach: 118 | avg loss: 0.6908138 | max loss: 0.8373207 | min loss: 0.5889224\n",
      "Epoach: 119 | avg loss: 0.6771698 | max loss: 0.7471247 | min loss: 0.62425184\n",
      "Epoach: 120 | avg loss: 0.65928316 | max loss: 0.8301623 | min loss: 0.52392673\n",
      "Epoach: 121 | avg loss: 0.65930843 | max loss: 0.78214145 | min loss: 0.48800832\n",
      "Epoach: 122 | avg loss: 0.65529835 | max loss: 0.7545198 | min loss: 0.5878449\n",
      "Epoach: 123 | avg loss: 0.6439022 | max loss: 0.7744088 | min loss: 0.5263699\n",
      "Epoach: 124 | avg loss: 0.60744256 | max loss: 0.6587608 | min loss: 0.55978316\n",
      "Epoach: 125 | avg loss: 0.59374267 | max loss: 0.69237614 | min loss: 0.5375333\n",
      "Epoach: 126 | avg loss: 0.6020177 | max loss: 0.73112184 | min loss: 0.5381518\n",
      "Epoach: 127 | avg loss: 0.5859036 | max loss: 0.71383715 | min loss: 0.4610639\n",
      "Epoach: 128 | avg loss: 0.61201316 | max loss: 0.75823414 | min loss: 0.44406387\n",
      "Epoach: 129 | avg loss: 0.6583324 | max loss: 0.76173365 | min loss: 0.56201494\n",
      "Epoach: 130 | avg loss: 0.6209163 | max loss: 0.7611158 | min loss: 0.51582\n",
      "Epoach: 131 | avg loss: 0.58922017 | max loss: 0.6545118 | min loss: 0.49517593\n",
      "Epoach: 132 | avg loss: 0.5557364 | max loss: 0.683005 | min loss: 0.44405216\n",
      "Epoach: 133 | avg loss: 0.55038977 | max loss: 0.655609 | min loss: 0.42640734\n",
      "Epoach: 134 | avg loss: 0.6134729 | max loss: 0.74969697 | min loss: 0.4787252\n",
      "Epoach: 135 | avg loss: 0.5611159 | max loss: 0.75524104 | min loss: 0.46958137\n",
      "Epoach: 136 | avg loss: 0.5766944 | max loss: 0.72496617 | min loss: 0.49865812\n",
      "Epoach: 137 | avg loss: 0.5430242 | max loss: 0.6089145 | min loss: 0.43318677\n",
      "Epoach: 138 | avg loss: 0.5716126 | max loss: 0.67351705 | min loss: 0.4834994\n",
      "Epoach: 139 | avg loss: 0.5471214 | max loss: 0.597075 | min loss: 0.46271202\n",
      "Epoach: 140 | avg loss: 0.5676613 | max loss: 0.6605309 | min loss: 0.4864361\n",
      "Epoach: 141 | avg loss: 0.5728836 | max loss: 0.69511104 | min loss: 0.50964725\n",
      "Epoach: 142 | avg loss: 0.59549654 | max loss: 0.6771568 | min loss: 0.5071621\n",
      "Epoach: 143 | avg loss: 0.5446222 | max loss: 0.678639 | min loss: 0.4115996\n",
      "Epoach: 144 | avg loss: 0.5664444 | max loss: 0.77585936 | min loss: 0.48580614\n",
      "Epoach: 145 | avg loss: 0.53858244 | max loss: 0.57703495 | min loss: 0.49383995\n",
      "Epoach: 146 | avg loss: 0.5412052 | max loss: 0.6401731 | min loss: 0.45783156\n",
      "Epoach: 147 | avg loss: 0.5364645 | max loss: 0.65554696 | min loss: 0.4702422\n",
      "Epoach: 148 | avg loss: 0.5610204 | max loss: 0.6737663 | min loss: 0.468998\n",
      "Epoach: 149 | avg loss: 0.55193007 | max loss: 0.6059048 | min loss: 0.41683093\n",
      "Epoach: 150 | avg loss: 0.6048594 | max loss: 0.75615454 | min loss: 0.46841595\n",
      "Epoach: 151 | avg loss: 0.5640281 | max loss: 0.66599166 | min loss: 0.46616665\n",
      "Epoach: 152 | avg loss: 0.5611761 | max loss: 0.6187646 | min loss: 0.51233715\n",
      "Epoach: 153 | avg loss: 0.56836456 | max loss: 0.67356277 | min loss: 0.45521745\n",
      "Epoach: 154 | avg loss: 0.571473 | max loss: 0.66299427 | min loss: 0.45734778\n",
      "Epoach: 155 | avg loss: 0.5391092 | max loss: 0.6038761 | min loss: 0.4568079\n",
      "Epoach: 156 | avg loss: 0.5261129 | max loss: 0.6492528 | min loss: 0.42924815\n",
      "Epoach: 157 | avg loss: 0.47607374 | max loss: 0.5481219 | min loss: 0.38851267\n",
      "Epoach: 158 | avg loss: 0.5027197 | max loss: 0.59720063 | min loss: 0.3669651\n",
      "Epoach: 159 | avg loss: 0.51133907 | max loss: 0.58494043 | min loss: 0.44866076\n",
      "Epoach: 160 | avg loss: 0.50907063 | max loss: 0.5936918 | min loss: 0.4517646\n",
      "Epoach: 161 | avg loss: 0.5280941 | max loss: 0.6068733 | min loss: 0.4366027\n",
      "Epoach: 162 | avg loss: 0.48723918 | max loss: 0.6281823 | min loss: 0.3917992\n",
      "Epoach: 163 | avg loss: 0.50090873 | max loss: 0.59353703 | min loss: 0.45891523\n",
      "Epoach: 164 | avg loss: 0.5370444 | max loss: 0.67718875 | min loss: 0.41735184\n",
      "Epoach: 165 | avg loss: 0.51267636 | max loss: 0.57083017 | min loss: 0.43967533\n",
      "Epoach: 166 | avg loss: 0.52919817 | max loss: 0.64130306 | min loss: 0.38914847\n",
      "Epoach: 167 | avg loss: 0.4710936 | max loss: 0.54974186 | min loss: 0.4020148\n",
      "Epoach: 168 | avg loss: 0.50110745 | max loss: 0.54549706 | min loss: 0.46145192\n",
      "Epoach: 169 | avg loss: 0.49494484 | max loss: 0.5894722 | min loss: 0.36734825\n",
      "Epoach: 170 | avg loss: 0.5422605 | max loss: 0.6474099 | min loss: 0.46986023\n",
      "Epoach: 171 | avg loss: 0.48516458 | max loss: 0.5631871 | min loss: 0.44545704\n",
      "Epoach: 172 | avg loss: 0.4902472 | max loss: 0.60369253 | min loss: 0.35971227\n",
      "Epoach: 173 | avg loss: 0.54221684 | max loss: 0.67703414 | min loss: 0.44050384\n",
      "Epoach: 174 | avg loss: 0.47342104 | max loss: 0.5404783 | min loss: 0.38800937\n",
      "Epoach: 175 | avg loss: 0.5347667 | max loss: 0.6178648 | min loss: 0.46794105\n",
      "Epoach: 176 | avg loss: 0.4787957 | max loss: 0.5677042 | min loss: 0.34619474\n",
      "Epoach: 177 | avg loss: 0.49771306 | max loss: 0.54903924 | min loss: 0.44951144\n",
      "Epoach: 178 | avg loss: 0.47974068 | max loss: 0.59686345 | min loss: 0.38127694\n",
      "Epoach: 179 | avg loss: 0.45797455 | max loss: 0.55279267 | min loss: 0.3675576\n",
      "Epoach: 180 | avg loss: 0.4656181 | max loss: 0.55038506 | min loss: 0.4305457\n",
      "Epoach: 181 | avg loss: 0.48831338 | max loss: 0.5846508 | min loss: 0.43272284\n",
      "Epoach: 182 | avg loss: 0.44936192 | max loss: 0.5019616 | min loss: 0.40751737\n",
      "Epoach: 183 | avg loss: 0.5097659 | max loss: 0.6372379 | min loss: 0.45042032\n",
      "Epoach: 184 | avg loss: 0.53295755 | max loss: 0.59614563 | min loss: 0.46169478\n",
      "Epoach: 185 | avg loss: 0.49761102 | max loss: 0.619636 | min loss: 0.44645578\n",
      "Epoach: 186 | avg loss: 0.45915195 | max loss: 0.5557845 | min loss: 0.38532114\n",
      "Epoach: 187 | avg loss: 0.4635127 | max loss: 0.5810809 | min loss: 0.38553727\n",
      "Epoach: 188 | avg loss: 0.5174029 | max loss: 0.5954187 | min loss: 0.38542408\n",
      "Epoach: 189 | avg loss: 0.46192688 | max loss: 0.541762 | min loss: 0.3739981\n",
      "Epoach: 190 | avg loss: 0.46233958 | max loss: 0.52940786 | min loss: 0.37405127\n",
      "Epoach: 191 | avg loss: 0.47103685 | max loss: 0.54220885 | min loss: 0.36030138\n",
      "Epoach: 192 | avg loss: 0.4434317 | max loss: 0.49993065 | min loss: 0.3476065\n",
      "Epoach: 193 | avg loss: 0.44737953 | max loss: 0.5175049 | min loss: 0.36683983\n",
      "Epoach: 194 | avg loss: 0.46034005 | max loss: 0.5784026 | min loss: 0.3707763\n",
      "Epoach: 195 | avg loss: 0.43389553 | max loss: 0.5228306 | min loss: 0.35374406\n",
      "Epoach: 196 | avg loss: 0.42167646 | max loss: 0.5362216 | min loss: 0.32309714\n",
      "Epoach: 197 | avg loss: 0.47312352 | max loss: 0.5756947 | min loss: 0.34831208\n",
      "Epoach: 198 | avg loss: 0.42086798 | max loss: 0.45516503 | min loss: 0.34462592\n",
      "Epoach: 199 | avg loss: 0.4193983 | max loss: 0.5131086 | min loss: 0.38641962\n",
      "Epoach: 200 | avg loss: 0.459244 | max loss: 0.4870106 | min loss: 0.39227945\n",
      "Epoach: 201 | avg loss: 0.4449171 | max loss: 0.55326974 | min loss: 0.33911157\n",
      "Epoach: 202 | avg loss: 0.43211097 | max loss: 0.5551057 | min loss: 0.2777602\n",
      "Epoach: 203 | avg loss: 0.44923043 | max loss: 0.5353396 | min loss: 0.41361707\n",
      "Epoach: 204 | avg loss: 0.4438328 | max loss: 0.5487628 | min loss: 0.34512442\n",
      "Epoach: 205 | avg loss: 0.4401412 | max loss: 0.51566 | min loss: 0.3250758\n",
      "Epoach: 206 | avg loss: 0.45797426 | max loss: 0.5295466 | min loss: 0.42394105\n",
      "Epoach: 207 | avg loss: 0.4303391 | max loss: 0.503896 | min loss: 0.3698328\n",
      "Epoach: 208 | avg loss: 0.46938306 | max loss: 0.5344311 | min loss: 0.43434024\n",
      "Epoach: 209 | avg loss: 0.41618118 | max loss: 0.47590956 | min loss: 0.35844442\n",
      "Epoach: 210 | avg loss: 0.45383272 | max loss: 0.52943814 | min loss: 0.3991217\n",
      "Epoach: 211 | avg loss: 0.4468105 | max loss: 0.55683583 | min loss: 0.3298636\n",
      "Epoach: 212 | avg loss: 0.46632674 | max loss: 0.5336813 | min loss: 0.4233501\n",
      "Epoach: 213 | avg loss: 0.44099343 | max loss: 0.5465752 | min loss: 0.37915856\n",
      "Epoach: 214 | avg loss: 0.40640128 | max loss: 0.4780641 | min loss: 0.34165472\n",
      "Epoach: 215 | avg loss: 0.44389662 | max loss: 0.5370783 | min loss: 0.35703793\n",
      "Epoach: 216 | avg loss: 0.40372628 | max loss: 0.50185436 | min loss: 0.2628373\n",
      "Epoach: 217 | avg loss: 0.47670743 | max loss: 0.56632423 | min loss: 0.34849572\n",
      "Epoach: 218 | avg loss: 0.38344994 | max loss: 0.41735268 | min loss: 0.3263274\n",
      "Epoach: 219 | avg loss: 0.40648395 | max loss: 0.52850676 | min loss: 0.31574827\n",
      "Epoach: 220 | avg loss: 0.42760855 | max loss: 0.50398874 | min loss: 0.3553598\n",
      "Epoach: 221 | avg loss: 0.42139685 | max loss: 0.49498636 | min loss: 0.3531658\n",
      "Epoach: 222 | avg loss: 0.41738033 | max loss: 0.5181808 | min loss: 0.351992\n",
      "Epoach: 223 | avg loss: 0.46277618 | max loss: 0.57030594 | min loss: 0.38329232\n",
      "Epoach: 224 | avg loss: 0.44026214 | max loss: 0.54250836 | min loss: 0.342009\n",
      "Epoach: 225 | avg loss: 0.41375482 | max loss: 0.4663179 | min loss: 0.3211931\n",
      "Epoach: 226 | avg loss: 0.3872746 | max loss: 0.45310915 | min loss: 0.3222948\n",
      "Epoach: 227 | avg loss: 0.41758317 | max loss: 0.58989835 | min loss: 0.30723304\n",
      "Epoach: 228 | avg loss: 0.34625983 | max loss: 0.39288414 | min loss: 0.30822444\n",
      "Epoach: 229 | avg loss: 0.50651073 | max loss: 0.5769627 | min loss: 0.42024142\n",
      "Epoach: 230 | avg loss: 0.4124887 | max loss: 0.49606946 | min loss: 0.32685298\n",
      "Epoach: 231 | avg loss: 0.39544928 | max loss: 0.5103904 | min loss: 0.29076648\n",
      "Epoach: 232 | avg loss: 0.42169404 | max loss: 0.49222612 | min loss: 0.34574887\n",
      "Epoach: 233 | avg loss: 0.4119692 | max loss: 0.50555766 | min loss: 0.34795982\n",
      "Epoach: 234 | avg loss: 0.42581433 | max loss: 0.5597672 | min loss: 0.3034637\n",
      "Epoach: 235 | avg loss: 0.35605264 | max loss: 0.42098022 | min loss: 0.28704083\n",
      "Epoach: 236 | avg loss: 0.39191002 | max loss: 0.47494403 | min loss: 0.2687551\n",
      "Epoach: 237 | avg loss: 0.42147803 | max loss: 0.48876548 | min loss: 0.3783381\n",
      "Epoach: 238 | avg loss: 0.4147076 | max loss: 0.46798378 | min loss: 0.3562926\n",
      "Epoach: 239 | avg loss: 0.42787406 | max loss: 0.54623604 | min loss: 0.33522582\n",
      "Epoach: 240 | avg loss: 0.40570715 | max loss: 0.49411094 | min loss: 0.3386447\n",
      "Epoach: 241 | avg loss: 0.40083182 | max loss: 0.44650012 | min loss: 0.34015298\n",
      "Epoach: 242 | avg loss: 0.39143503 | max loss: 0.4340413 | min loss: 0.3524983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoach: 243 | avg loss: 0.40735593 | max loss: 0.5042627 | min loss: 0.31701452\n",
      "Epoach: 244 | avg loss: 0.38371906 | max loss: 0.47881553 | min loss: 0.3237277\n",
      "Epoach: 245 | avg loss: 0.41004854 | max loss: 0.51557785 | min loss: 0.3346104\n",
      "Epoach: 246 | avg loss: 0.38782218 | max loss: 0.45546472 | min loss: 0.29507425\n",
      "Epoach: 247 | avg loss: 0.3810554 | max loss: 0.47742644 | min loss: 0.2977942\n",
      "Epoach: 248 | avg loss: 0.387599 | max loss: 0.4583307 | min loss: 0.3143763\n",
      "Epoach: 249 | avg loss: 0.36149937 | max loss: 0.40152442 | min loss: 0.30163467\n",
      "Epoach: 250 | avg loss: 0.37099195 | max loss: 0.43509474 | min loss: 0.30106178\n",
      "Epoach: 251 | avg loss: 0.35733464 | max loss: 0.40697438 | min loss: 0.3056534\n",
      "Epoach: 252 | avg loss: 0.41914988 | max loss: 0.5357771 | min loss: 0.3324978\n",
      "Epoach: 253 | avg loss: 0.38033384 | max loss: 0.44137362 | min loss: 0.30842757\n",
      "Epoach: 254 | avg loss: 0.39263186 | max loss: 0.43971306 | min loss: 0.30840763\n",
      "Epoach: 255 | avg loss: 0.3638817 | max loss: 0.49077535 | min loss: 0.2785602\n",
      "Epoach: 256 | avg loss: 0.3653368 | max loss: 0.41045332 | min loss: 0.2999615\n",
      "Epoach: 257 | avg loss: 0.3825694 | max loss: 0.46335024 | min loss: 0.3361529\n",
      "Epoach: 258 | avg loss: 0.35250545 | max loss: 0.4280415 | min loss: 0.28206304\n",
      "Epoach: 259 | avg loss: 0.37505412 | max loss: 0.4754682 | min loss: 0.30087584\n",
      "Epoach: 260 | avg loss: 0.3826769 | max loss: 0.43816024 | min loss: 0.31257683\n",
      "Epoach: 261 | avg loss: 0.40522677 | max loss: 0.49757782 | min loss: 0.36341494\n",
      "Epoach: 262 | avg loss: 0.41730562 | max loss: 0.57381344 | min loss: 0.358432\n",
      "Epoach: 263 | avg loss: 0.40405864 | max loss: 0.47457352 | min loss: 0.3326818\n",
      "Epoach: 264 | avg loss: 0.37055063 | max loss: 0.44153202 | min loss: 0.28832093\n",
      "Epoach: 265 | avg loss: 0.36452737 | max loss: 0.44055712 | min loss: 0.31108138\n",
      "Epoach: 266 | avg loss: 0.34055594 | max loss: 0.41039902 | min loss: 0.271436\n",
      "Epoach: 267 | avg loss: 0.3805063 | max loss: 0.4515595 | min loss: 0.32260683\n",
      "Epoach: 268 | avg loss: 0.39546722 | max loss: 0.53969884 | min loss: 0.3150956\n",
      "Epoach: 269 | avg loss: 0.34552932 | max loss: 0.48445487 | min loss: 0.28966653\n",
      "Epoach: 270 | avg loss: 0.36500704 | max loss: 0.413933 | min loss: 0.30750072\n",
      "Epoach: 271 | avg loss: 0.3705896 | max loss: 0.4550383 | min loss: 0.2757223\n",
      "Epoach: 272 | avg loss: 0.36103436 | max loss: 0.44972488 | min loss: 0.2651304\n",
      "Epoach: 273 | avg loss: 0.36189735 | max loss: 0.45039392 | min loss: 0.26770866\n",
      "Epoach: 274 | avg loss: 0.36744058 | max loss: 0.44400123 | min loss: 0.29416212\n",
      "Epoach: 275 | avg loss: 0.36305434 | max loss: 0.38314793 | min loss: 0.31603104\n",
      "Epoach: 276 | avg loss: 0.38747948 | max loss: 0.51041436 | min loss: 0.29597747\n",
      "Epoach: 277 | avg loss: 0.36760324 | max loss: 0.43008506 | min loss: 0.2559909\n",
      "Epoach: 278 | avg loss: 0.41271973 | max loss: 0.5456639 | min loss: 0.34194967\n",
      "Epoach: 279 | avg loss: 0.3675218 | max loss: 0.43353918 | min loss: 0.31286538\n",
      "Epoach: 280 | avg loss: 0.3382963 | max loss: 0.4119321 | min loss: 0.24540493\n",
      "Epoach: 281 | avg loss: 0.36521906 | max loss: 0.46428335 | min loss: 0.29493728\n",
      "Epoach: 282 | avg loss: 0.38101295 | max loss: 0.4339694 | min loss: 0.33915114\n",
      "Epoach: 283 | avg loss: 0.38427377 | max loss: 0.5251879 | min loss: 0.29383975\n",
      "Epoach: 284 | avg loss: 0.35588473 | max loss: 0.42328027 | min loss: 0.2593666\n",
      "Epoach: 285 | avg loss: 0.37480348 | max loss: 0.46237528 | min loss: 0.2415978\n",
      "Epoach: 286 | avg loss: 0.37292445 | max loss: 0.43345022 | min loss: 0.32240593\n",
      "Epoach: 287 | avg loss: 0.3348344 | max loss: 0.4388215 | min loss: 0.27567846\n",
      "Epoach: 288 | avg loss: 0.3532914 | max loss: 0.42302322 | min loss: 0.30822843\n",
      "Epoach: 289 | avg loss: 0.36467165 | max loss: 0.45284674 | min loss: 0.2941166\n",
      "Epoach: 290 | avg loss: 0.3555635 | max loss: 0.3930501 | min loss: 0.32196385\n",
      "Epoach: 291 | avg loss: 0.3437379 | max loss: 0.46300477 | min loss: 0.2561049\n",
      "Epoach: 292 | avg loss: 0.37290692 | max loss: 0.44519496 | min loss: 0.31881374\n",
      "Epoach: 293 | avg loss: 0.3307099 | max loss: 0.39812756 | min loss: 0.2447119\n",
      "Epoach: 294 | avg loss: 0.35683638 | max loss: 0.45708823 | min loss: 0.25742662\n",
      "Epoach: 295 | avg loss: 0.36042 | max loss: 0.44636703 | min loss: 0.25378138\n",
      "Epoach: 296 | avg loss: 0.35381436 | max loss: 0.4218495 | min loss: 0.2628907\n",
      "Epoach: 297 | avg loss: 0.3356889 | max loss: 0.39784473 | min loss: 0.24273431\n",
      "Epoach: 298 | avg loss: 0.3562366 | max loss: 0.42945942 | min loss: 0.26067293\n",
      "Epoach: 299 | avg loss: 0.37695208 | max loss: 0.46519983 | min loss: 0.3237934\n",
      "Epoach: 300 | avg loss: 0.3458924 | max loss: 0.41517046 | min loss: 0.26464498\n",
      "Epoach: 301 | avg loss: 0.35801244 | max loss: 0.43221286 | min loss: 0.24659407\n",
      "Epoach: 302 | avg loss: 0.34910536 | max loss: 0.4005872 | min loss: 0.28373468\n",
      "Epoach: 303 | avg loss: 0.36889142 | max loss: 0.45544192 | min loss: 0.29630667\n",
      "Epoach: 304 | avg loss: 0.34055516 | max loss: 0.5294894 | min loss: 0.2531842\n",
      "Epoach: 305 | avg loss: 0.3592859 | max loss: 0.48091587 | min loss: 0.28200018\n",
      "Epoach: 306 | avg loss: 0.31633168 | max loss: 0.3433525 | min loss: 0.25997472\n",
      "Epoach: 307 | avg loss: 0.35109442 | max loss: 0.41545036 | min loss: 0.26562697\n",
      "Epoach: 308 | avg loss: 0.34838635 | max loss: 0.38945377 | min loss: 0.31655788\n",
      "Epoach: 309 | avg loss: 0.34756005 | max loss: 0.41910243 | min loss: 0.27400118\n",
      "Epoach: 310 | avg loss: 0.3345706 | max loss: 0.39229035 | min loss: 0.26129636\n",
      "Epoach: 311 | avg loss: 0.37862423 | max loss: 0.44742563 | min loss: 0.29198694\n",
      "Epoach: 312 | avg loss: 0.28814566 | max loss: 0.31949553 | min loss: 0.22735973\n",
      "Epoach: 313 | avg loss: 0.35485315 | max loss: 0.4944259 | min loss: 0.28533155\n",
      "Epoach: 314 | avg loss: 0.3145002 | max loss: 0.42407364 | min loss: 0.25178626\n",
      "Epoach: 315 | avg loss: 0.33158165 | max loss: 0.40307134 | min loss: 0.26121128\n",
      "Epoach: 316 | avg loss: 0.3375162 | max loss: 0.41708368 | min loss: 0.26295945\n",
      "Epoach: 317 | avg loss: 0.35118794 | max loss: 0.40090325 | min loss: 0.27481052\n",
      "Epoach: 318 | avg loss: 0.3326463 | max loss: 0.38240755 | min loss: 0.27470186\n",
      "Epoach: 319 | avg loss: 0.296228 | max loss: 0.3491573 | min loss: 0.25718695\n",
      "Epoach: 320 | avg loss: 0.33652884 | max loss: 0.4504025 | min loss: 0.28496367\n",
      "Epoach: 321 | avg loss: 0.31137723 | max loss: 0.38085938 | min loss: 0.20597354\n",
      "Epoach: 322 | avg loss: 0.32882428 | max loss: 0.36018598 | min loss: 0.28373492\n",
      "Epoach: 323 | avg loss: 0.37083995 | max loss: 0.4384532 | min loss: 0.29119778\n",
      "Epoach: 324 | avg loss: 0.31354767 | max loss: 0.36147445 | min loss: 0.22613277\n",
      "Epoach: 325 | avg loss: 0.3217702 | max loss: 0.3812627 | min loss: 0.27265382\n",
      "Epoach: 326 | avg loss: 0.34247366 | max loss: 0.45251426 | min loss: 0.24599221\n",
      "Epoach: 327 | avg loss: 0.32168445 | max loss: 0.4298815 | min loss: 0.2478041\n",
      "Epoach: 328 | avg loss: 0.3231333 | max loss: 0.36634982 | min loss: 0.27778015\n",
      "Epoach: 329 | avg loss: 0.31921798 | max loss: 0.4494064 | min loss: 0.1852825\n",
      "Epoach: 330 | avg loss: 0.3250592 | max loss: 0.42799705 | min loss: 0.22103181\n",
      "Epoach: 331 | avg loss: 0.3199144 | max loss: 0.37485403 | min loss: 0.2911891\n",
      "Epoach: 332 | avg loss: 0.3080485 | max loss: 0.41631544 | min loss: 0.26730305\n",
      "Epoach: 333 | avg loss: 0.31001237 | max loss: 0.45373398 | min loss: 0.20765668\n",
      "Epoach: 334 | avg loss: 0.3084644 | max loss: 0.38124323 | min loss: 0.2430446\n",
      "Epoach: 335 | avg loss: 0.32688546 | max loss: 0.42368293 | min loss: 0.22338876\n",
      "Epoach: 336 | avg loss: 0.30568975 | max loss: 0.42364997 | min loss: 0.25456658\n",
      "Epoach: 337 | avg loss: 0.34004065 | max loss: 0.40325454 | min loss: 0.26771188\n",
      "Epoach: 338 | avg loss: 0.30244887 | max loss: 0.43693697 | min loss: 0.2057643\n",
      "Epoach: 339 | avg loss: 0.32200533 | max loss: 0.42598727 | min loss: 0.2605095\n",
      "Epoach: 340 | avg loss: 0.3192137 | max loss: 0.35488296 | min loss: 0.28201512\n",
      "Epoach: 341 | avg loss: 0.30282718 | max loss: 0.40364712 | min loss: 0.21438883\n",
      "Epoach: 342 | avg loss: 0.32446235 | max loss: 0.39224446 | min loss: 0.2421075\n",
      "Epoach: 343 | avg loss: 0.31548566 | max loss: 0.37554848 | min loss: 0.22570841\n",
      "Epoach: 344 | avg loss: 0.34070548 | max loss: 0.39060354 | min loss: 0.30150104\n",
      "Epoach: 345 | avg loss: 0.3244542 | max loss: 0.39725637 | min loss: 0.20976457\n",
      "Epoach: 346 | avg loss: 0.34228587 | max loss: 0.41204315 | min loss: 0.3032717\n",
      "Epoach: 347 | avg loss: 0.31864214 | max loss: 0.45897865 | min loss: 0.19400041\n",
      "Epoach: 348 | avg loss: 0.30640566 | max loss: 0.36408904 | min loss: 0.25107855\n",
      "Epoach: 349 | avg loss: 0.3359234 | max loss: 0.45552075 | min loss: 0.28840283\n",
      "Epoach: 350 | avg loss: 0.32167384 | max loss: 0.42827106 | min loss: 0.22650425\n",
      "Epoach: 351 | avg loss: 0.3221473 | max loss: 0.39623904 | min loss: 0.25204593\n",
      "Epoach: 352 | avg loss: 0.32079363 | max loss: 0.37204024 | min loss: 0.21376492\n",
      "Epoach: 353 | avg loss: 0.33651298 | max loss: 0.42773062 | min loss: 0.25093406\n",
      "Epoach: 354 | avg loss: 0.30619058 | max loss: 0.37513816 | min loss: 0.26234502\n",
      "Epoach: 355 | avg loss: 0.3223455 | max loss: 0.38073766 | min loss: 0.25069034\n",
      "Epoach: 356 | avg loss: 0.32750538 | max loss: 0.3687564 | min loss: 0.27716687\n",
      "Epoach: 357 | avg loss: 0.33286023 | max loss: 0.40918154 | min loss: 0.21565953\n",
      "Epoach: 358 | avg loss: 0.30824763 | max loss: 0.40449995 | min loss: 0.25030595\n",
      "Epoach: 359 | avg loss: 0.2962598 | max loss: 0.37492013 | min loss: 0.25758326\n",
      "Epoach: 360 | avg loss: 0.29955116 | max loss: 0.3644443 | min loss: 0.25258303\n",
      "Epoach: 361 | avg loss: 0.29733706 | max loss: 0.33907154 | min loss: 0.23981166\n",
      "Epoach: 362 | avg loss: 0.2784963 | max loss: 0.32959402 | min loss: 0.22376631\n",
      "Epoach: 363 | avg loss: 0.2930376 | max loss: 0.3740096 | min loss: 0.2543382\n",
      "Epoach: 364 | avg loss: 0.3178276 | max loss: 0.35640308 | min loss: 0.23888126\n",
      "Epoach: 365 | avg loss: 0.29833722 | max loss: 0.364605 | min loss: 0.22352588\n",
      "Epoach: 366 | avg loss: 0.3115456 | max loss: 0.3700633 | min loss: 0.22120863\n",
      "Epoach: 367 | avg loss: 0.31436843 | max loss: 0.37671417 | min loss: 0.22438522\n",
      "Epoach: 368 | avg loss: 0.3148796 | max loss: 0.40831524 | min loss: 0.22870867\n",
      "Epoach: 369 | avg loss: 0.29277617 | max loss: 0.37537724 | min loss: 0.22297648\n",
      "Epoach: 370 | avg loss: 0.29659742 | max loss: 0.36829305 | min loss: 0.20240244\n",
      "Epoach: 371 | avg loss: 0.29305524 | max loss: 0.37122425 | min loss: 0.2130861\n",
      "Epoach: 372 | avg loss: 0.30116913 | max loss: 0.39889857 | min loss: 0.2531084\n",
      "Epoach: 373 | avg loss: 0.29565585 | max loss: 0.35799646 | min loss: 0.20618111\n",
      "Epoach: 374 | avg loss: 0.27841675 | max loss: 0.35226578 | min loss: 0.2294288\n",
      "Epoach: 375 | avg loss: 0.3161283 | max loss: 0.35970065 | min loss: 0.2254512\n",
      "Epoach: 376 | avg loss: 0.3288067 | max loss: 0.5069753 | min loss: 0.23158601\n",
      "Epoach: 377 | avg loss: 0.29477963 | max loss: 0.37271976 | min loss: 0.21692818\n",
      "Epoach: 378 | avg loss: 0.30559725 | max loss: 0.35475367 | min loss: 0.2326614\n",
      "Epoach: 379 | avg loss: 0.2999193 | max loss: 0.33308068 | min loss: 0.24687874\n",
      "Epoach: 380 | avg loss: 0.320894 | max loss: 0.38951463 | min loss: 0.22885862\n",
      "Epoach: 381 | avg loss: 0.27272993 | max loss: 0.36815104 | min loss: 0.1820579\n",
      "Epoach: 382 | avg loss: 0.31189388 | max loss: 0.44413027 | min loss: 0.23279828\n",
      "Epoach: 383 | avg loss: 0.28203624 | max loss: 0.3819792 | min loss: 0.21756968\n",
      "Epoach: 384 | avg loss: 0.27976382 | max loss: 0.3606699 | min loss: 0.24223338\n",
      "Epoach: 385 | avg loss: 0.28475535 | max loss: 0.32378432 | min loss: 0.25061145\n",
      "Epoach: 386 | avg loss: 0.30753416 | max loss: 0.36212635 | min loss: 0.27080116\n",
      "Epoach: 387 | avg loss: 0.2749383 | max loss: 0.39793244 | min loss: 0.2207995\n",
      "Epoach: 388 | avg loss: 0.28375623 | max loss: 0.34750062 | min loss: 0.21604407\n",
      "Epoach: 389 | avg loss: 0.31179166 | max loss: 0.34700233 | min loss: 0.27474013\n",
      "Epoach: 390 | avg loss: 0.30152348 | max loss: 0.39996555 | min loss: 0.24361937\n",
      "Epoach: 391 | avg loss: 0.28306723 | max loss: 0.35839236 | min loss: 0.19528046\n",
      "Epoach: 392 | avg loss: 0.27980584 | max loss: 0.33740526 | min loss: 0.22010587\n",
      "Epoach: 393 | avg loss: 0.29192308 | max loss: 0.32119802 | min loss: 0.24628426\n",
      "Epoach: 394 | avg loss: 0.28837264 | max loss: 0.33455732 | min loss: 0.21687454\n",
      "Epoach: 395 | avg loss: 0.29900092 | max loss: 0.38561904 | min loss: 0.25541762\n",
      "Epoach: 396 | avg loss: 0.30013853 | max loss: 0.38671917 | min loss: 0.21284921\n",
      "Epoach: 397 | avg loss: 0.28355503 | max loss: 0.3160229 | min loss: 0.2507202\n",
      "Epoach: 398 | avg loss: 0.2982961 | max loss: 0.407006 | min loss: 0.2281234\n",
      "Epoach: 399 | avg loss: 0.2908712 | max loss: 0.3530132 | min loss: 0.20804764\n",
      "Epoach: 400 | avg loss: 0.28233838 | max loss: 0.35283792 | min loss: 0.1990493\n",
      "Epoach: 401 | avg loss: 0.27912226 | max loss: 0.38764083 | min loss: 0.17770699\n",
      "Epoach: 402 | avg loss: 0.28870553 | max loss: 0.33367318 | min loss: 0.21932666\n",
      "Epoach: 403 | avg loss: 0.3030039 | max loss: 0.38478476 | min loss: 0.24472216\n",
      "Epoach: 404 | avg loss: 0.25786895 | max loss: 0.29098427 | min loss: 0.22164713\n",
      "Epoach: 405 | avg loss: 0.28795737 | max loss: 0.40287217 | min loss: 0.21582231\n",
      "Epoach: 406 | avg loss: 0.28821328 | max loss: 0.33135545 | min loss: 0.19860858\n",
      "Epoach: 407 | avg loss: 0.29866588 | max loss: 0.4140431 | min loss: 0.21089512\n",
      "Epoach: 408 | avg loss: 0.28639725 | max loss: 0.32029253 | min loss: 0.24543597\n",
      "Epoach: 409 | avg loss: 0.28076696 | max loss: 0.33148944 | min loss: 0.23826858\n",
      "Epoach: 410 | avg loss: 0.26696667 | max loss: 0.3169673 | min loss: 0.18943319\n",
      "Epoach: 411 | avg loss: 0.27645987 | max loss: 0.35725528 | min loss: 0.23076302\n",
      "Epoach: 412 | avg loss: 0.2431779 | max loss: 0.31776863 | min loss: 0.17559591\n",
      "Epoach: 413 | avg loss: 0.26467448 | max loss: 0.3305627 | min loss: 0.1762349\n",
      "Epoach: 414 | avg loss: 0.3070326 | max loss: 0.35123554 | min loss: 0.25427085\n",
      "Epoach: 415 | avg loss: 0.31704777 | max loss: 0.41817588 | min loss: 0.25064838\n",
      "Epoach: 416 | avg loss: 0.29044998 | max loss: 0.369623 | min loss: 0.22960567\n",
      "Epoach: 417 | avg loss: 0.29634386 | max loss: 0.3610251 | min loss: 0.21290939\n",
      "Epoach: 418 | avg loss: 0.2920649 | max loss: 0.3774925 | min loss: 0.21654183\n",
      "Epoach: 419 | avg loss: 0.31160274 | max loss: 0.38080424 | min loss: 0.27246037\n",
      "Epoach: 420 | avg loss: 0.23820987 | max loss: 0.3031213 | min loss: 0.18881461\n",
      "Epoach: 421 | avg loss: 0.29564565 | max loss: 0.37585786 | min loss: 0.2231074\n",
      "Epoach: 422 | avg loss: 0.33228913 | max loss: 0.36186838 | min loss: 0.30842137\n",
      "Epoach: 423 | avg loss: 0.28525692 | max loss: 0.334338 | min loss: 0.23667435\n",
      "Epoach: 424 | avg loss: 0.28886843 | max loss: 0.34225237 | min loss: 0.24810201\n",
      "Epoach: 425 | avg loss: 0.26157624 | max loss: 0.32998016 | min loss: 0.19845751\n",
      "Epoach: 426 | avg loss: 0.25298423 | max loss: 0.32787275 | min loss: 0.18159671\n",
      "Epoach: 427 | avg loss: 0.28206426 | max loss: 0.3973027 | min loss: 0.22429809\n",
      "Epoach: 428 | avg loss: 0.27988517 | max loss: 0.34177673 | min loss: 0.2309191\n",
      "Epoach: 429 | avg loss: 0.2565783 | max loss: 0.31627405 | min loss: 0.1981813\n",
      "Epoach: 430 | avg loss: 0.24977803 | max loss: 0.26485604 | min loss: 0.204356\n",
      "Epoach: 431 | avg loss: 0.28084692 | max loss: 0.33989096 | min loss: 0.19199571\n",
      "Epoach: 432 | avg loss: 0.24781835 | max loss: 0.35217643 | min loss: 0.15881687\n",
      "Epoach: 433 | avg loss: 0.2606985 | max loss: 0.30348718 | min loss: 0.21379128\n",
      "Epoach: 434 | avg loss: 0.2678573 | max loss: 0.3410841 | min loss: 0.2052505\n",
      "Epoach: 435 | avg loss: 0.27847782 | max loss: 0.34789678 | min loss: 0.21306428\n",
      "Epoach: 436 | avg loss: 0.2745862 | max loss: 0.31886113 | min loss: 0.22420476\n",
      "Epoach: 437 | avg loss: 0.2900841 | max loss: 0.3645718 | min loss: 0.21377903\n",
      "Epoach: 438 | avg loss: 0.2673666 | max loss: 0.3431955 | min loss: 0.2033957\n",
      "Epoach: 439 | avg loss: 0.24984807 | max loss: 0.2917267 | min loss: 0.17231205\n",
      "Epoach: 440 | avg loss: 0.27018493 | max loss: 0.3223409 | min loss: 0.24125057\n",
      "Epoach: 441 | avg loss: 0.28669125 | max loss: 0.35027182 | min loss: 0.22206436\n",
      "Epoach: 442 | avg loss: 0.25494948 | max loss: 0.36492425 | min loss: 0.15309694\n",
      "Epoach: 443 | avg loss: 0.2882193 | max loss: 0.41261095 | min loss: 0.23216517\n",
      "Epoach: 444 | avg loss: 0.24831763 | max loss: 0.32101893 | min loss: 0.20346089\n",
      "Epoach: 445 | avg loss: 0.25659132 | max loss: 0.35006785 | min loss: 0.21141094\n",
      "Epoach: 446 | avg loss: 0.25887838 | max loss: 0.29170543 | min loss: 0.1617034\n",
      "Epoach: 447 | avg loss: 0.287824 | max loss: 0.32241082 | min loss: 0.20468\n",
      "Epoach: 448 | avg loss: 0.28503525 | max loss: 0.34428912 | min loss: 0.22587796\n",
      "Epoach: 449 | avg loss: 0.25252718 | max loss: 0.30106607 | min loss: 0.15855482\n",
      "Epoach: 450 | avg loss: 0.27149394 | max loss: 0.41585466 | min loss: 0.17639855\n",
      "Epoach: 451 | avg loss: 0.25070935 | max loss: 0.29364806 | min loss: 0.20924616\n",
      "Epoach: 452 | avg loss: 0.27616298 | max loss: 0.31574267 | min loss: 0.23123594\n",
      "Epoach: 453 | avg loss: 0.26611385 | max loss: 0.37401697 | min loss: 0.16877335\n",
      "Epoach: 454 | avg loss: 0.25352955 | max loss: 0.3325475 | min loss: 0.12806377\n",
      "Epoach: 455 | avg loss: 0.309891 | max loss: 0.37605697 | min loss: 0.24184015\n",
      "Epoach: 456 | avg loss: 0.22971302 | max loss: 0.2534817 | min loss: 0.19196805\n",
      "Epoach: 457 | avg loss: 0.25179854 | max loss: 0.27397102 | min loss: 0.22589506\n",
      "Epoach: 458 | avg loss: 0.2640772 | max loss: 0.31436294 | min loss: 0.20076936\n",
      "Epoach: 459 | avg loss: 0.2732744 | max loss: 0.39162055 | min loss: 0.20339155\n",
      "Epoach: 460 | avg loss: 0.24761526 | max loss: 0.36787313 | min loss: 0.16778459\n",
      "Epoach: 461 | avg loss: 0.29374278 | max loss: 0.3753479 | min loss: 0.23520322\n",
      "Epoach: 462 | avg loss: 0.22590575 | max loss: 0.26867712 | min loss: 0.16572882\n",
      "Epoach: 463 | avg loss: 0.2309564 | max loss: 0.26805872 | min loss: 0.18239771\n",
      "Epoach: 464 | avg loss: 0.25810182 | max loss: 0.29343826 | min loss: 0.21421997\n",
      "Epoach: 465 | avg loss: 0.27391967 | max loss: 0.33845645 | min loss: 0.20047945\n",
      "Epoach: 466 | avg loss: 0.2914186 | max loss: 0.32568103 | min loss: 0.25478634\n",
      "Epoach: 467 | avg loss: 0.2506488 | max loss: 0.35238218 | min loss: 0.1835992\n",
      "Epoach: 468 | avg loss: 0.24513537 | max loss: 0.2725966 | min loss: 0.20092523\n",
      "Epoach: 469 | avg loss: 0.24180841 | max loss: 0.3087926 | min loss: 0.1939202\n",
      "Epoach: 470 | avg loss: 0.28677243 | max loss: 0.46101958 | min loss: 0.23811164\n",
      "Epoach: 471 | avg loss: 0.23328388 | max loss: 0.2812149 | min loss: 0.18315549\n",
      "Epoach: 472 | avg loss: 0.26860052 | max loss: 0.32296646 | min loss: 0.21093951\n",
      "Epoach: 473 | avg loss: 0.25576967 | max loss: 0.3012535 | min loss: 0.18399674\n",
      "Epoach: 474 | avg loss: 0.2487796 | max loss: 0.32418236 | min loss: 0.17826545\n",
      "Epoach: 475 | avg loss: 0.24170828 | max loss: 0.3063294 | min loss: 0.18459632\n",
      "Epoach: 476 | avg loss: 0.27614993 | max loss: 0.31362832 | min loss: 0.22451156\n",
      "Epoach: 477 | avg loss: 0.23965609 | max loss: 0.28666264 | min loss: 0.20027912\n",
      "Epoach: 478 | avg loss: 0.25296885 | max loss: 0.29771274 | min loss: 0.2090576\n",
      "Epoach: 479 | avg loss: 0.24241605 | max loss: 0.35125405 | min loss: 0.16454372\n",
      "Epoach: 480 | avg loss: 0.25360504 | max loss: 0.31404635 | min loss: 0.18379597\n",
      "Epoach: 481 | avg loss: 0.24031864 | max loss: 0.2747991 | min loss: 0.17602871\n",
      "Epoach: 482 | avg loss: 0.25215146 | max loss: 0.27882412 | min loss: 0.22598472\n",
      "Epoach: 483 | avg loss: 0.24581696 | max loss: 0.29157653 | min loss: 0.17511845\n",
      "Epoach: 484 | avg loss: 0.25703728 | max loss: 0.28455254 | min loss: 0.20614214\n",
      "Epoach: 485 | avg loss: 0.26469585 | max loss: 0.3145266 | min loss: 0.20826325\n",
      "Epoach: 486 | avg loss: 0.2949335 | max loss: 0.38472292 | min loss: 0.18550467\n",
      "Epoach: 487 | avg loss: 0.24116969 | max loss: 0.2770569 | min loss: 0.1898362\n",
      "Epoach: 488 | avg loss: 0.27740175 | max loss: 0.32830653 | min loss: 0.2300554\n",
      "Epoach: 489 | avg loss: 0.2573118 | max loss: 0.29685467 | min loss: 0.2068811\n",
      "Epoach: 490 | avg loss: 0.23610184 | max loss: 0.30651248 | min loss: 0.16721573\n",
      "Epoach: 491 | avg loss: 0.23400182 | max loss: 0.31284976 | min loss: 0.15515347\n",
      "Epoach: 492 | avg loss: 0.26220098 | max loss: 0.32455567 | min loss: 0.21207166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoach: 493 | avg loss: 0.25434887 | max loss: 0.29023743 | min loss: 0.21306252\n",
      "Epoach: 494 | avg loss: 0.23801884 | max loss: 0.3016582 | min loss: 0.17613351\n",
      "Epoach: 495 | avg loss: 0.22312634 | max loss: 0.28792608 | min loss: 0.16094565\n",
      "Epoach: 496 | avg loss: 0.25431412 | max loss: 0.35450387 | min loss: 0.16501051\n",
      "Epoach: 497 | avg loss: 0.20831385 | max loss: 0.29539222 | min loss: 0.16311947\n",
      "Epoach: 498 | avg loss: 0.25394672 | max loss: 0.2960355 | min loss: 0.1797258\n",
      "Epoach: 499 | avg loss: 0.26470274 | max loss: 0.34599465 | min loss: 0.18545547\n",
      "Epoach: 500 | avg loss: 0.22826105 | max loss: 0.3097149 | min loss: 0.18781382\n",
      "Epoach: 501 | avg loss: 0.22862191 | max loss: 0.30208138 | min loss: 0.1372377\n",
      "Epoach: 502 | avg loss: 0.2519961 | max loss: 0.38351086 | min loss: 0.19869298\n",
      "Epoach: 503 | avg loss: 0.27166843 | max loss: 0.32903162 | min loss: 0.20956878\n",
      "Epoach: 504 | avg loss: 0.27050254 | max loss: 0.32535392 | min loss: 0.21622923\n",
      "Epoach: 505 | avg loss: 0.24580842 | max loss: 0.34287602 | min loss: 0.18866882\n",
      "Epoach: 506 | avg loss: 0.24718156 | max loss: 0.31681794 | min loss: 0.20025265\n",
      "Epoach: 507 | avg loss: 0.2653494 | max loss: 0.33001274 | min loss: 0.17666647\n",
      "Epoach: 508 | avg loss: 0.25824904 | max loss: 0.36163872 | min loss: 0.17287132\n",
      "Epoach: 509 | avg loss: 0.25021642 | max loss: 0.3031631 | min loss: 0.19247803\n",
      "Epoach: 510 | avg loss: 0.28333622 | max loss: 0.3299843 | min loss: 0.23227316\n",
      "Epoach: 511 | avg loss: 0.23206857 | max loss: 0.29835862 | min loss: 0.17268655\n",
      "Epoach: 512 | avg loss: 0.24910298 | max loss: 0.3072005 | min loss: 0.1759175\n",
      "Epoach: 513 | avg loss: 0.2412262 | max loss: 0.28262183 | min loss: 0.20308766\n",
      "Epoach: 514 | avg loss: 0.24056306 | max loss: 0.33546937 | min loss: 0.1775657\n",
      "Epoach: 515 | avg loss: 0.26192877 | max loss: 0.36103547 | min loss: 0.20928216\n",
      "Epoach: 516 | avg loss: 0.2720574 | max loss: 0.35811466 | min loss: 0.18872312\n",
      "Epoach: 517 | avg loss: 0.24609332 | max loss: 0.2994057 | min loss: 0.21235643\n",
      "Epoach: 518 | avg loss: 0.22950831 | max loss: 0.25623512 | min loss: 0.18849027\n",
      "Epoach: 519 | avg loss: 0.23623113 | max loss: 0.29264483 | min loss: 0.17647639\n",
      "Epoach: 520 | avg loss: 0.22683367 | max loss: 0.2735632 | min loss: 0.15176935\n",
      "Epoach: 521 | avg loss: 0.2354279 | max loss: 0.29879484 | min loss: 0.18436503\n",
      "Epoach: 522 | avg loss: 0.24372092 | max loss: 0.30256897 | min loss: 0.1736129\n",
      "Epoach: 523 | avg loss: 0.24372318 | max loss: 0.31697458 | min loss: 0.19010776\n",
      "Epoach: 524 | avg loss: 0.2043754 | max loss: 0.2883793 | min loss: 0.12195289\n",
      "Epoach: 525 | avg loss: 0.23762298 | max loss: 0.3449908 | min loss: 0.18046686\n",
      "Epoach: 526 | avg loss: 0.2617502 | max loss: 0.3234787 | min loss: 0.21577287\n",
      "Epoach: 527 | avg loss: 0.22894847 | max loss: 0.3159951 | min loss: 0.17833582\n",
      "Epoach: 528 | avg loss: 0.22203305 | max loss: 0.30558327 | min loss: 0.18661916\n",
      "Epoach: 529 | avg loss: 0.22373173 | max loss: 0.3046782 | min loss: 0.16639528\n",
      "Epoach: 530 | avg loss: 0.21550487 | max loss: 0.28237897 | min loss: 0.15975714\n",
      "Epoach: 531 | avg loss: 0.258401 | max loss: 0.34985268 | min loss: 0.1983779\n",
      "Epoach: 532 | avg loss: 0.2418235 | max loss: 0.29867005 | min loss: 0.17657867\n",
      "Epoach: 533 | avg loss: 0.26834163 | max loss: 0.326639 | min loss: 0.23028138\n",
      "Epoach: 534 | avg loss: 0.23455867 | max loss: 0.3112404 | min loss: 0.14333035\n",
      "Epoach: 535 | avg loss: 0.23834498 | max loss: 0.28447658 | min loss: 0.17846465\n",
      "Epoach: 536 | avg loss: 0.25463012 | max loss: 0.32917157 | min loss: 0.19166936\n",
      "Epoach: 537 | avg loss: 0.24755031 | max loss: 0.31204915 | min loss: 0.15694422\n",
      "Epoach: 538 | avg loss: 0.20028403 | max loss: 0.3119437 | min loss: 0.12326516\n",
      "Epoach: 539 | avg loss: 0.2359766 | max loss: 0.275397 | min loss: 0.17091882\n",
      "Epoach: 540 | avg loss: 0.23630357 | max loss: 0.2649712 | min loss: 0.1999157\n",
      "Epoach: 541 | avg loss: 0.24115983 | max loss: 0.30550146 | min loss: 0.14078951\n",
      "Epoach: 542 | avg loss: 0.23587596 | max loss: 0.29150248 | min loss: 0.19798514\n",
      "Epoach: 543 | avg loss: 0.19640882 | max loss: 0.23281461 | min loss: 0.12498693\n",
      "Epoach: 544 | avg loss: 0.2371319 | max loss: 0.28405625 | min loss: 0.18557066\n",
      "Epoach: 545 | avg loss: 0.24158837 | max loss: 0.30808005 | min loss: 0.15742457\n",
      "Epoach: 546 | avg loss: 0.2349765 | max loss: 0.29663682 | min loss: 0.19203041\n",
      "Epoach: 547 | avg loss: 0.25111485 | max loss: 0.30273247 | min loss: 0.19585288\n",
      "Epoach: 548 | avg loss: 0.21495616 | max loss: 0.26262176 | min loss: 0.17124501\n",
      "Epoach: 549 | avg loss: 0.23969033 | max loss: 0.30854782 | min loss: 0.17757562\n",
      "Epoach: 550 | avg loss: 0.24232063 | max loss: 0.33565375 | min loss: 0.16580059\n",
      "Epoach: 551 | avg loss: 0.23131488 | max loss: 0.30446234 | min loss: 0.19283605\n",
      "Epoach: 552 | avg loss: 0.24672224 | max loss: 0.30248523 | min loss: 0.16560706\n",
      "Epoach: 553 | avg loss: 0.24252857 | max loss: 0.29084617 | min loss: 0.15177143\n",
      "Epoach: 554 | avg loss: 0.2586128 | max loss: 0.3229611 | min loss: 0.21299742\n",
      "Epoach: 555 | avg loss: 0.23707798 | max loss: 0.30126172 | min loss: 0.17146666\n",
      "Epoach: 556 | avg loss: 0.22662729 | max loss: 0.28675377 | min loss: 0.17603575\n",
      "Epoach: 557 | avg loss: 0.21550624 | max loss: 0.3069548 | min loss: 0.15760124\n",
      "Epoach: 558 | avg loss: 0.23171894 | max loss: 0.29080182 | min loss: 0.1606552\n",
      "Epoach: 559 | avg loss: 0.23703644 | max loss: 0.26358566 | min loss: 0.20660616\n",
      "Epoach: 560 | avg loss: 0.20572768 | max loss: 0.25953737 | min loss: 0.15050212\n",
      "Epoach: 561 | avg loss: 0.20703614 | max loss: 0.27200985 | min loss: 0.1365258\n",
      "Epoach: 562 | avg loss: 0.2516997 | max loss: 0.33032975 | min loss: 0.19057491\n",
      "Epoach: 563 | avg loss: 0.2201084 | max loss: 0.27574122 | min loss: 0.13030444\n",
      "Epoach: 564 | avg loss: 0.24045938 | max loss: 0.30501798 | min loss: 0.1788916\n",
      "Epoach: 565 | avg loss: 0.21245536 | max loss: 0.26526967 | min loss: 0.18172695\n",
      "Epoach: 566 | avg loss: 0.24914187 | max loss: 0.28973985 | min loss: 0.19973186\n",
      "Epoach: 567 | avg loss: 0.2182762 | max loss: 0.2468386 | min loss: 0.16304892\n",
      "Epoach: 568 | avg loss: 0.2196174 | max loss: 0.25213295 | min loss: 0.169877\n",
      "Epoach: 569 | avg loss: 0.22920163 | max loss: 0.3068066 | min loss: 0.18434876\n",
      "Epoach: 570 | avg loss: 0.22064531 | max loss: 0.29228285 | min loss: 0.18744212\n",
      "Epoach: 571 | avg loss: 0.21321331 | max loss: 0.28872746 | min loss: 0.15790124\n",
      "Epoach: 572 | avg loss: 0.22397664 | max loss: 0.2522516 | min loss: 0.18001729\n",
      "Epoach: 573 | avg loss: 0.226403 | max loss: 0.31553578 | min loss: 0.15856372\n",
      "Epoach: 574 | avg loss: 0.23821476 | max loss: 0.28602475 | min loss: 0.17858465\n",
      "Epoach: 575 | avg loss: 0.22245193 | max loss: 0.27315184 | min loss: 0.1616221\n",
      "Epoach: 576 | avg loss: 0.21847677 | max loss: 0.28409797 | min loss: 0.15840343\n",
      "Epoach: 577 | avg loss: 0.22890453 | max loss: 0.29183885 | min loss: 0.15393688\n",
      "Epoach: 578 | avg loss: 0.22109598 | max loss: 0.25425214 | min loss: 0.1535102\n",
      "Epoach: 579 | avg loss: 0.21933886 | max loss: 0.2890119 | min loss: 0.17725581\n",
      "Epoach: 580 | avg loss: 0.22355059 | max loss: 0.27521515 | min loss: 0.14135233\n",
      "Epoach: 581 | avg loss: 0.22262591 | max loss: 0.26573217 | min loss: 0.18944845\n",
      "Epoach: 582 | avg loss: 0.23532686 | max loss: 0.29028794 | min loss: 0.18949021\n",
      "Epoach: 583 | avg loss: 0.20185444 | max loss: 0.29887444 | min loss: 0.123999484\n",
      "Epoach: 584 | avg loss: 0.21440083 | max loss: 0.30829522 | min loss: 0.14171487\n",
      "Epoach: 585 | avg loss: 0.24600042 | max loss: 0.3038146 | min loss: 0.18169132\n",
      "Epoach: 586 | avg loss: 0.20458654 | max loss: 0.23733908 | min loss: 0.14193338\n",
      "Epoach: 587 | avg loss: 0.22101001 | max loss: 0.2577894 | min loss: 0.171974\n",
      "Epoach: 588 | avg loss: 0.2206926 | max loss: 0.2826965 | min loss: 0.16734482\n",
      "Epoach: 589 | avg loss: 0.21492867 | max loss: 0.28112978 | min loss: 0.15084374\n",
      "Epoach: 590 | avg loss: 0.20707557 | max loss: 0.26403183 | min loss: 0.14216784\n",
      "Epoach: 591 | avg loss: 0.2149058 | max loss: 0.26556545 | min loss: 0.16024831\n",
      "Epoach: 592 | avg loss: 0.22273968 | max loss: 0.24574223 | min loss: 0.1993598\n",
      "Epoach: 593 | avg loss: 0.2106381 | max loss: 0.25466105 | min loss: 0.14861053\n",
      "Epoach: 594 | avg loss: 0.21207947 | max loss: 0.30183056 | min loss: 0.16171607\n",
      "Epoach: 595 | avg loss: 0.22859311 | max loss: 0.2716291 | min loss: 0.16745853\n",
      "Epoach: 596 | avg loss: 0.2077674 | max loss: 0.23800805 | min loss: 0.15185265\n",
      "Epoach: 597 | avg loss: 0.22700867 | max loss: 0.33300924 | min loss: 0.16545406\n",
      "Epoach: 598 | avg loss: 0.22481683 | max loss: 0.31353414 | min loss: 0.15330617\n",
      "Epoach: 599 | avg loss: 0.2517905 | max loss: 0.3132836 | min loss: 0.17566353\n",
      "Epoach: 600 | avg loss: 0.2041664 | max loss: 0.27883616 | min loss: 0.15418886\n",
      "Epoach: 601 | avg loss: 0.21151891 | max loss: 0.23900126 | min loss: 0.18682703\n",
      "Epoach: 602 | avg loss: 0.22189124 | max loss: 0.25510055 | min loss: 0.17977779\n",
      "Epoach: 603 | avg loss: 0.22588447 | max loss: 0.26224467 | min loss: 0.18375489\n",
      "Epoach: 604 | avg loss: 0.22961766 | max loss: 0.34281218 | min loss: 0.19114593\n",
      "Epoach: 605 | avg loss: 0.20353839 | max loss: 0.25134718 | min loss: 0.1663073\n",
      "Epoach: 606 | avg loss: 0.21684363 | max loss: 0.282409 | min loss: 0.16116874\n",
      "Epoach: 607 | avg loss: 0.22796074 | max loss: 0.2771328 | min loss: 0.17264482\n",
      "Epoach: 608 | avg loss: 0.21906751 | max loss: 0.25555626 | min loss: 0.18402535\n",
      "Epoach: 609 | avg loss: 0.24253185 | max loss: 0.28704607 | min loss: 0.1947828\n",
      "Epoach: 610 | avg loss: 0.21202138 | max loss: 0.2676025 | min loss: 0.12729102\n",
      "Epoach: 611 | avg loss: 0.21488443 | max loss: 0.28816473 | min loss: 0.1638399\n",
      "Epoach: 612 | avg loss: 0.22706503 | max loss: 0.28341618 | min loss: 0.14321123\n",
      "Epoach: 613 | avg loss: 0.21562195 | max loss: 0.29277837 | min loss: 0.17366612\n",
      "Epoach: 614 | avg loss: 0.24053062 | max loss: 0.3183813 | min loss: 0.1835455\n",
      "Epoach: 615 | avg loss: 0.21975428 | max loss: 0.30512524 | min loss: 0.17987077\n",
      "Epoach: 616 | avg loss: 0.21381226 | max loss: 0.27877408 | min loss: 0.1384534\n",
      "Epoach: 617 | avg loss: 0.22111323 | max loss: 0.27257025 | min loss: 0.17935295\n",
      "Epoach: 618 | avg loss: 0.21807128 | max loss: 0.26658362 | min loss: 0.18293418\n",
      "Epoach: 619 | avg loss: 0.20515302 | max loss: 0.26621312 | min loss: 0.1396544\n",
      "Epoach: 620 | avg loss: 0.21241781 | max loss: 0.2550975 | min loss: 0.16215971\n",
      "Epoach: 621 | avg loss: 0.18751742 | max loss: 0.30088228 | min loss: 0.10564205\n",
      "Epoach: 622 | avg loss: 0.19789198 | max loss: 0.27785853 | min loss: 0.15851289\n",
      "Epoach: 623 | avg loss: 0.2211626 | max loss: 0.26368314 | min loss: 0.18016906\n",
      "Epoach: 624 | avg loss: 0.2154111 | max loss: 0.25056297 | min loss: 0.1850394\n",
      "Epoach: 625 | avg loss: 0.19133036 | max loss: 0.24147311 | min loss: 0.14482376\n",
      "Epoach: 626 | avg loss: 0.20908889 | max loss: 0.22602448 | min loss: 0.16277324\n",
      "Epoach: 627 | avg loss: 0.22251575 | max loss: 0.29019672 | min loss: 0.1434893\n",
      "Epoach: 628 | avg loss: 0.2095944 | max loss: 0.27042723 | min loss: 0.1721785\n",
      "Epoach: 629 | avg loss: 0.21285316 | max loss: 0.2889468 | min loss: 0.14571021\n",
      "Epoach: 630 | avg loss: 0.22030154 | max loss: 0.24006893 | min loss: 0.19361636\n",
      "Epoach: 631 | avg loss: 0.19318801 | max loss: 0.27427077 | min loss: 0.10980998\n",
      "Epoach: 632 | avg loss: 0.20789498 | max loss: 0.28334284 | min loss: 0.15157495\n",
      "Epoach: 633 | avg loss: 0.2109799 | max loss: 0.29256475 | min loss: 0.14635018\n",
      "Epoach: 634 | avg loss: 0.21027225 | max loss: 0.30530035 | min loss: 0.15067825\n",
      "Epoach: 635 | avg loss: 0.2059139 | max loss: 0.30116493 | min loss: 0.14069957\n",
      "Epoach: 636 | avg loss: 0.20315233 | max loss: 0.24074093 | min loss: 0.14870512\n",
      "Epoach: 637 | avg loss: 0.2033976 | max loss: 0.24599546 | min loss: 0.16287555\n",
      "Epoach: 638 | avg loss: 0.21332425 | max loss: 0.29964077 | min loss: 0.14355147\n",
      "Epoach: 639 | avg loss: 0.19303815 | max loss: 0.24155368 | min loss: 0.14721474\n",
      "Epoach: 640 | avg loss: 0.22924504 | max loss: 0.29248986 | min loss: 0.18888521\n",
      "Epoach: 641 | avg loss: 0.22111657 | max loss: 0.29139745 | min loss: 0.18525594\n",
      "Epoach: 642 | avg loss: 0.21086234 | max loss: 0.28719118 | min loss: 0.1439566\n",
      "Epoach: 643 | avg loss: 0.20896795 | max loss: 0.27886012 | min loss: 0.17834972\n",
      "Epoach: 644 | avg loss: 0.1870355 | max loss: 0.22678822 | min loss: 0.102910064\n",
      "Epoach: 645 | avg loss: 0.18827192 | max loss: 0.20956066 | min loss: 0.14455289\n",
      "Epoach: 646 | avg loss: 0.2268104 | max loss: 0.30323112 | min loss: 0.149053\n",
      "Epoach: 647 | avg loss: 0.1864544 | max loss: 0.23947924 | min loss: 0.1477016\n",
      "Epoach: 648 | avg loss: 0.22644725 | max loss: 0.28973007 | min loss: 0.17465614\n",
      "Epoach: 649 | avg loss: 0.19711867 | max loss: 0.2831479 | min loss: 0.14386562\n",
      "Epoach: 650 | avg loss: 0.1747872 | max loss: 0.2099475 | min loss: 0.10343734\n",
      "Epoach: 651 | avg loss: 0.20475209 | max loss: 0.26988393 | min loss: 0.1519891\n",
      "Epoach: 652 | avg loss: 0.18089902 | max loss: 0.24030209 | min loss: 0.113957435\n",
      "Epoach: 653 | avg loss: 0.20793734 | max loss: 0.2759543 | min loss: 0.1399983\n",
      "Epoach: 654 | avg loss: 0.21011524 | max loss: 0.2689156 | min loss: 0.14173308\n",
      "Epoach: 655 | avg loss: 0.23333786 | max loss: 0.3243086 | min loss: 0.1461899\n",
      "Epoach: 656 | avg loss: 0.20839521 | max loss: 0.25752556 | min loss: 0.09951013\n",
      "Epoach: 657 | avg loss: 0.17240265 | max loss: 0.21153338 | min loss: 0.11845814\n",
      "Epoach: 658 | avg loss: 0.19498979 | max loss: 0.21960795 | min loss: 0.16265774\n",
      "Epoach: 659 | avg loss: 0.20779747 | max loss: 0.26056287 | min loss: 0.15837625\n",
      "Epoach: 660 | avg loss: 0.19279289 | max loss: 0.23247269 | min loss: 0.13183406\n",
      "Epoach: 661 | avg loss: 0.18544605 | max loss: 0.28229707 | min loss: 0.103746764\n",
      "Epoach: 662 | avg loss: 0.20292142 | max loss: 0.27791476 | min loss: 0.15424928\n",
      "Epoach: 663 | avg loss: 0.21465507 | max loss: 0.2631139 | min loss: 0.14206031\n",
      "Epoach: 664 | avg loss: 0.20072658 | max loss: 0.25747946 | min loss: 0.16040914\n",
      "Epoach: 665 | avg loss: 0.19537462 | max loss: 0.25720975 | min loss: 0.1488953\n",
      "Epoach: 666 | avg loss: 0.20278417 | max loss: 0.27214515 | min loss: 0.1316314\n",
      "Epoach: 667 | avg loss: 0.20139572 | max loss: 0.24829486 | min loss: 0.1364623\n",
      "Epoach: 668 | avg loss: 0.17890953 | max loss: 0.20675217 | min loss: 0.12397257\n",
      "Epoach: 669 | avg loss: 0.21685645 | max loss: 0.31481203 | min loss: 0.14351285\n",
      "Epoach: 670 | avg loss: 0.2075115 | max loss: 0.27599669 | min loss: 0.14619291\n",
      "Epoach: 671 | avg loss: 0.18524605 | max loss: 0.26652917 | min loss: 0.13006294\n",
      "Epoach: 672 | avg loss: 0.19707759 | max loss: 0.23326074 | min loss: 0.14327368\n",
      "Epoach: 673 | avg loss: 0.2158035 | max loss: 0.2512705 | min loss: 0.17865175\n",
      "Epoach: 674 | avg loss: 0.18960509 | max loss: 0.2615859 | min loss: 0.13899118\n",
      "Epoach: 675 | avg loss: 0.18515298 | max loss: 0.28147072 | min loss: 0.13357946\n",
      "Epoach: 676 | avg loss: 0.19219059 | max loss: 0.23654449 | min loss: 0.12498915\n",
      "Epoach: 677 | avg loss: 0.17783266 | max loss: 0.23187545 | min loss: 0.14655825\n",
      "Epoach: 678 | avg loss: 0.20642784 | max loss: 0.29055214 | min loss: 0.13701865\n",
      "Epoach: 679 | avg loss: 0.20239094 | max loss: 0.27615488 | min loss: 0.13720527\n",
      "Epoach: 680 | avg loss: 0.20520884 | max loss: 0.2718367 | min loss: 0.16258416\n",
      "Epoach: 681 | avg loss: 0.18503058 | max loss: 0.22623874 | min loss: 0.13956362\n",
      "Epoach: 682 | avg loss: 0.1883229 | max loss: 0.2829462 | min loss: 0.10999471\n",
      "Epoach: 683 | avg loss: 0.21384974 | max loss: 0.27723223 | min loss: 0.10498466\n",
      "Epoach: 684 | avg loss: 0.1849192 | max loss: 0.22975776 | min loss: 0.14284924\n",
      "Epoach: 685 | avg loss: 0.20616356 | max loss: 0.3266027 | min loss: 0.10789027\n",
      "Epoach: 686 | avg loss: 0.17373642 | max loss: 0.19016692 | min loss: 0.13963236\n",
      "Epoach: 687 | avg loss: 0.1856789 | max loss: 0.25607875 | min loss: 0.13483852\n",
      "Epoach: 688 | avg loss: 0.18771145 | max loss: 0.23018175 | min loss: 0.14836699\n",
      "Epoach: 689 | avg loss: 0.19065556 | max loss: 0.27467972 | min loss: 0.13347098\n",
      "Epoach: 690 | avg loss: 0.19161813 | max loss: 0.25120297 | min loss: 0.10747197\n",
      "Epoach: 691 | avg loss: 0.18677954 | max loss: 0.2639544 | min loss: 0.15429145\n",
      "Epoach: 692 | avg loss: 0.21787533 | max loss: 0.30252206 | min loss: 0.16772643\n",
      "Epoach: 693 | avg loss: 0.20652601 | max loss: 0.25887457 | min loss: 0.17624527\n",
      "Epoach: 694 | avg loss: 0.19360101 | max loss: 0.21839002 | min loss: 0.17390335\n",
      "Epoach: 695 | avg loss: 0.19210878 | max loss: 0.23405358 | min loss: 0.14059696\n",
      "Epoach: 696 | avg loss: 0.1843862 | max loss: 0.23678425 | min loss: 0.14443627\n",
      "Epoach: 697 | avg loss: 0.19790186 | max loss: 0.2609697 | min loss: 0.1558002\n",
      "Epoach: 698 | avg loss: 0.21458155 | max loss: 0.2511775 | min loss: 0.17888516\n",
      "Epoach: 699 | avg loss: 0.18793523 | max loss: 0.24957035 | min loss: 0.13724191\n",
      "Epoach: 700 | avg loss: 0.186317 | max loss: 0.26109314 | min loss: 0.118479505\n",
      "Epoach: 701 | avg loss: 0.18116938 | max loss: 0.23990016 | min loss: 0.11433668\n",
      "Epoach: 702 | avg loss: 0.17682672 | max loss: 0.21229449 | min loss: 0.13965762\n",
      "Epoach: 703 | avg loss: 0.19900018 | max loss: 0.28145236 | min loss: 0.14839835\n",
      "Epoach: 704 | avg loss: 0.19156463 | max loss: 0.21595037 | min loss: 0.137229\n",
      "Epoach: 705 | avg loss: 0.20399791 | max loss: 0.3383078 | min loss: 0.15564819\n",
      "Epoach: 706 | avg loss: 0.20547812 | max loss: 0.27541876 | min loss: 0.13182098\n",
      "Epoach: 707 | avg loss: 0.17705092 | max loss: 0.2596737 | min loss: 0.12390137\n",
      "Epoach: 708 | avg loss: 0.17537904 | max loss: 0.20668238 | min loss: 0.13169444\n",
      "Epoach: 709 | avg loss: 0.21119663 | max loss: 0.28077596 | min loss: 0.17394459\n",
      "Epoach: 710 | avg loss: 0.2189357 | max loss: 0.30007276 | min loss: 0.15122736\n",
      "Epoach: 711 | avg loss: 0.22303984 | max loss: 0.28300494 | min loss: 0.18262503\n",
      "Epoach: 712 | avg loss: 0.1939108 | max loss: 0.2592755 | min loss: 0.12957653\n",
      "Epoach: 713 | avg loss: 0.20665802 | max loss: 0.28947893 | min loss: 0.16163413\n",
      "Epoach: 714 | avg loss: 0.18738371 | max loss: 0.25805295 | min loss: 0.14475174\n",
      "Epoach: 715 | avg loss: 0.1881133 | max loss: 0.28426853 | min loss: 0.121384546\n",
      "Epoach: 716 | avg loss: 0.18007687 | max loss: 0.27484727 | min loss: 0.14271376\n",
      "Epoach: 717 | avg loss: 0.18914053 | max loss: 0.24467908 | min loss: 0.14138882\n",
      "Epoach: 718 | avg loss: 0.20975682 | max loss: 0.30191374 | min loss: 0.1582453\n",
      "Epoach: 719 | avg loss: 0.18537149 | max loss: 0.23490167 | min loss: 0.13290055\n",
      "Epoach: 720 | avg loss: 0.2062429 | max loss: 0.2906409 | min loss: 0.13049579\n",
      "Epoach: 721 | avg loss: 0.19316423 | max loss: 0.24520233 | min loss: 0.17041749\n",
      "Epoach: 722 | avg loss: 0.18972744 | max loss: 0.24104491 | min loss: 0.13704976\n",
      "Epoach: 723 | avg loss: 0.21166413 | max loss: 0.25914788 | min loss: 0.17601566\n",
      "Epoach: 724 | avg loss: 0.18021038 | max loss: 0.21416593 | min loss: 0.14258657\n",
      "Epoach: 725 | avg loss: 0.20555949 | max loss: 0.26043063 | min loss: 0.14925775\n",
      "Epoach: 726 | avg loss: 0.18146698 | max loss: 0.269193 | min loss: 0.12763369\n",
      "Epoach: 727 | avg loss: 0.16485012 | max loss: 0.20414746 | min loss: 0.14277762\n",
      "Epoach: 728 | avg loss: 0.19001012 | max loss: 0.22967783 | min loss: 0.16963893\n",
      "Epoach: 729 | avg loss: 0.18048416 | max loss: 0.23094359 | min loss: 0.11167788\n",
      "Epoach: 730 | avg loss: 0.18053702 | max loss: 0.21907626 | min loss: 0.132923\n",
      "Epoach: 731 | avg loss: 0.19097823 | max loss: 0.22497335 | min loss: 0.14688289\n",
      "Epoach: 732 | avg loss: 0.1771354 | max loss: 0.22583096 | min loss: 0.13811252\n",
      "Epoach: 733 | avg loss: 0.16467449 | max loss: 0.22394167 | min loss: 0.08796264\n",
      "Epoach: 734 | avg loss: 0.20320292 | max loss: 0.27090943 | min loss: 0.15350032\n",
      "Epoach: 735 | avg loss: 0.18815121 | max loss: 0.22736385 | min loss: 0.1375816\n",
      "Epoach: 736 | avg loss: 0.17334935 | max loss: 0.21109971 | min loss: 0.12330267\n",
      "Epoach: 737 | avg loss: 0.19160438 | max loss: 0.24119292 | min loss: 0.14031754\n",
      "Epoach: 738 | avg loss: 0.18443704 | max loss: 0.2396695 | min loss: 0.12181274\n",
      "Epoach: 739 | avg loss: 0.19425675 | max loss: 0.26831597 | min loss: 0.13735393\n",
      "Epoach: 740 | avg loss: 0.1781321 | max loss: 0.22736098 | min loss: 0.14613414\n",
      "Epoach: 741 | avg loss: 0.1945495 | max loss: 0.24824205 | min loss: 0.122534275\n",
      "Epoach: 742 | avg loss: 0.16767311 | max loss: 0.21548335 | min loss: 0.12204555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoach: 743 | avg loss: 0.17539416 | max loss: 0.2122785 | min loss: 0.12786412\n",
      "Epoach: 744 | avg loss: 0.16023585 | max loss: 0.19707641 | min loss: 0.115837656\n",
      "Epoach: 745 | avg loss: 0.1799075 | max loss: 0.2668154 | min loss: 0.11339701\n",
      "Epoach: 746 | avg loss: 0.18956819 | max loss: 0.20785676 | min loss: 0.15741517\n",
      "Epoach: 747 | avg loss: 0.1672079 | max loss: 0.19771615 | min loss: 0.114578985\n",
      "Epoach: 748 | avg loss: 0.19373025 | max loss: 0.24427074 | min loss: 0.14855902\n",
      "Epoach: 749 | avg loss: 0.20005928 | max loss: 0.29413846 | min loss: 0.15801251\n",
      "Epoach: 750 | avg loss: 0.21554911 | max loss: 0.2824101 | min loss: 0.124118894\n",
      "Epoach: 751 | avg loss: 0.18884698 | max loss: 0.2525644 | min loss: 0.122447826\n",
      "Epoach: 752 | avg loss: 0.17703968 | max loss: 0.21971366 | min loss: 0.11433394\n",
      "Epoach: 753 | avg loss: 0.18385942 | max loss: 0.23219936 | min loss: 0.11878079\n",
      "Epoach: 754 | avg loss: 0.18037857 | max loss: 0.23499517 | min loss: 0.12782644\n",
      "Epoach: 755 | avg loss: 0.18477452 | max loss: 0.25507593 | min loss: 0.13451561\n",
      "Epoach: 756 | avg loss: 0.17297563 | max loss: 0.20421994 | min loss: 0.13478799\n",
      "Epoach: 757 | avg loss: 0.2006926 | max loss: 0.26915228 | min loss: 0.14838731\n",
      "Epoach: 758 | avg loss: 0.16673791 | max loss: 0.22291139 | min loss: 0.12322791\n",
      "Epoach: 759 | avg loss: 0.19312614 | max loss: 0.260473 | min loss: 0.12865354\n",
      "Epoach: 760 | avg loss: 0.19459274 | max loss: 0.24597524 | min loss: 0.13174799\n",
      "Epoach: 761 | avg loss: 0.19109671 | max loss: 0.23887232 | min loss: 0.15726623\n",
      "Epoach: 762 | avg loss: 0.18882188 | max loss: 0.25176552 | min loss: 0.15621826\n",
      "Epoach: 763 | avg loss: 0.20227706 | max loss: 0.2592687 | min loss: 0.10974622\n",
      "Epoach: 764 | avg loss: 0.20147945 | max loss: 0.23626505 | min loss: 0.15861432\n",
      "Epoach: 765 | avg loss: 0.16454636 | max loss: 0.20071611 | min loss: 0.110661335\n",
      "Epoach: 766 | avg loss: 0.18894109 | max loss: 0.22931409 | min loss: 0.13249257\n",
      "Epoach: 767 | avg loss: 0.18303892 | max loss: 0.2788827 | min loss: 0.12170357\n",
      "Epoach: 768 | avg loss: 0.18298383 | max loss: 0.2406253 | min loss: 0.13435432\n",
      "Epoach: 769 | avg loss: 0.18068138 | max loss: 0.23642048 | min loss: 0.10441802\n",
      "Epoach: 770 | avg loss: 0.17419414 | max loss: 0.2328837 | min loss: 0.11774574\n",
      "Epoach: 771 | avg loss: 0.1916261 | max loss: 0.23665652 | min loss: 0.14945535\n",
      "Epoach: 772 | avg loss: 0.17694768 | max loss: 0.23627463 | min loss: 0.1321048\n",
      "Epoach: 773 | avg loss: 0.17681667 | max loss: 0.21730933 | min loss: 0.1183059\n",
      "Epoach: 774 | avg loss: 0.16447832 | max loss: 0.2220768 | min loss: 0.086161904\n",
      "Epoach: 775 | avg loss: 0.17363207 | max loss: 0.2586713 | min loss: 0.13157749\n",
      "Epoach: 776 | avg loss: 0.1851339 | max loss: 0.21958217 | min loss: 0.15333733\n",
      "Epoach: 777 | avg loss: 0.1866797 | max loss: 0.2632333 | min loss: 0.13522731\n",
      "Epoach: 778 | avg loss: 0.18057954 | max loss: 0.24775022 | min loss: 0.14306228\n",
      "Epoach: 779 | avg loss: 0.17297821 | max loss: 0.23013845 | min loss: 0.13061137\n",
      "Epoach: 780 | avg loss: 0.18082434 | max loss: 0.25758356 | min loss: 0.11392955\n",
      "Epoach: 781 | avg loss: 0.17132285 | max loss: 0.23281434 | min loss: 0.1201\n",
      "Epoach: 782 | avg loss: 0.1798687 | max loss: 0.27757937 | min loss: 0.102683514\n",
      "Epoach: 783 | avg loss: 0.15803507 | max loss: 0.20250644 | min loss: 0.11133482\n",
      "Epoach: 784 | avg loss: 0.1925 | max loss: 0.25884768 | min loss: 0.13109116\n",
      "Epoach: 785 | avg loss: 0.16923127 | max loss: 0.24738637 | min loss: 0.11386448\n",
      "Epoach: 786 | avg loss: 0.18983236 | max loss: 0.25056675 | min loss: 0.1488688\n",
      "Epoach: 787 | avg loss: 0.15285297 | max loss: 0.19502795 | min loss: 0.11678034\n",
      "Epoach: 788 | avg loss: 0.1526054 | max loss: 0.2090387 | min loss: 0.10230898\n",
      "Epoach: 789 | avg loss: 0.18533307 | max loss: 0.23223546 | min loss: 0.123783275\n",
      "Epoach: 790 | avg loss: 0.17002913 | max loss: 0.19464782 | min loss: 0.13384852\n",
      "Epoach: 791 | avg loss: 0.19101337 | max loss: 0.22593899 | min loss: 0.15400764\n",
      "Epoach: 792 | avg loss: 0.16482991 | max loss: 0.20428756 | min loss: 0.10596195\n",
      "Epoach: 793 | avg loss: 0.19403476 | max loss: 0.2795555 | min loss: 0.1661669\n",
      "Epoach: 794 | avg loss: 0.17625266 | max loss: 0.26441222 | min loss: 0.09651783\n",
      "Epoach: 795 | avg loss: 0.19031209 | max loss: 0.24282518 | min loss: 0.14158092\n",
      "Epoach: 796 | avg loss: 0.19901273 | max loss: 0.2708401 | min loss: 0.12379425\n",
      "Epoach: 797 | avg loss: 0.17380576 | max loss: 0.23632601 | min loss: 0.11655383\n",
      "Epoach: 798 | avg loss: 0.17367917 | max loss: 0.23921338 | min loss: 0.113556966\n",
      "Epoach: 799 | avg loss: 0.18299201 | max loss: 0.21658266 | min loss: 0.13838892\n",
      "Epoach: 800 | avg loss: 0.16685995 | max loss: 0.20178525 | min loss: 0.120600075\n",
      "Epoach: 801 | avg loss: 0.18087372 | max loss: 0.23164724 | min loss: 0.12228649\n",
      "Epoach: 802 | avg loss: 0.17448504 | max loss: 0.23174599 | min loss: 0.11420411\n",
      "Epoach: 803 | avg loss: 0.17737523 | max loss: 0.23480926 | min loss: 0.13100177\n",
      "Epoach: 804 | avg loss: 0.16948234 | max loss: 0.20392251 | min loss: 0.12283267\n",
      "Epoach: 805 | avg loss: 0.15082145 | max loss: 0.19921066 | min loss: 0.10224147\n",
      "Epoach: 806 | avg loss: 0.15916148 | max loss: 0.23466566 | min loss: 0.10184466\n",
      "Epoach: 807 | avg loss: 0.1708236 | max loss: 0.2270175 | min loss: 0.1151457\n",
      "Epoach: 808 | avg loss: 0.20981276 | max loss: 0.27784407 | min loss: 0.14909105\n",
      "Epoach: 809 | avg loss: 0.17875606 | max loss: 0.2178278 | min loss: 0.11983759\n",
      "Epoach: 810 | avg loss: 0.17915303 | max loss: 0.2321123 | min loss: 0.13270602\n",
      "Epoach: 811 | avg loss: 0.14379899 | max loss: 0.17473334 | min loss: 0.1085528\n",
      "Epoach: 812 | avg loss: 0.17062205 | max loss: 0.24078713 | min loss: 0.10286902\n",
      "Epoach: 813 | avg loss: 0.16996986 | max loss: 0.22489443 | min loss: 0.113297656\n",
      "Epoach: 814 | avg loss: 0.18510038 | max loss: 0.22336724 | min loss: 0.14495796\n",
      "Epoach: 815 | avg loss: 0.17007348 | max loss: 0.22091798 | min loss: 0.13296784\n",
      "Epoach: 816 | avg loss: 0.16208953 | max loss: 0.22303349 | min loss: 0.11335257\n",
      "Epoach: 817 | avg loss: 0.15259457 | max loss: 0.1853467 | min loss: 0.122039646\n",
      "Epoach: 818 | avg loss: 0.17634173 | max loss: 0.26097178 | min loss: 0.104056224\n",
      "Epoach: 819 | avg loss: 0.14553957 | max loss: 0.18128021 | min loss: 0.09868935\n",
      "Epoach: 820 | avg loss: 0.20121081 | max loss: 0.24768493 | min loss: 0.16904011\n",
      "Epoach: 821 | avg loss: 0.1755174 | max loss: 0.1968796 | min loss: 0.15115768\n",
      "Epoach: 822 | avg loss: 0.16376053 | max loss: 0.22977436 | min loss: 0.124361545\n",
      "Epoach: 823 | avg loss: 0.13901135 | max loss: 0.1752604 | min loss: 0.10602228\n",
      "Epoach: 824 | avg loss: 0.16605452 | max loss: 0.24949038 | min loss: 0.12749648\n",
      "Epoach: 825 | avg loss: 0.17845713 | max loss: 0.23261367 | min loss: 0.14428328\n",
      "Epoach: 826 | avg loss: 0.15637228 | max loss: 0.2126542 | min loss: 0.10395329\n",
      "Epoach: 827 | avg loss: 0.16112757 | max loss: 0.19435623 | min loss: 0.09937073\n",
      "Epoach: 828 | avg loss: 0.18284428 | max loss: 0.2257742 | min loss: 0.13611956\n",
      "Epoach: 829 | avg loss: 0.15804088 | max loss: 0.21487696 | min loss: 0.12561314\n",
      "Epoach: 830 | avg loss: 0.16416839 | max loss: 0.21205795 | min loss: 0.13671681\n",
      "Epoach: 831 | avg loss: 0.17322594 | max loss: 0.22534412 | min loss: 0.13346949\n",
      "Epoach: 832 | avg loss: 0.17163815 | max loss: 0.20233865 | min loss: 0.110603556\n",
      "Epoach: 833 | avg loss: 0.19995898 | max loss: 0.28835475 | min loss: 0.122456335\n",
      "Epoach: 834 | avg loss: 0.1901617 | max loss: 0.29635853 | min loss: 0.11911685\n",
      "Epoach: 835 | avg loss: 0.18655562 | max loss: 0.23534453 | min loss: 0.10147382\n",
      "Epoach: 836 | avg loss: 0.14294967 | max loss: 0.22053775 | min loss: 0.103414774\n",
      "Epoach: 837 | avg loss: 0.17013547 | max loss: 0.2383192 | min loss: 0.14553186\n",
      "Epoach: 838 | avg loss: 0.17927313 | max loss: 0.20798224 | min loss: 0.13556942\n",
      "Epoach: 839 | avg loss: 0.15200476 | max loss: 0.25646013 | min loss: 0.10550298\n",
      "Epoach: 840 | avg loss: 0.15398589 | max loss: 0.18916795 | min loss: 0.12519568\n",
      "Epoach: 841 | avg loss: 0.14939207 | max loss: 0.1917924 | min loss: 0.11296297\n",
      "Epoach: 842 | avg loss: 0.17818967 | max loss: 0.21958804 | min loss: 0.123020254\n",
      "Epoach: 843 | avg loss: 0.16492826 | max loss: 0.24074554 | min loss: 0.105928555\n",
      "Epoach: 844 | avg loss: 0.1602081 | max loss: 0.22418582 | min loss: 0.10072531\n",
      "Epoach: 845 | avg loss: 0.16378088 | max loss: 0.22468115 | min loss: 0.10250886\n",
      "Epoach: 846 | avg loss: 0.15260108 | max loss: 0.18543112 | min loss: 0.111952305\n",
      "Epoach: 847 | avg loss: 0.1588335 | max loss: 0.21442865 | min loss: 0.118514076\n",
      "Epoach: 848 | avg loss: 0.1640853 | max loss: 0.2903183 | min loss: 0.10559887\n",
      "Epoach: 849 | avg loss: 0.15921435 | max loss: 0.21195143 | min loss: 0.12095147\n",
      "Epoach: 850 | avg loss: 0.16916113 | max loss: 0.22459969 | min loss: 0.12061748\n",
      "Epoach: 851 | avg loss: 0.15198211 | max loss: 0.23378697 | min loss: 0.10110749\n",
      "Epoach: 852 | avg loss: 0.14059404 | max loss: 0.19488978 | min loss: 0.11181615\n",
      "Epoach: 853 | avg loss: 0.18953465 | max loss: 0.2699678 | min loss: 0.1471216\n",
      "Epoach: 854 | avg loss: 0.17241494 | max loss: 0.22580168 | min loss: 0.12279171\n",
      "Epoach: 855 | avg loss: 0.16906148 | max loss: 0.20469134 | min loss: 0.13704997\n",
      "Epoach: 856 | avg loss: 0.16308385 | max loss: 0.21092501 | min loss: 0.122148365\n",
      "Epoach: 857 | avg loss: 0.18246317 | max loss: 0.2830686 | min loss: 0.11467628\n",
      "Epoach: 858 | avg loss: 0.1464209 | max loss: 0.20243187 | min loss: 0.07084635\n",
      "Epoach: 859 | avg loss: 0.1572907 | max loss: 0.23098224 | min loss: 0.10565172\n",
      "Epoach: 860 | avg loss: 0.1575001 | max loss: 0.17757009 | min loss: 0.13824382\n",
      "Epoach: 861 | avg loss: 0.17718162 | max loss: 0.24396136 | min loss: 0.1315912\n",
      "Epoach: 862 | avg loss: 0.17369944 | max loss: 0.23014063 | min loss: 0.13217604\n",
      "Epoach: 863 | avg loss: 0.16791004 | max loss: 0.22444654 | min loss: 0.10190785\n",
      "Epoach: 864 | avg loss: 0.17999041 | max loss: 0.20553789 | min loss: 0.16302066\n",
      "Epoach: 865 | avg loss: 0.16807842 | max loss: 0.22750843 | min loss: 0.131376\n",
      "Epoach: 866 | avg loss: 0.16599125 | max loss: 0.20925392 | min loss: 0.13205418\n",
      "Epoach: 867 | avg loss: 0.13028705 | max loss: 0.16860333 | min loss: 0.08889012\n",
      "Epoach: 868 | avg loss: 0.16147949 | max loss: 0.20757021 | min loss: 0.12213665\n",
      "Epoach: 869 | avg loss: 0.15931664 | max loss: 0.19001976 | min loss: 0.10973185\n",
      "Epoach: 870 | avg loss: 0.16296697 | max loss: 0.2587161 | min loss: 0.12878376\n",
      "Epoach: 871 | avg loss: 0.17632216 | max loss: 0.22779594 | min loss: 0.12555003\n",
      "Epoach: 872 | avg loss: 0.16339679 | max loss: 0.23688479 | min loss: 0.119158044\n",
      "Epoach: 873 | avg loss: 0.14662111 | max loss: 0.19980223 | min loss: 0.10259971\n",
      "Epoach: 874 | avg loss: 0.13886832 | max loss: 0.23459724 | min loss: 0.07814905\n",
      "Epoach: 875 | avg loss: 0.15997314 | max loss: 0.20612043 | min loss: 0.11111657\n",
      "Epoach: 876 | avg loss: 0.16658297 | max loss: 0.22234 | min loss: 0.13798475\n",
      "Epoach: 877 | avg loss: 0.17675157 | max loss: 0.2236366 | min loss: 0.1379702\n",
      "Epoach: 878 | avg loss: 0.18006185 | max loss: 0.2289173 | min loss: 0.13323201\n",
      "Epoach: 879 | avg loss: 0.16840102 | max loss: 0.21102083 | min loss: 0.11020336\n",
      "Epoach: 880 | avg loss: 0.14625083 | max loss: 0.18548544 | min loss: 0.08929507\n",
      "Epoach: 881 | avg loss: 0.15717995 | max loss: 0.17722425 | min loss: 0.118739605\n",
      "Epoach: 882 | avg loss: 0.14540556 | max loss: 0.19640552 | min loss: 0.09311992\n",
      "Epoach: 883 | avg loss: 0.16988447 | max loss: 0.22310509 | min loss: 0.11110653\n",
      "Epoach: 884 | avg loss: 0.17911196 | max loss: 0.22694132 | min loss: 0.11955618\n",
      "Epoach: 885 | avg loss: 0.16920578 | max loss: 0.22794524 | min loss: 0.11290013\n",
      "Epoach: 886 | avg loss: 0.15678751 | max loss: 0.21748549 | min loss: 0.12413679\n",
      "Epoach: 887 | avg loss: 0.1529563 | max loss: 0.18883587 | min loss: 0.13093917\n",
      "Epoach: 888 | avg loss: 0.15292412 | max loss: 0.19956751 | min loss: 0.10042618\n",
      "Epoach: 889 | avg loss: 0.14044333 | max loss: 0.16137967 | min loss: 0.10719984\n",
      "Epoach: 890 | avg loss: 0.14543363 | max loss: 0.18392959 | min loss: 0.09052995\n",
      "Epoach: 891 | avg loss: 0.17264411 | max loss: 0.23460379 | min loss: 0.13946669\n",
      "Epoach: 892 | avg loss: 0.19115075 | max loss: 0.2543335 | min loss: 0.14461789\n",
      "Epoach: 893 | avg loss: 0.1516794 | max loss: 0.1812698 | min loss: 0.105026826\n",
      "Epoach: 894 | avg loss: 0.17209893 | max loss: 0.22084133 | min loss: 0.10243459\n",
      "Epoach: 895 | avg loss: 0.15412465 | max loss: 0.23165105 | min loss: 0.088294074\n",
      "Epoach: 896 | avg loss: 0.1448519 | max loss: 0.19285989 | min loss: 0.117695175\n",
      "Epoach: 897 | avg loss: 0.16548261 | max loss: 0.21738955 | min loss: 0.099662445\n",
      "Epoach: 898 | avg loss: 0.13995847 | max loss: 0.19800588 | min loss: 0.10749661\n",
      "Epoach: 899 | avg loss: 0.15510371 | max loss: 0.1880001 | min loss: 0.12038886\n",
      "Epoach: 900 | avg loss: 0.15298963 | max loss: 0.19699416 | min loss: 0.09243281\n",
      "Epoach: 901 | avg loss: 0.18840939 | max loss: 0.24454507 | min loss: 0.12391787\n",
      "Epoach: 902 | avg loss: 0.16661513 | max loss: 0.19271171 | min loss: 0.13453332\n",
      "Epoach: 903 | avg loss: 0.1822063 | max loss: 0.24312055 | min loss: 0.14208916\n",
      "Epoach: 904 | avg loss: 0.155679 | max loss: 0.20296149 | min loss: 0.10370675\n",
      "Epoach: 905 | avg loss: 0.14922069 | max loss: 0.20533763 | min loss: 0.11653139\n",
      "Epoach: 906 | avg loss: 0.14743297 | max loss: 0.19423585 | min loss: 0.114357024\n",
      "Epoach: 907 | avg loss: 0.14846712 | max loss: 0.17697075 | min loss: 0.098764606\n",
      "Epoach: 908 | avg loss: 0.1768556 | max loss: 0.22378957 | min loss: 0.1542303\n",
      "Epoach: 909 | avg loss: 0.18571517 | max loss: 0.24376541 | min loss: 0.13891691\n",
      "Epoach: 910 | avg loss: 0.15268506 | max loss: 0.19631469 | min loss: 0.124381416\n",
      "Epoach: 911 | avg loss: 0.14351794 | max loss: 0.17214838 | min loss: 0.09918756\n",
      "Epoach: 912 | avg loss: 0.15471129 | max loss: 0.20106527 | min loss: 0.11700305\n",
      "Epoach: 913 | avg loss: 0.14877537 | max loss: 0.19124103 | min loss: 0.11339574\n",
      "Epoach: 914 | avg loss: 0.1752344 | max loss: 0.2263929 | min loss: 0.13345715\n",
      "Epoach: 915 | avg loss: 0.16306773 | max loss: 0.21460463 | min loss: 0.12971464\n",
      "Epoach: 916 | avg loss: 0.15307496 | max loss: 0.20676604 | min loss: 0.111269906\n",
      "Epoach: 917 | avg loss: 0.14995062 | max loss: 0.18307984 | min loss: 0.11517926\n",
      "Epoach: 918 | avg loss: 0.1553727 | max loss: 0.22035955 | min loss: 0.11396482\n",
      "Epoach: 919 | avg loss: 0.14296955 | max loss: 0.19971354 | min loss: 0.105932415\n",
      "Epoach: 920 | avg loss: 0.1382753 | max loss: 0.23924169 | min loss: 0.08018247\n",
      "Epoach: 921 | avg loss: 0.14229208 | max loss: 0.17888355 | min loss: 0.11276516\n",
      "Epoach: 922 | avg loss: 0.13417459 | max loss: 0.2087323 | min loss: 0.075220264\n",
      "Epoach: 923 | avg loss: 0.16108724 | max loss: 0.19439039 | min loss: 0.13555798\n",
      "Epoach: 924 | avg loss: 0.14403132 | max loss: 0.18536562 | min loss: 0.111378886\n",
      "Epoach: 925 | avg loss: 0.14453666 | max loss: 0.17834407 | min loss: 0.100809745\n",
      "Epoach: 926 | avg loss: 0.1560862 | max loss: 0.19748154 | min loss: 0.11996529\n",
      "Epoach: 927 | avg loss: 0.15524343 | max loss: 0.19615076 | min loss: 0.09288686\n",
      "Epoach: 928 | avg loss: 0.15449771 | max loss: 0.2007241 | min loss: 0.11546831\n",
      "Epoach: 929 | avg loss: 0.15953065 | max loss: 0.21251313 | min loss: 0.10841955\n",
      "Epoach: 930 | avg loss: 0.1513013 | max loss: 0.21044324 | min loss: 0.0962784\n",
      "Epoach: 931 | avg loss: 0.15412094 | max loss: 0.18792334 | min loss: 0.11994859\n",
      "Epoach: 932 | avg loss: 0.1569886 | max loss: 0.22109234 | min loss: 0.086137995\n",
      "Epoach: 933 | avg loss: 0.17955792 | max loss: 0.22804523 | min loss: 0.1403209\n",
      "Epoach: 934 | avg loss: 0.17067215 | max loss: 0.22718999 | min loss: 0.108562216\n",
      "Epoach: 935 | avg loss: 0.15158296 | max loss: 0.22264129 | min loss: 0.11037003\n",
      "Epoach: 936 | avg loss: 0.12695283 | max loss: 0.16116937 | min loss: 0.09262785\n",
      "Epoach: 937 | avg loss: 0.13872612 | max loss: 0.19366021 | min loss: 0.09727912\n",
      "Epoach: 938 | avg loss: 0.14858146 | max loss: 0.21231197 | min loss: 0.06588192\n",
      "Epoach: 939 | avg loss: 0.1294907 | max loss: 0.14486608 | min loss: 0.09049949\n",
      "Epoach: 940 | avg loss: 0.16902243 | max loss: 0.24530494 | min loss: 0.08940715\n",
      "Epoach: 941 | avg loss: 0.15586185 | max loss: 0.20203608 | min loss: 0.11440572\n",
      "Epoach: 942 | avg loss: 0.14599462 | max loss: 0.18142341 | min loss: 0.11926867\n",
      "Epoach: 943 | avg loss: 0.15071309 | max loss: 0.1888146 | min loss: 0.079805814\n",
      "Epoach: 944 | avg loss: 0.1666342 | max loss: 0.20294411 | min loss: 0.14298886\n",
      "Epoach: 945 | avg loss: 0.13237101 | max loss: 0.17048231 | min loss: 0.093996234\n",
      "Epoach: 946 | avg loss: 0.124035254 | max loss: 0.20550027 | min loss: 0.084083304\n",
      "Epoach: 947 | avg loss: 0.15051186 | max loss: 0.21120515 | min loss: 0.124365285\n",
      "Epoach: 948 | avg loss: 0.14603436 | max loss: 0.19642514 | min loss: 0.09448485\n",
      "Epoach: 949 | avg loss: 0.14998493 | max loss: 0.21015362 | min loss: 0.09079948\n",
      "Epoach: 950 | avg loss: 0.15753613 | max loss: 0.18402827 | min loss: 0.12736244\n",
      "Epoach: 951 | avg loss: 0.14958486 | max loss: 0.18901365 | min loss: 0.1232129\n",
      "Epoach: 952 | avg loss: 0.11583398 | max loss: 0.17409578 | min loss: 0.07184293\n",
      "Epoach: 953 | avg loss: 0.12778589 | max loss: 0.16868037 | min loss: 0.090907834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoach: 950 | avg loss: 0.16267064 | max loss: 0.2000409 | min loss: 0.136145163614516 | time: 11.04591703414917152\n",
      "Epoach: 951 | avg loss: 0.15244317 | max loss: 0.20265019 | min loss: 0.11175933511759335 | time: 11.0131354331970211\n",
      "Epoach: 952 | avg loss: 0.13572313 | max loss: 0.1899216 | min loss: 0.1068796110687961 | time: 11.0069119930267339\n",
      "Epoach: 953 | avg loss: 0.16409722 | max loss: 0.23220104 | min loss: 0.1020675710206757 | time: 11.037168979644775\n",
      "Epoach: 954 | avg loss: 0.1309264 | max loss: 0.16313866 | min loss: 0.11155788611557886 | time: 11.00368738174438514\n",
      "Epoach: 955 | avg loss: 0.15871972 | max loss: 0.18337372 | min loss: 0.136174383617438 | time: 11.0145530700683637\n",
      "Epoach: 956 | avg loss: 0.15214795 | max loss: 0.28397435 | min loss: 0.0710054607100546 | time: 11.029855012893677\n",
      "Epoach: 957 | avg loss: 0.15109581 | max loss: 0.17666426 | min loss: 0.1295439.1295439 | time: 11.0759370326995851\n",
      "Epoach: 958 | avg loss: 0.14123742 | max loss: 0.1852453 | min loss: 0.086214458621445 | time: 11.02251505851745655\n",
      "Epoach: 959 | avg loss: 0.16877288 | max loss: 0.2035014 | min loss: 0.1307829613078296 | time: 11.0040676593780527\n",
      "Step: 7 | loss: 0.1456203 | current max loss: 0.18769762 | current min loss: 0.11139253 | time: 11.1001079082489014\n",
      "Predict query: 81\n",
      "TEST set, len  82\n",
      "MAP @10 bioASQ: 0.032777777777777774\n",
      "MAP @25: 0.04554505268781022\n",
      "MAP @50: 0.052654387172337856\n",
      "MAP @100: 0.05784191208618226\n",
      "MAP @200: 0.0637974501325862\n",
      "MAP @300: 0.06706083391281617\n",
      "MAP: 0.07119822713915824\n",
      "RECALL@10: 0.1359833904358347\n",
      "RECALL@50: 0.3479336281474963\n",
      "RECALL@100: 0.44801031828789717\n",
      "TEST set, len  85\n",
      "MAP @10 bioASQ: 0.15598832866479923\n",
      "MAP @25: 0.2161786642763018\n",
      "MAP @50: 0.2615432315463861\n",
      "MAP @100: 0.28652772097394447\n",
      "MAP @200: 0.2982910731262128\n",
      "MAP @300: 0.3011615146232835\n",
      "MAP: 0.3092879145014553\n",
      "RECALL@10: 0.5014942495916697\n",
      "RECALL@50: 0.8618734053226265\n",
      "RECALL@100: 0.9455219180869454\n",
      "Epoach: 960 | avg loss: 0.14097308 | max loss: 0.18769762 | min loss: 0.11139253\n",
      "Epoach: 961 | avg loss: 0.14149523 | max loss: 0.2218602 | min loss: 0.0829178908291789 | time: 10.9872109889984139\n",
      "Epoach: 962 | avg loss: 0.15669993 | max loss: 0.22483292 | min loss: 0.09087990908799 | time: 11.00119996070861881\n",
      "Epoach: 963 | avg loss: 0.121161915 | max loss: 0.1731316 | min loss: 0.08029615029615 | time: 11.00080847740173384\n",
      "Epoach: 964 | avg loss: 0.1471691 | max loss: 0.21857266 | min loss: 0.093078636093078636 | time: 11.03179264068603572\n",
      "Epoach: 965 | avg loss: 0.14974345 | max loss: 0.19224682 | min loss: 0.10950081.10950081 | time: 11.089083433151245\n",
      "Epoach: 966 | avg loss: 0.154953 | max loss: 0.20702982 | min loss: 0.112848550.11284855 | time: 11.037848949432373\n",
      "Epoach: 967 | avg loss: 0.15906355 | max loss: 0.1994035 | min loss: 0.11237597512375975 | time: 11.05658125877380494\n",
      "Epoach: 968 | avg loss: 0.15650426 | max loss: 0.21561725 | min loss: 0.10643817506438175 | time: 10.9669146537780762\n",
      "Epoach: 969 | avg loss: 0.14088187 | max loss: 0.16793686 | min loss: 0.1233489412334894 | time: 11.026108980178833\n",
      "Epoach: 970 | avg loss: 0.14387475 | max loss: 0.23325431 | min loss: 0.0963667909636679 | time: 10.985936880111694452\n",
      "Epoach: 971 | avg loss: 0.16050595 | max loss: 0.20272146 | min loss: 0.1282996412829964 | time: 11.030813217163086\n",
      "Epoach: 972 | avg loss: 0.15742391 | max loss: 0.19432522 | min loss: 0.0832142408321424 | time: 10.934549093246467\n",
      "Epoach: 973 | avg loss: 0.13773513 | max loss: 0.18768582 | min loss: 0.0968009209680092 | time: 11.039634943008423\n",
      "Epoach: 974 | avg loss: 0.15648425 | max loss: 0.20638362 | min loss: 0.1207316812073168 | time: 10.983601808547974\n",
      "Epoach: 975 | avg loss: 0.16709967 | max loss: 0.2262968 | min loss: 0.1189307.1189307 | time: 11.0474042892456054\n",
      "Epoach: 976 | avg loss: 0.1432213 | max loss: 0.17835152 | min loss: 0.075957924075957924 | time: 11.03091096878051861\n",
      "Epoach: 977 | avg loss: 0.15696819 | max loss: 0.18370494 | min loss: 0.127801272780127 | time: 11.0681898593902593\n",
      "Epoach: 978 | avg loss: 0.14392573 | max loss: 0.1903676 | min loss: 0.105797805105797805 | time: 11.0352389812469485\n",
      "Epoach: 979 | avg loss: 0.13716182 | max loss: 0.2043146 | min loss: 0.095612419561241 | time: 11.0509271621704146606\n",
      "Step: 7 | loss: 0.11965169 | current max loss: 0.23257637 | current min loss: 0.108585656 | time: 11.021334171295166\n",
      "Predict query: 81\n",
      "TEST set, len  82\n",
      "MAP @10 bioASQ: 0.03585753000387145\n",
      "MAP @25: 0.046232741169347946\n",
      "MAP @50: 0.0535120101725588\n",
      "MAP @100: 0.06007656873292889\n",
      "MAP @200: 0.06604727862312952\n",
      "MAP @300: 0.06925378220923331\n",
      "MAP: 0.08296426500584188\n",
      "RECALL@10: 0.14422239557225736\n",
      "RECALL@50: 0.32401099697471636\n",
      "RECALL@100: 0.4424871387034096\n",
      "TEST set, len  85\n",
      "MAP @10 bioASQ: 0.1656363211951447\n",
      "MAP @25: 0.22216485201256253\n",
      "MAP @50: 0.2639693418742906\n",
      "MAP @100: 0.29130137751034524\n",
      "MAP @200: 0.3033802960203419\n",
      "MAP @300: 0.3053991387320452\n",
      "MAP: 0.3018453590657162\n",
      "RECALL@10: 0.5144593431043789\n",
      "RECALL@50: 0.8491288909715703\n",
      "RECALL@100: 0.9495548995458926\n",
      "Epoach: 980 | avg loss: 0.14531815 | max loss: 0.23257637 | min loss: 0.108585656\n",
      "Epoach: 981 | avg loss: 0.13021253 | max loss: 0.18461049 | min loss: 0.0879837408798374 | time: 11.050083875656128\n",
      "Epoach: 982 | avg loss: 0.14715347 | max loss: 0.22018977 | min loss: 0.1087219110872191 | time: 11.066186428070068\n",
      "Epoach: 983 | avg loss: 0.13080472 | max loss: 0.14586642 | min loss: 0.1089334710893347 | time: 11.0971713066101074\n",
      "Epoach: 984 | avg loss: 0.16214713 | max loss: 0.22626442 | min loss: 0.11267809612678096 | time: 11.0289347171783455\n",
      "Epoach: 985 | avg loss: 0.11853247 | max loss: 0.15039243 | min loss: 0.0781746368174636 | time: 11.080292940139770394\n",
      "Epoach: 986 | avg loss: 0.15382627 | max loss: 0.20180485 | min loss: 0.1075439810754398 | time: 11.050221204757695147\n",
      "Epoach: 987 | avg loss: 0.13887873 | max loss: 0.17882103 | min loss: 0.0977378909773789 | time: 11.0040881633758549\n",
      "Epoach: 988 | avg loss: 0.12607336 | max loss: 0.1500845 | min loss: 0.08277970682779706 | time: 11.07324838638305742\n",
      "Epoach: 989 | avg loss: 0.14345282 | max loss: 0.20044349 | min loss: 0.0940004209400042 | time: 11.068215608596802\n",
      "Epoach: 990 | avg loss: 0.12559573 | max loss: 0.16209799 | min loss: 0.04554994845549948 | time: 11.0540461540222176\n",
      "Epoach: 991 | avg loss: 0.12832987 | max loss: 0.16047823 | min loss: 0.0834102608341026 | time: 10.96672701835632333\n",
      "Epoach: 992 | avg loss: 0.13611898 | max loss: 0.15830863 | min loss: 0.1073415.1073415 | time: 11.0102152824401866\n",
      "Epoach: 993 | avg loss: 0.13304445 | max loss: 0.18301097 | min loss: 0.1095337710953377 | time: 11.054373025894165\n",
      "Epoach: 994 | avg loss: 0.16056831 | max loss: 0.20198868 | min loss: 0.1108163311081633 | time: 11.034053564071655\n",
      "Epoach: 995 | avg loss: 0.13692793 | max loss: 0.16834337 | min loss: 0.0928672209286722 | time: 11.011092185974121\n",
      "Epoach: 996 | avg loss: 0.16334999 | max loss: 0.21721025 | min loss: 0.1341197213411972 | time: 11.062739849090576\n",
      "Epoach: 997 | avg loss: 0.122523546 | max loss: 0.18512616 | min loss: 0.080728088072808 | time: 11.00350594520568897\n",
      "Epoach: 998 | avg loss: 0.12503296 | max loss: 0.1957272 | min loss: 0.0730282107302821 | time: 11.0381624698638924\n",
      "Epoach: 999 | avg loss: 0.15700005 | max loss: 0.20362897 | min loss: 0.1219129312191293 | time: 11.028763771057129\n"
     ]
    }
   ],
   "source": [
    "gen = TrainDataGenerator(train_articles_collection, tk, 256)\n",
    "\n",
    "gen_iter = iter(gen)\n",
    "\n",
    "#loss = []\n",
    "\n",
    "for i,line in enumerate(loss):\n",
    "    print(\"Epoach:\",i,\"| avg loss:\",np.mean(loss[i]),\"| max loss:\",np.max(loss[i]),\"| min loss:\",np.min(loss[i]))\n",
    "\n",
    "import time\n",
    "\n",
    "max_bio_map_val = 0.10\n",
    "max_bio_map_test = 0\n",
    "\n",
    "for epoach in range(950,1000):\n",
    "    loss_per_epoach = []\n",
    "    for step in range(len(gen)):\n",
    "        X = next(gen_iter)\n",
    "        \n",
    "        start = time.time()\n",
    "        loss_per_epoach.append(har_trainable_model.train_on_batch(X))\n",
    "        print(\"Step:\",step,\"| loss:\",loss_per_epoach[-1],\"| current max loss:\",np.max(loss_per_epoach),\"| current min loss:\",np.min(loss_per_epoach),\"| time:\",time.time()-start,end=\"\\r\")\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    if epoach%20==0:\n",
    "        print(\"\")\n",
    "        validate_query_results = validate_test_data(validation_articles_collection)\n",
    "        print(\"\")\n",
    "        bio_map_val,_,_ = validation_score(validate_query_results)\n",
    "        _ = validation_score(validate_test_data(validation_train_articles_collection))\n",
    "        \n",
    "        if bio_map_val >= max_bio_map_val:\n",
    "            max_bio_map_val = bio_map_val\n",
    "            print(\"\")\n",
    "            print(\"Run for the test set\")\n",
    "            test_query_results = validate_test_data(test_articles_collection)\n",
    "            bio_map_test = validation_score(test_query_results)\n",
    "            \n",
    "            if bio_map_test >= max_bio_map_test:\n",
    "                max_bio_map_test = bio_map_test\n",
    "                \n",
    "                har_model.save_weights(\"har_weights.h5\")\n",
    "                #har_model.save(\"har_model.h5\")\n",
    "                \n",
    "    loss.append(loss_per_epoach)\n",
    "    print(\"\",end=\"\\r\")#clear the line\n",
    "    print(\"Epoach:\",epoach,\"| avg loss:\",np.mean(loss[-1]),\"| max loss:\",np.max(loss[-1]),\"| min loss:\",np.min(loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2137\n",
      "85\n",
      "validation train size 85\n"
     ]
    }
   ],
   "source": [
    "validation_train_articles_collection = {\"bioasq_data\":[],\"collection\":train_articles_collection[\"collection\"]}\n",
    "\n",
    "_temp = train_articles_collection[\"bioasq_data\"][:]\n",
    "random.shuffle(_temp)\n",
    "print(len(_temp))\n",
    "\n",
    "validation_percentage = 0.04\n",
    "\n",
    "split_index = int(len(_temp)*validation_percentage)\n",
    "print(split_index)\n",
    "\n",
    "validation_train_articles_collection[\"bioasq_data\"] = _temp[:split_index]\n",
    "#test_articles_collection[\"bioasq_data\"] = _temp[split_index:]\n",
    "\n",
    "print(\"validation train size\",len(validation_train_articles_collection[\"bioasq_data\"]))\n",
    "\n",
    "del _temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST set, len  542\n",
      "MAP @10 bioASQ: 0.04496676038188954\n",
      "MAP @25: 0.0634085317578492\n",
      "MAP @50: 0.07977501611963189\n",
      "MAP @100: 0.09286495409665439\n",
      "MAP @200: 0.10269169701889902\n",
      "MAP @300: 0.10747185573911922\n",
      "MAP: 0.077011123930585\n",
      "RECALL@10: 0.1441388080767276\n",
      "RECALL@50: 0.34713683151100727\n",
      "RECALL@100: 0.474338002391366\n"
     ]
    }
   ],
   "source": [
    "test = validate_test_data(test_articles_collection)\n",
    "_test_validation_scores = validation_score(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST set, len  2137\n",
      "MAP @10 bioASQ: 0.17697686639778398\n",
      "MAP @25: 0.25510361756180516\n",
      "MAP @50: 0.30344370648908725\n",
      "MAP @100: 0.33134215979994863\n",
      "MAP @200: 0.3429918827961693\n",
      "MAP @300: 0.34565611336998997\n",
      "MAP: 0.32187774858350454\n",
      "RECALL@10: 0.5204272246827295\n",
      "RECALL@50: 0.8622614773925681\n",
      "RECALL@100: 0.9447249088296876\n"
     ]
    }
   ],
   "source": [
    "_train = validate_test_data(train_articles_collection)\n",
    "_train_validation_scores = validation_score(_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "har_model.save_weights(\"har_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
