{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "import os\n",
    "os.chdir(\"/home/tiagoalmeida/bioASQ-taskb/\")\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from bisect import bisect\n",
    "\n",
    "\n",
    "##add keras to the modules\n",
    "module_path = os.path.abspath(os.path.join('pubmed_data'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from pubmed_data import pubmed_helper as ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepRank\n",
    "Reference PAPER :https://arxiv.org/pdf/1710.05649.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network structure\n",
    " - [General Network Configuration](#var_def)\n",
    " - [Input Network](#input_net)\n",
    " - [Measure Network](#measure_net)\n",
    " - [Aggregation Network](#aggreation_net)\n",
    " - [Final Network](#final_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load regex_full_tokens_tokenizer.p\n",
      "Load regex_full_tokens_word_embedding.p\n"
     ]
    }
   ],
   "source": [
    "#Load tokenizer and the embedding matrix\n",
    "\n",
    "MODE = \"regex_full_tokens\"\n",
    "tk = ph.load_tokenizer(mode=MODE)\n",
    "emb_dict = ph.load_embeddings(mode=MODE)\n",
    "\n",
    "assert len(tk.word_counts) == len(emb_dict)\n",
    "\n",
    "#Number of different words\n",
    "VOCAB_SIZE = len(tk.word_counts)+1\n",
    "\n",
    "#Dimension of embeddings\n",
    "EMB_DIM = emb_dict[1].shape[0]\n",
    "\n",
    "emb_matrix = np.zeros((VOCAB_SIZE, EMB_DIM))\n",
    "\n",
    "for i,vector in emb_dict.items():\n",
    "    emb_matrix[i] = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenized stopwords\n",
    "\n",
    "biomedical_stop_words = [\"a\", \"about\", \"again\", \"all\", \"almost\", \"also\", \"although\", \"always\", \"among\", \"an\", \"and\", \"another\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"between\", \"both\", \"but\", \"by\", \"can\", \"could\", \"did\", \"do\", \"does\", \"done\", \"due\", \"during\", \"each\", \"either\", \"enough\", \"especially\", \"etc\", \"for\", \"found\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"here\", \"how\", \"however\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"just\", \"kg\", \"km\", \"made\", \"mainly\", \"make\", \"may\", \"mg\", \"might\", \"ml\", \"mm\", \"most\", \"mostly\", \"must\", \"nearly\", \"neither\", \"no\", \"nor\", \"obtained\", \"of\", \"often\", \"on\", \"our\", \"overall\", \"perhaps\", \"pmid\", \"quite\", \"rather\", \"really\", \"regarding\", \"seem\", \"seen\", \"several\", \"should\", \"show\", \"showed\", \"shown\", \"shows\", \"significantly\", \"since\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"then\", \"there\", \"therefore\", \"these\", \"they\", \"this\", \"those\", \"through\", \"thus\", \"to\", \"upon\", \"use\", \"used\", \"using\", \"various\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"which\", \"while\", \"with\", \"within\", \"without\", \"would\"]\n",
    "biomedical_stop_words_tokens = set(tk.texts_to_sequences([biomedical_stop_words])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='var_def'></a>\n",
    "## General Network Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import unstack, stack\n",
    "##Test \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, activations\n",
    "from tensorflow.keras.initializers import Zeros, Ones, Constant\n",
    "from tensorflow.keras.layers import Dense, Lambda, Bidirectional, Dot,Masking,Reshape, Concatenate, Layer, Embedding, Input, Conv2D, GlobalMaxPooling2D, Flatten, TimeDistributed, GRU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.activations import tanh, sigmoid\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from models.deep_model_for_ir.custom_layers import MaskedSelfAttention\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "#Number max of term per query\n",
    "MAX_Q_TERM = 13\n",
    "\n",
    "#Number max of the snippet terms\n",
    "QUERY_CENTRIC_CONTEX = 15\n",
    "\n",
    "#Number max of passages per query term\n",
    "MAX_PASSAGES_PER_QUERY = 5\n",
    "\n",
    "#Snippet position padding value\n",
    "SNIPPET_POSITION_PADDING_VALUE = -1\n",
    "\n",
    "#Mode for the creation of the S matrix\n",
    "S_MATRIX_MODE = 0\n",
    "#S_MATRIX_DIMENSION = EMB_DIM*2+1\n",
    "\n",
    "#Train embedding weights\n",
    "EMB_TRAINABLE = False\n",
    "\n",
    "#Number of filters in CNN\n",
    "CNN_FILTERS = 100\n",
    "CNN_KERNELS = (3,3)\n",
    "\n",
    "#RNN DIM\n",
    "USE_BIDIRECTIONAL = False\n",
    "GRU_REPRESENTATION_DIM = 58\n",
    "\n",
    "ACTIVATION_FUNCTION = \"selu\"\n",
    "\n",
    "REGULARIZATION = regularizers.l2(0.0001)\n",
    "\n",
    "#Term gating network mode\n",
    "TERM_GATING_MODE =  3#2- weigt fixed per position, 1 - DRMM like term gating\n",
    "\n",
    "assert S_MATRIX_MODE in [0,1]\n",
    "assert TERM_GATING_MODE in [0,1,2,3]\n",
    "\n",
    "#MACRO STYLE\n",
    "def S_MATRIX_3D_DIMENSION():\n",
    "    if S_MATRIX_MODE==0:\n",
    "        return 1\n",
    "    elif S_MATRIX_MODE==1:\n",
    "        return EMB_DIM*2+1\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='input_net'></a>\n",
    "## Input Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "snippet_emb_model summary\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "snippet_token (InputLayer)   (None, 13, 5, 15)         0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  (None, 13, 5, 15, 200)    858359000 \n",
      "_________________________________________________________________\n",
      "snippet_transpose (Lambda)   (None, 13, 5, 200, 15)    0         \n",
      "=================================================================\n",
      "Total params: 858,359,000\n",
      "Trainable params: 0\n",
      "Non-trainable params: 858,359,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "  ---- Custom Layers ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "class SimilarityMatrix(Layer):\n",
    "    \n",
    "    def __init__(self, query_max_term, snippet_max_term, interaction_mode=0, **kwargs):\n",
    "        \"\"\"\n",
    "        interaction mode 0: only use similarity matrix\n",
    "                    mode 1: similarity matrix + query and snippet embeddings\n",
    "        \"\"\"\n",
    "        assert interaction_mode in [0,1] #only valid modes\n",
    "        \n",
    "        self.query_max_term = query_max_term\n",
    "        self.snippet_max_term = snippet_max_term\n",
    "        self.interaction_mode = interaction_mode\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self,x):\n",
    "        if self.interaction_mode==0:\n",
    "            #sim => dot product (None, MAX_Q_TERM, EMB_DIM) x (None, MAX_Q_TERM, MAX_PASSAGE_PER_Q, EMB_DIM, QUERY_CENTRIC_CONTEX)\n",
    "            query = K.expand_dims(x[0], axis=1) #(None, 1, MAX_Q_TERM, EMB_DIM)\n",
    "            query = K.expand_dims(query, axis=1) #(None, 1, 1, MAX_Q_TERM, EMB_DIM)\n",
    "            query = K.repeat_elements(query,x[1].shape[1],axis=1) #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, EMB_DIM)\n",
    "            query = K.repeat_elements(query,x[1].shape[2],axis=2)\n",
    "            s_matrix = K.batch_dot(query,x[1]) #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, EMB_DIM)\n",
    "            \n",
    "            s_matrix = K.expand_dims(s_matrix)\n",
    "            \n",
    "            return s_matrix #Add one more dimension #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, EMB_DIM, 1)\n",
    "        elif self.interaction_mode==1:\n",
    "            raise NotImplementedError(\"interaction mode of layer SimilarityMatrix is not implemented\")\n",
    "    \"\"\"                  \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.interaction_mode==0:\n",
    "            return (input_shape[0][0], input_shape[0][1], self.query_max_term, self.snippet_max_term, 1)\n",
    "        elif self.interaction_mode==1:\n",
    "            return (input_shape[0][0], input_shape[0][1], self.query_max_term, self.snippet_max_term, input_shape[0][2]*input_shape[1][2]+1) \n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "     ---- Layers ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "#Embedding Layer\n",
    "embedding = Embedding(VOCAB_SIZE,EMB_DIM, name=\"embedding_layer\",weights=[emb_matrix], trainable=EMB_TRAINABLE)\n",
    "\n",
    "#S matrix ref in the paper\n",
    "similarity_matrix = SimilarityMatrix(MAX_Q_TERM, QUERY_CENTRIC_CONTEX, interaction_mode=S_MATRIX_MODE, name=\"query_snippet_similarity\")\n",
    "\n",
    "#transpose (None, QUERY_CENTRIC_CONTEX, EMB_DIM) => (None, EMB_DIM, QUERY_CENTRIC_CONTEX) \n",
    "transpose_layer = Lambda(lambda x:K.permute_dimensions(x,[0,1,2,4,3]), name=\"snippet_transpose\") \n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    " ---- Auxiliar Models ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "#Snippet single embedding transformation\n",
    "snippet_token_input = Input(shape = (MAX_Q_TERM, MAX_PASSAGES_PER_QUERY, QUERY_CENTRIC_CONTEX,), name = \"snippet_token\")\n",
    "snippet_emb = embedding(snippet_token_input)\n",
    "snippet_emb_transpose = transpose_layer(snippet_emb)\n",
    "snippet_emb_model = Model(inputs = [snippet_token_input], outputs=[snippet_emb_transpose], name = \"snippet_emb_model\")\n",
    "print(\"\\n\\nsnippet_emb_model summary\")\n",
    "snippet_emb_model.summary()\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "  ---- Input Network ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "if DEBUG:\n",
    "    query_token_input = Input(shape=(MAX_Q_TERM,), name=\"query_tokens\")\n",
    "\n",
    "\n",
    "    snippets_tokens_input = Input(shape = (MAX_Q_TERM, MAX_PASSAGES_PER_QUERY, QUERY_CENTRIC_CONTEX), name = \"snippet_tokens_ipmodel\") \n",
    "    \n",
    "    query_emb = embedding(query_token_input)\n",
    "\n",
    "    snippet_emb = embedding(snippets_tokens_input)\n",
    "    snippet_emb_transpose = transpose_layer(snippet_emb)\n",
    "    \n",
    "    sim_matrix_layer = similarity_matrix([query_emb,snippet_emb_transpose])\n",
    "    \n",
    "    \n",
    "    \n",
    "    input_model = Model(inputs = [query_token_input,snippets_tokens_input], outputs=[sim_matrix_layer], name=\"input_model\")\n",
    "    print(\"\\n\\ninput_model summary\")\n",
    "    input_model.summary()\n",
    "    \n",
    "    \n",
    "    print(\"\\nOutput tensor\",sim_matrix_layer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='measure_net'></a>\n",
    "## Measure Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "cnn_extraction_model summary\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masked_conv2d (MaskedConv2D) (None, 11, 13, 100)       1000      \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 1,000\n",
      "Trainable params: 1,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Tensor(\"time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri (None, 5, 100)            1000      \n",
      "=================================================================\n",
      "Total params: 1,000\n",
      "Trainable params: 1,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "class MaskedConv2D(Layer):\n",
    "    \n",
    "    def __init__(self, filters, kernel_size, activation, regularizer=None, **kargs):\n",
    "        super(MaskedConv2D, self).__init__(**kargs)\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        \n",
    "        if regularizer is None or isinstance(regularizer,str):\n",
    "            self.regularizer = regularizers.get(regularizer)\n",
    "        else:\n",
    "            self.regularizer = regularizer\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.conv2dlayer = Conv2D( filters = self.filters, kernel_size=self.kernel_size, activation=self.activation, kernel_regularizer=self.regularizer )\n",
    "        self.conv2dlayer.build(input_shape)\n",
    "        self._trainable_weights = self.conv2dlayer.trainable_weights\n",
    "        \n",
    "        super(MaskedConv2D, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        condition = K.all(x) #if all the values are the same\n",
    "        inv_condition = (1-K.cast(condition, K.floatx()))\n",
    "        print(inv_condition)\n",
    "        feature_maps = self.conv2dlayer(x)\n",
    "        \n",
    "        return feature_maps * inv_condition\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    " ---- Auxiliar Models ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "#Exctrate high-level features from query and snippet interactions with CNN\n",
    "cnn_extraction_model = Sequential(name=\"cnn_extraction_model\")\n",
    "cnn_extraction_model.add(MaskedConv2D(input_shape = (MAX_Q_TERM, QUERY_CENTRIC_CONTEX, S_MATRIX_3D_DIMENSION()), filters = CNN_FILTERS, kernel_size=CNN_KERNELS, activation=ACTIVATION_FUNCTION ))\n",
    "cnn_extraction_model.add(GlobalMaxPooling2D())\n",
    "print(\"\\n\\ncnn_extraction_model summary\")\n",
    "cnn_extraction_model.summary()\n",
    "\n",
    "\n",
    "td_cnn_extraction_model = Sequential(name=\"TD_cnn_extraction_model\")\n",
    "td_cnn_extraction_model.add(TimeDistributed(cnn_extraction_model, input_shape=(MAX_PASSAGES_PER_QUERY, MAX_Q_TERM, QUERY_CENTRIC_CONTEX, S_MATRIX_3D_DIMENSION())))\n",
    "td_cnn_extraction_model.summary()\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "     ---- Layers ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "#concatenation layer over the last dimension\n",
    "concat_snippet_position = Concatenate( name = \"concat_snippet_position\")\n",
    "\n",
    "self_attention = MaskedSelfAttention(CNN_FILTERS+1)\n",
    "\n",
    "#add dimension Layer\n",
    "add_passage_dim = Lambda(lambda x:K.expand_dims(x,axis=1), name=\"add_passage_dim\")#Reshape(target_shape=(1,GRU_REPRESENTATION_DIM))\n",
    "\n",
    "#add last dimension Layer\n",
    "add_dim = Lambda(lambda x:K.expand_dims(x), name=\"add_dim\")\n",
    "\n",
    "#reciprocal function\n",
    "reciprocal_f = Lambda(lambda x:1/(x+2), name=\"reciprocal_function\")\n",
    "\n",
    "#concatenation layer over second dimension (passage dimension)\n",
    "concat_representation = Concatenate(axis = 1,name = \"concat_representation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='measure_net'></a>\n",
    "## Aggregation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "  ---- Custom Layers ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "snippet_rnn_rep_dim = CNN_FILTERS+1\n",
    "\n",
    "\n",
    "    \n",
    "class TermGatingDRMM_FFN(Layer):\n",
    "    \n",
    "    def __init__(self, embedding_dim = EMB_DIM, rnn_dim = snippet_rnn_rep_dim ,activation=None, initializer='glorot_normal', regularizer=None):\n",
    "        super(TermGatingDRMM_FFN, self).__init__()\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        \n",
    "        if regularizer is None or isinstance(regularizer,str):\n",
    "            self.regularizer = regularizers.get(regularizer)\n",
    "        else:\n",
    "            self.regularizer = regularizer\n",
    "        \n",
    "        self.emb_dim = embedding_dim\n",
    "        self.rnn_dim = rnn_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        #term gating W\n",
    "        self.W_query = self.add_variable(name = \"term_gating_We\",\n",
    "                                   shape = [self.emb_dim,1],\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.dense_score = Dense(1,kernel_regularizer = self.regularizer, activation=self.activation)\n",
    "        \n",
    "        dense_shape = input_shape[1]\n",
    "        print(dense_shape)\n",
    "        \n",
    "        self.dense_score.build((dense_shape[0],dense_shape[2]))\n",
    "        self._trainable_weights += self.dense_score.trainable_weights\n",
    "        #self.ones = K.constant(np.ones((aggreation_dimension,1)))\n",
    "        \n",
    "        super(TermGatingDRMM_FFN, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        query_embeddings = x[0] #(None, MAX_Q_TERM, EMB_SIZE)\n",
    "        snippet_representation_per_query = x[1] #(None, MAX_Q_TERM, BI_GRU_DIM)\n",
    "        \n",
    "        #compute gated weights\n",
    "        gated_logits = K.squeeze(K.dot(query_embeddings, self.W_query), axis = -1 )\n",
    "        #print(gated_logits)\n",
    "        gated_distribution = K.expand_dims(K.softmax(gated_logits))\n",
    "        #print(gated_distribution)\n",
    "        #snippet projection\n",
    "        \n",
    "        weighted_score = K.sum(snippet_representation_per_query * gated_distribution,  axis = 1)\n",
    "        print(weighted_score)\n",
    "        \n",
    "        return self.dense_score(weighted_score) # Replace with K.sum of all elements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='final_net'></a>\n",
    "## Final Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"TD_cnn_extraction_model/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"masked_self_attention/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"masked_self_attention/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"masked_self_attention/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"masked_self_attention/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"masked_self_attention/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"masked_self_attention/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"masked_self_attention/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"masked_self_attention/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"masked_self_attention/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_1/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"masked_self_attention_1/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"masked_self_attention_1/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"masked_self_attention_1/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"masked_self_attention_1/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"masked_self_attention_1/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"masked_self_attention_1/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"masked_self_attention_1/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"masked_self_attention_1/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"masked_self_attention_1/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_2/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"masked_self_attention_2/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"masked_self_attention_2/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"masked_self_attention_2/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"masked_self_attention_2/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"masked_self_attention_2/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"masked_self_attention_2/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"masked_self_attention_2/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"masked_self_attention_2/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"masked_self_attention_2/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_3/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"masked_self_attention_3/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"masked_self_attention_3/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"masked_self_attention_3/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"masked_self_attention_3/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"masked_self_attention_3/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"masked_self_attention_3/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"masked_self_attention_3/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"masked_self_attention_3/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"masked_self_attention_3/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_4/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"masked_self_attention_4/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"masked_self_attention_4/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"masked_self_attention_4/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"masked_self_attention_4/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"masked_self_attention_4/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"masked_self_attention_4/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"masked_self_attention_4/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"masked_self_attention_4/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"masked_self_attention_4/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_5/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"masked_self_attention_5/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"masked_self_attention_5/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"masked_self_attention_5/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"masked_self_attention_5/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"masked_self_attention_5/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"masked_self_attention_5/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"masked_self_attention_5/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"masked_self_attention_5/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"masked_self_attention_5/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_6/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"masked_self_attention_6/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"masked_self_attention_6/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"masked_self_attention_6/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"masked_self_attention_6/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"masked_self_attention_6/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"masked_self_attention_6/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"masked_self_attention_6/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"masked_self_attention_6/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"masked_self_attention_6/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_7/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"masked_self_attention_7/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"masked_self_attention_7/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"masked_self_attention_7/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"masked_self_attention_7/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"masked_self_attention_7/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"masked_self_attention_7/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"masked_self_attention_7/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"masked_self_attention_7/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"masked_self_attention_7/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_8/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"masked_self_attention_8/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"masked_self_attention_8/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"masked_self_attention_8/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"masked_self_attention_8/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"masked_self_attention_8/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"masked_self_attention_8/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"masked_self_attention_8/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"masked_self_attention_8/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"masked_self_attention_8/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_9/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"masked_self_attention_9/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"masked_self_attention_9/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"masked_self_attention_9/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"masked_self_attention_9/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"masked_self_attention_9/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"masked_self_attention_9/add:0\", shape=(?, 5, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_attention_softmax Tensor(\"masked_self_attention_9/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"masked_self_attention_9/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"masked_self_attention_9/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_10/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"masked_self_attention_10/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"masked_self_attention_10/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"masked_self_attention_10/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"masked_self_attention_10/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"masked_self_attention_10/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"masked_self_attention_10/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"masked_self_attention_10/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"masked_self_attention_10/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"masked_self_attention_10/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_11/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"masked_self_attention_11/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"masked_self_attention_11/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"masked_self_attention_11/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"masked_self_attention_11/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"masked_self_attention_11/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"masked_self_attention_11/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"masked_self_attention_11/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"masked_self_attention_11/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"masked_self_attention_11/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_12/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"masked_self_attention_12/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"masked_self_attention_12/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"masked_self_attention_12/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"masked_self_attention_12/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"masked_self_attention_12/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"masked_self_attention_12/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"masked_self_attention_12/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"masked_self_attention_12/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"masked_self_attention_12/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "(?, 13, 101)\n",
      "Tensor(\"term_gating_drmm_ffn/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ds_query_tokens (InputLayer)    (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ds_snippet_tokens (InputLayer)  (None, 13, 5, 15)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     multiple             858359000   ds_query_tokens[0][0]            \n",
      "                                                                 ds_snippet_tokens[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "snippet_transpose (Lambda)      (None, 13, 5, 200, 1 0           embedding_layer[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "query_snippet_similarity (Simil (None, 13, 5, 13, 15 0           embedding_layer[1][0]            \n",
      "                                                                 snippet_transpose[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ds_snippet_position_tokens (Inp (None, 13, 5)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "unstack_query_term (Lambda)     multiple             0           query_snippet_similarity[0][0]   \n",
      "                                                                 ds_snippet_position_tokens[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reciprocal_function (Lambda)    (None, 5)            0           unstack_query_term[1][0]         \n",
      "                                                                 unstack_query_term[1][1]         \n",
      "                                                                 unstack_query_term[1][2]         \n",
      "                                                                 unstack_query_term[1][3]         \n",
      "                                                                 unstack_query_term[1][4]         \n",
      "                                                                 unstack_query_term[1][5]         \n",
      "                                                                 unstack_query_term[1][6]         \n",
      "                                                                 unstack_query_term[1][7]         \n",
      "                                                                 unstack_query_term[1][8]         \n",
      "                                                                 unstack_query_term[1][9]         \n",
      "                                                                 unstack_query_term[1][10]        \n",
      "                                                                 unstack_query_term[1][11]        \n",
      "                                                                 unstack_query_term[1][12]        \n",
      "__________________________________________________________________________________________________\n",
      "TD_cnn_extraction_model (Sequen (None, 5, 100)       1000        unstack_query_term[0][0]         \n",
      "                                                                 unstack_query_term[0][1]         \n",
      "                                                                 unstack_query_term[0][2]         \n",
      "                                                                 unstack_query_term[0][3]         \n",
      "                                                                 unstack_query_term[0][4]         \n",
      "                                                                 unstack_query_term[0][5]         \n",
      "                                                                 unstack_query_term[0][6]         \n",
      "                                                                 unstack_query_term[0][7]         \n",
      "                                                                 unstack_query_term[0][8]         \n",
      "                                                                 unstack_query_term[0][9]         \n",
      "                                                                 unstack_query_term[0][10]        \n",
      "                                                                 unstack_query_term[0][11]        \n",
      "                                                                 unstack_query_term[0][12]        \n",
      "__________________________________________________________________________________________________\n",
      "add_dim (Lambda)                (None, 5, 1)         0           reciprocal_function[0][0]        \n",
      "                                                                 reciprocal_function[1][0]        \n",
      "                                                                 reciprocal_function[2][0]        \n",
      "                                                                 reciprocal_function[3][0]        \n",
      "                                                                 reciprocal_function[4][0]        \n",
      "                                                                 reciprocal_function[5][0]        \n",
      "                                                                 reciprocal_function[6][0]        \n",
      "                                                                 reciprocal_function[7][0]        \n",
      "                                                                 reciprocal_function[8][0]        \n",
      "                                                                 reciprocal_function[9][0]        \n",
      "                                                                 reciprocal_function[10][0]       \n",
      "                                                                 reciprocal_function[11][0]       \n",
      "                                                                 reciprocal_function[12][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concat_snippet_position (Concat (None, 5, 101)       0           TD_cnn_extraction_model[1][0]    \n",
      "                                                                 add_dim[0][0]                    \n",
      "                                                                 TD_cnn_extraction_model[2][0]    \n",
      "                                                                 add_dim[1][0]                    \n",
      "                                                                 TD_cnn_extraction_model[3][0]    \n",
      "                                                                 add_dim[2][0]                    \n",
      "                                                                 TD_cnn_extraction_model[4][0]    \n",
      "                                                                 add_dim[3][0]                    \n",
      "                                                                 TD_cnn_extraction_model[5][0]    \n",
      "                                                                 add_dim[4][0]                    \n",
      "                                                                 TD_cnn_extraction_model[6][0]    \n",
      "                                                                 add_dim[5][0]                    \n",
      "                                                                 TD_cnn_extraction_model[7][0]    \n",
      "                                                                 add_dim[6][0]                    \n",
      "                                                                 TD_cnn_extraction_model[8][0]    \n",
      "                                                                 add_dim[7][0]                    \n",
      "                                                                 TD_cnn_extraction_model[9][0]    \n",
      "                                                                 add_dim[8][0]                    \n",
      "                                                                 TD_cnn_extraction_model[10][0]   \n",
      "                                                                 add_dim[9][0]                    \n",
      "                                                                 TD_cnn_extraction_model[11][0]   \n",
      "                                                                 add_dim[10][0]                   \n",
      "                                                                 TD_cnn_extraction_model[12][0]   \n",
      "                                                                 add_dim[11][0]                   \n",
      "                                                                 TD_cnn_extraction_model[13][0]   \n",
      "                                                                 add_dim[12][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masked_self_attention (MaskedSe (None, 101)          10302       concat_snippet_position[0][0]    \n",
      "                                                                 concat_snippet_position[1][0]    \n",
      "                                                                 concat_snippet_position[2][0]    \n",
      "                                                                 concat_snippet_position[3][0]    \n",
      "                                                                 concat_snippet_position[4][0]    \n",
      "                                                                 concat_snippet_position[5][0]    \n",
      "                                                                 concat_snippet_position[6][0]    \n",
      "                                                                 concat_snippet_position[7][0]    \n",
      "                                                                 concat_snippet_position[8][0]    \n",
      "                                                                 concat_snippet_position[9][0]    \n",
      "                                                                 concat_snippet_position[10][0]   \n",
      "                                                                 concat_snippet_position[11][0]   \n",
      "                                                                 concat_snippet_position[12][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_passage_dim (Lambda)        (None, 1, 101)       0           masked_self_attention[0][0]      \n",
      "                                                                 masked_self_attention[1][0]      \n",
      "                                                                 masked_self_attention[2][0]      \n",
      "                                                                 masked_self_attention[3][0]      \n",
      "                                                                 masked_self_attention[4][0]      \n",
      "                                                                 masked_self_attention[5][0]      \n",
      "                                                                 masked_self_attention[6][0]      \n",
      "                                                                 masked_self_attention[7][0]      \n",
      "                                                                 masked_self_attention[8][0]      \n",
      "                                                                 masked_self_attention[9][0]      \n",
      "                                                                 masked_self_attention[10][0]     \n",
      "                                                                 masked_self_attention[11][0]     \n",
      "                                                                 masked_self_attention[12][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concat_representation (Concaten (None, 13, 101)      0           add_passage_dim[0][0]            \n",
      "                                                                 add_passage_dim[1][0]            \n",
      "                                                                 add_passage_dim[2][0]            \n",
      "                                                                 add_passage_dim[3][0]            \n",
      "                                                                 add_passage_dim[4][0]            \n",
      "                                                                 add_passage_dim[5][0]            \n",
      "                                                                 add_passage_dim[6][0]            \n",
      "                                                                 add_passage_dim[7][0]            \n",
      "                                                                 add_passage_dim[8][0]            \n",
      "                                                                 add_passage_dim[9][0]            \n",
      "                                                                 add_passage_dim[10][0]           \n",
      "                                                                 add_passage_dim[11][0]           \n",
      "                                                                 add_passage_dim[12][0]           \n",
      "__________________________________________________________________________________________________\n",
      "term_gating_drmm_ffn (TermGatin (None, 1)            302         embedding_layer[1][0]            \n",
      "                                                                 concat_representation[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 858,370,604\n",
      "Trainable params: 11,604\n",
      "Non-trainable params: 858,359,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "  ---- Final Network ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "query_token_input = Input(shape=(MAX_Q_TERM,), name=\"ds_query_tokens\")\n",
    "doc_score_snippet_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), name = \"ds_snippet_tokens\")\n",
    "doc_score_snippet_position_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY), name = \"ds_snippet_position_tokens\")\n",
    "\n",
    "\n",
    "unstack_by_q_term = Lambda(lambda x:unstack(x,axis=1), name=\"unstack_query_term\")\n",
    "\n",
    "#doc_score_snippet_by_q_term = unstack_by_q_term(doc_score_snippet_input)\n",
    "#doc_score_snippet_position_by_q_term = unstack_by_q_term(doc_score_snippet_position_input)\n",
    "\n",
    "#INPUT in token format\n",
    "#query_token_input = Input(shape=(MAX_Q_TERM,), name=\"query_tokens\")\n",
    "#snippets_tokens_input = [Input(shape = (MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), name = \"snippet_tokens_\"+str(q)) for q in range(MAX_Q_TERM)]\n",
    "#inputs_contex_position = [Input(shape = (MAX_PASSAGES_PER_QUERY,), name = \"q_context_position_\"+str(q)) for q in range(MAX_Q_TERM)]\n",
    "\n",
    "query_emb = embedding(query_token_input)\n",
    "\n",
    "doc_score_snippet_emb = embedding(doc_score_snippet_input)\n",
    "doc_score_snippet_emb_transpose = transpose_layer(doc_score_snippet_emb)\n",
    "\n",
    "query_snippets_s_matrix = similarity_matrix([query_emb,doc_score_snippet_emb_transpose])\n",
    "\n",
    "list_of_s_matrix_by_q_term = unstack_by_q_term(query_snippets_s_matrix)\n",
    "list_of_snippet_postion_by_q_term = unstack_by_q_term(doc_score_snippet_position_input)\n",
    "\n",
    "relevance_representation = []\n",
    "for i in range(MAX_Q_TERM):\n",
    "    \n",
    "    snippet_relative_position = reciprocal_f(list_of_snippet_postion_by_q_term[i])\n",
    "    \n",
    "    local_relevance = td_cnn_extraction_model(list_of_s_matrix_by_q_term[i])\n",
    "    \n",
    "    local_relevance_position = concat_snippet_position([local_relevance,add_dim(snippet_relative_position)])\n",
    "    \n",
    "    relevance_representation.append(add_passage_dim(self_attention(local_relevance_position)))\n",
    "\n",
    "concat_relevance = concat_representation(relevance_representation)\n",
    "\n",
    "if TERM_GATING_MODE==0:\n",
    "    term_gating = TermGating(vocab_size=VOCAB_SIZE, activation=ACTIVATION_FUNCTION)\n",
    "    document_score = term_gating([query_token_input,concat_relevance])\n",
    "    \n",
    "elif TERM_GATING_MODE==1:\n",
    "    term_gating = TermGatingDRMM()\n",
    "    document_score = term_gating([query_emb,concat_relevance])\n",
    "\n",
    "elif TERM_GATING_MODE==2:\n",
    "    term_gating = TermGatingDRMM_Projection()\n",
    "    document_score = term_gating([query_emb,concat_relevance])\n",
    "\n",
    "elif TERM_GATING_MODE==3:\n",
    "    term_gating = TermGatingDRMM_FFN(activation=ACTIVATION_FUNCTION, regularizer=REGULARIZATION)\n",
    "    document_score = term_gating([query_emb,concat_relevance])\n",
    "\n",
    "document_score_model = Model(inputs = [query_token_input, doc_score_snippet_input, doc_score_snippet_position_input], outputs = [document_score], name=\"query_document_score\")\n",
    "document_score_model.summary()      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL Trainable arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"query_document_score/TD_cnn_extraction_model/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"query_document_score/masked_self_attention/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score/masked_self_attention/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score/masked_self_attention/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score/masked_self_attention/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score/masked_self_attention/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score/masked_self_attention/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score/masked_self_attention/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score/masked_self_attention/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score/masked_self_attention/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score/masked_self_attention_1/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score/masked_self_attention_1/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score/masked_self_attention_1/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score/masked_self_attention_1/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score/masked_self_attention_1/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score/masked_self_attention_1/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score/masked_self_attention_1/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score/masked_self_attention_1/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score/masked_self_attention_1/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score/masked_self_attention_2/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score/masked_self_attention_2/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score/masked_self_attention_2/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score/masked_self_attention_2/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score/masked_self_attention_2/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score/masked_self_attention_2/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score/masked_self_attention_2/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score/masked_self_attention_2/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score/masked_self_attention_2/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score/masked_self_attention_3/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score/masked_self_attention_3/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score/masked_self_attention_3/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score/masked_self_attention_3/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score/masked_self_attention_3/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score/masked_self_attention_3/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score/masked_self_attention_3/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score/masked_self_attention_3/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score/masked_self_attention_3/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score/masked_self_attention_4/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score/masked_self_attention_4/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score/masked_self_attention_4/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score/masked_self_attention_4/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score/masked_self_attention_4/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score/masked_self_attention_4/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score/masked_self_attention_4/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score/masked_self_attention_4/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score/masked_self_attention_4/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score/masked_self_attention_5/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score/masked_self_attention_5/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score/masked_self_attention_5/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score/masked_self_attention_5/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score/masked_self_attention_5/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score/masked_self_attention_5/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score/masked_self_attention_5/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score/masked_self_attention_5/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score/masked_self_attention_5/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score/masked_self_attention_6/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score/masked_self_attention_6/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score/masked_self_attention_6/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score/masked_self_attention_6/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score/masked_self_attention_6/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score/masked_self_attention_6/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score/masked_self_attention_6/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score/masked_self_attention_6/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score/masked_self_attention_6/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score/masked_self_attention_7/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score/masked_self_attention_7/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score/masked_self_attention_7/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score/masked_self_attention_7/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score/masked_self_attention_7/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score/masked_self_attention_7/add:0\", shape=(?, 5, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_attention_softmax Tensor(\"query_document_score/masked_self_attention_7/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score/masked_self_attention_7/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score/masked_self_attention_7/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score/masked_self_attention_8/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score/masked_self_attention_8/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score/masked_self_attention_8/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score/masked_self_attention_8/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score/masked_self_attention_8/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score/masked_self_attention_8/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score/masked_self_attention_8/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score/masked_self_attention_8/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score/masked_self_attention_8/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score/masked_self_attention_9/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score/masked_self_attention_9/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score/masked_self_attention_9/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score/masked_self_attention_9/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score/masked_self_attention_9/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score/masked_self_attention_9/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score/masked_self_attention_9/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score/masked_self_attention_9/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score/masked_self_attention_9/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score/masked_self_attention_10/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score/masked_self_attention_10/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score/masked_self_attention_10/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score/masked_self_attention_10/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score/masked_self_attention_10/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score/masked_self_attention_10/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score/masked_self_attention_10/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score/masked_self_attention_10/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score/masked_self_attention_10/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score/masked_self_attention_11/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score/masked_self_attention_11/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score/masked_self_attention_11/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score/masked_self_attention_11/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score/masked_self_attention_11/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score/masked_self_attention_11/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score/masked_self_attention_11/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score/masked_self_attention_11/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score/masked_self_attention_11/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score/masked_self_attention_12/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score/masked_self_attention_12/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score/masked_self_attention_12/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score/masked_self_attention_12/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score/masked_self_attention_12/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score/masked_self_attention_12/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score/masked_self_attention_12/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score/masked_self_attention_12/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score/masked_self_attention_12/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"query_document_score/term_gating_drmm_ffn/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/sub:0\", shape=(), dtype=float32)\n",
      "condition Tensor(\"query_document_score_1/masked_self_attention/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score_1/masked_self_attention/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score_1/masked_self_attention/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score_1/masked_self_attention/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score_1/masked_self_attention/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score_1/masked_self_attention/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score_1/masked_self_attention/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score_1/masked_self_attention/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score_1/masked_self_attention/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score_1/masked_self_attention_1/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score_1/masked_self_attention_1/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score_1/masked_self_attention_1/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score_1/masked_self_attention_1/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score_1/masked_self_attention_1/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score_1/masked_self_attention_1/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score_1/masked_self_attention_1/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score_1/masked_self_attention_1/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_attention_rep Tensor(\"query_document_score_1/masked_self_attention_1/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score_1/masked_self_attention_2/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score_1/masked_self_attention_2/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score_1/masked_self_attention_2/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score_1/masked_self_attention_2/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score_1/masked_self_attention_2/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score_1/masked_self_attention_2/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score_1/masked_self_attention_2/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score_1/masked_self_attention_2/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score_1/masked_self_attention_2/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score_1/masked_self_attention_3/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score_1/masked_self_attention_3/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score_1/masked_self_attention_3/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score_1/masked_self_attention_3/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score_1/masked_self_attention_3/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score_1/masked_self_attention_3/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score_1/masked_self_attention_3/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score_1/masked_self_attention_3/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score_1/masked_self_attention_3/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score_1/masked_self_attention_4/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score_1/masked_self_attention_4/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score_1/masked_self_attention_4/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score_1/masked_self_attention_4/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score_1/masked_self_attention_4/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score_1/masked_self_attention_4/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score_1/masked_self_attention_4/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score_1/masked_self_attention_4/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score_1/masked_self_attention_4/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score_1/masked_self_attention_5/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score_1/masked_self_attention_5/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score_1/masked_self_attention_5/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score_1/masked_self_attention_5/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score_1/masked_self_attention_5/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score_1/masked_self_attention_5/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score_1/masked_self_attention_5/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score_1/masked_self_attention_5/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score_1/masked_self_attention_5/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score_1/masked_self_attention_6/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score_1/masked_self_attention_6/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score_1/masked_self_attention_6/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score_1/masked_self_attention_6/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score_1/masked_self_attention_6/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score_1/masked_self_attention_6/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score_1/masked_self_attention_6/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score_1/masked_self_attention_6/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score_1/masked_self_attention_6/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score_1/masked_self_attention_7/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score_1/masked_self_attention_7/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score_1/masked_self_attention_7/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score_1/masked_self_attention_7/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score_1/masked_self_attention_7/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score_1/masked_self_attention_7/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score_1/masked_self_attention_7/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score_1/masked_self_attention_7/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score_1/masked_self_attention_7/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score_1/masked_self_attention_8/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score_1/masked_self_attention_8/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score_1/masked_self_attention_8/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score_1/masked_self_attention_8/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score_1/masked_self_attention_8/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score_1/masked_self_attention_8/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score_1/masked_self_attention_8/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score_1/masked_self_attention_8/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score_1/masked_self_attention_8/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score_1/masked_self_attention_9/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score_1/masked_self_attention_9/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score_1/masked_self_attention_9/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score_1/masked_self_attention_9/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score_1/masked_self_attention_9/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score_1/masked_self_attention_9/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score_1/masked_self_attention_9/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score_1/masked_self_attention_9/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score_1/masked_self_attention_9/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score_1/masked_self_attention_10/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score_1/masked_self_attention_10/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score_1/masked_self_attention_10/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score_1/masked_self_attention_10/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score_1/masked_self_attention_10/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score_1/masked_self_attention_10/add:0\", shape=(?, 5, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_attention_softmax Tensor(\"query_document_score_1/masked_self_attention_10/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score_1/masked_self_attention_10/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score_1/masked_self_attention_10/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score_1/masked_self_attention_11/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score_1/masked_self_attention_11/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score_1/masked_self_attention_11/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score_1/masked_self_attention_11/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score_1/masked_self_attention_11/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score_1/masked_self_attention_11/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score_1/masked_self_attention_11/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score_1/masked_self_attention_11/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score_1/masked_self_attention_11/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "condition Tensor(\"query_document_score_1/masked_self_attention_12/All:0\", shape=(?, 5, 1), dtype=bool)\n",
      "inv_condition Tensor(\"query_document_score_1/masked_self_attention_12/sub:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_projection Tensor(\"query_document_score_1/masked_self_attention_12/Reshape_2:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_tanh Tensor(\"query_document_score_1/masked_self_attention_12/Tanh:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention Tensor(\"query_document_score_1/masked_self_attention_12/Reshape_5:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_maked Tensor(\"query_document_score_1/masked_self_attention_12/add:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_attention_softmax Tensor(\"query_document_score_1/masked_self_attention_12/transpose_3:0\", shape=(?, 5, 1), dtype=float32)\n",
      "x_scored_emb Tensor(\"query_document_score_1/masked_self_attention_12/mul_1:0\", shape=(?, 5, 101), dtype=float32)\n",
      "x_attention_rep Tensor(\"query_document_score_1/masked_self_attention_12/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"query_document_score_1/term_gating_drmm_ffn/Sum:0\", shape=(?, 101), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dr_query_tokens (InputLayer)    (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_snippet_tokens (InputL (None, 13, 5, 15)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_snippet_position_token (None, 13, 5)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_snippet_tokens (InputL (None, 13, 5, 15)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_snippet_position_token (None, 13, 5)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "query_document_score (Model)    (None, 1)            858370604   dr_query_tokens[0][0]            \n",
      "                                                                 positive_snippet_tokens[0][0]    \n",
      "                                                                 positive_snippet_position_tokens[\n",
      "                                                                 dr_query_tokens[0][0]            \n",
      "                                                                 negative_snippet_tokens[0][0]    \n",
      "                                                                 negative_snippet_position_tokens[\n",
      "==================================================================================================\n",
      "Total params: 858,370,604\n",
      "Trainable params: 11,604\n",
      "Non-trainable params: 858,359,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "query_token_input = Input(shape=(MAX_Q_TERM,), name=\"dr_query_tokens\")\n",
    "positive_snippet_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), name = \"positive_snippet_tokens\")\n",
    "positive_snippet_position_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY), name = \"positive_snippet_position_tokens\")\n",
    "negative_snippet_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), name = \"negative_snippet_tokens\")\n",
    "negative_snippet_position_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY), name = \"negative_snippet_position_tokens\")\n",
    "\n",
    "positive_documents_score = document_score_model([query_token_input, positive_snippet_input, positive_snippet_position_input])\n",
    "negative_documents_score = document_score_model([query_token_input, negative_snippet_input, negative_snippet_position_input])\n",
    "\n",
    "#stack_socres = stack_scores_layer([positive_documents_score,negative_documents_score])\n",
    "\n",
    "\n",
    "\n",
    "#pairwise_loss_layer = Lambda(pairwise_hinge_loss, name=\"pairwise_hinge\")\n",
    "#pairwise_loss = pairwise_loss_layer([positive_documents_score,negative_documents_score])\n",
    "\n",
    "\n",
    "\n",
    "inputs = [query_token_input, positive_snippet_input, positive_snippet_position_input, negative_snippet_input, negative_snippet_position_input]\n",
    "\n",
    "deepRank_model = Model(inputs = inputs, outputs = [positive_documents_score, negative_documents_score], name=\"deep_rank\")\n",
    "\n",
    "\n",
    "p_loss = K.mean(K.maximum(0.0, 1.0 - positive_documents_score + negative_documents_score))\n",
    "\n",
    "deepRank_model.add_loss(p_loss)\n",
    "\n",
    "deepRank_model.summary() \n",
    "#m.predict([Q, Q_t1_passage, Q_t2_passage, Q_t3_passage, Q_t1_passage_pos, Q_t2_passage_pos, Q_t3_passage_pos])\n",
    "\n",
    "#deepRank_model.compile(loss=pairwise_hinge_loss, optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITE_GRAPH = False\n",
    "if WRITE_GRAPH:\n",
    "    from tensorflow.summary import FileWriter\n",
    "\n",
    "    graph = K.get_session().graph\n",
    "     # Your model implementation\n",
    "    #with graph.as_default():\n",
    "      # compile method actually creates the model in the graph.\n",
    "      #deepRank_model.compile(loss=identity_loss, optimizer='adam', metrics=['accuracy'])\n",
    "    writer = FileWriter(logdir='tensorboard/deepRank', graph=graph)\n",
    "    writer.flush()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_score.predict([Q, Q_t1_passage, Q_t2_passage, Q_t3_passage, Q_t1_passage_pos, Q_t2_passage_pos, Q_t3_passage_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open /backup/results/fast_method_relevant_results/train_data_deep_models_v2.tar.gz\n"
     ]
    }
   ],
   "source": [
    "path_dl_train = \"/backup/results/fast_method_relevant_results/train_data_deep_models_v2.tar.gz\"\n",
    "\n",
    "\n",
    "\n",
    "tar = tarfile.open(path_dl_train)\n",
    "#open\n",
    "print(\"Open\",path_dl_train)\n",
    "m = tar.getmembers()[0]\n",
    "f = tar.extractfile(m)\n",
    "train_articles_collection = pickle.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTILLY_POSITIVE_SAMPLES = 2\n",
    "NEGATIVE_SAMPLES = 3\n",
    "\n",
    "class TrainDataGenerator(object):\n",
    "    def __init__(self, article_collection, tokenizer, batch_queries_size):\n",
    "        \n",
    "        self.batch_size = batch_queries_size\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.train_data = article_collection[\"bioasq_data\"]\n",
    "        self.articles = article_collection[\"collection\"]\n",
    "        self.irrelevant_pmid = article_collection[\"irrelevant_pmid\"]\n",
    "        \n",
    "        self.num_steps = len(self.train_data)//self.batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        \n",
    "        query = []\n",
    "        query_positive_doc = []\n",
    "        query_positive_doc_position = []\n",
    "        query_negative_doc = []\n",
    "        query_negative_doc_position = []\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            #stop condition\n",
    "            if len(query)>=self.batch_size:\n",
    "                #missing fill the gap for the missing query_terms\n",
    "                query = np.array(query)\n",
    "                p=np.random.permutation(query.shape[0])\n",
    "                query = query[p]\n",
    "                query_positive_doc = np.array(query_positive_doc)[p]\n",
    "                query_positive_doc_position = np.array(query_positive_doc_position)[p]\n",
    "                query_negative_doc = np.array(query_negative_doc)[p]\n",
    "                query_negative_doc_position =  np.array(query_negative_doc_position)[p]\n",
    "                \n",
    "                X = [query, query_positive_doc, query_positive_doc_position, query_negative_doc, query_negative_doc_position]\n",
    "                #Y = [np.zeros((len(query))),np.zeros((len(query)))]\n",
    "                yield X\n",
    "\n",
    "\n",
    "                #reset\n",
    "                query = []\n",
    "                query_positive_doc = []\n",
    "                query_positive_doc_position = []\n",
    "                query_negative_doc = []\n",
    "                query_negative_doc_position = []\n",
    "            \n",
    "            #select a random question\n",
    "            random_query_index = random.randint(0, len(self.train_data)-1) \n",
    "            query_data = self.train_data[random_query_index]\n",
    "            \n",
    "            #list of partilly relevant documents\n",
    "            partilly_positive_pmid_docs = query_data[\"partilly_positive_pmid\"]\n",
    "\n",
    "            tokenized_query = query_data[\"query\"][:MAX_Q_TERM]\n",
    "            \n",
    "            for j in range(PARTILLY_POSITIVE_SAMPLES+NEGATIVE_SAMPLES):\n",
    "                #select a random positive\n",
    "                random_doc_index = random.randint(0, len(query_data[\"positive_pmid\"])-1) \n",
    "                doc_pmid = query_data[\"positive_pmid\"][random_doc_index]\n",
    "\n",
    "                tokenized_positive_doc = self.articles[doc_pmid]\n",
    "                positive_snippets, positive_snippets_position = self.__snippet_interaction(tokenized_query, tokenized_positive_doc)\n",
    "                \n",
    "                if j<PARTILLY_POSITIVE_SAMPLES:\n",
    "                    #select the partilly posivite doc\n",
    "                    random_ind = bisect(query_data[\"partially_positive_cumulative_prob\"],random.random())\n",
    "                    random_negative_doc_pmid = query_data[\"partilly_positive_pmid\"][random_ind]\n",
    "                    #print(self.__get_article(random_negative_doc_pmid))\n",
    "                    tokenized_negative_doc = self.articles[random_negative_doc_pmid]\n",
    "                    negative_snippets, negative_snippets_position = self.__snippet_interaction(tokenized_query, tokenized_negative_doc)\n",
    "                else:\n",
    "                    #select a random negative\n",
    "                    random_doc_index = random.randint(0, len(self.irrelevant_pmid)-1) \n",
    "                    doc_pmid = self.irrelevant_pmid[random_doc_index]\n",
    "                    \n",
    "                    tokenized_negative_doc = self.articles[doc_pmid]\n",
    "                    negative_snippets, negative_snippets_position = self.__snippet_interaction(tokenized_query, tokenized_negative_doc)\n",
    "                \n",
    "                \n",
    "                ### add ###\n",
    "\n",
    "                #not efficient\n",
    "                query.append(tokenized_query)\n",
    "\n",
    "                #positive doc\n",
    "                query_positive_doc.append(positive_snippets)\n",
    "                query_positive_doc_position.append(positive_snippets_position)\n",
    "\n",
    "                #negative doc\n",
    "                query_negative_doc.append(negative_snippets)\n",
    "                query_negative_doc_position.append(negative_snippets_position)\n",
    "            \n",
    "\n",
    "            \n",
    "    def __snippet_interaction(self, tokenized_query, tokenized_doc, snippet_length=QUERY_CENTRIC_CONTEX):\n",
    "        \n",
    "        snippets = []\n",
    "        snippets_position = [] \n",
    "\n",
    "        half_size = snippet_length//2\n",
    "        \n",
    "        #O(n^2) complexity, probably can do better with better data struct TODO see if is worthit\n",
    "        for query_token in tokenized_query:\n",
    "            \n",
    "            snippets_per_token = []\n",
    "            snippets_per_token_position = []\n",
    "            \n",
    "            if query_token != 0: #jump padded token\n",
    "            \n",
    "                for i,doc_token in enumerate(tokenized_doc):\n",
    "\n",
    "                    if doc_token==query_token:\n",
    "\n",
    "                        lower_index = i-half_size\n",
    "                        lower_index = max(0,lower_index)\n",
    "\n",
    "                        higher_index = i+half_size\n",
    "                        higher_index = min(len(tokenized_doc),higher_index)\n",
    "\n",
    "                        snippets_per_token.append(tokenized_doc[lower_index:higher_index])\n",
    "                        snippets_per_token_position.append(i)\n",
    "            \n",
    "            if len(snippets_per_token)==0:\n",
    "                snippets.append(np.zeros((MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), dtype=np.int32))\n",
    "                snippets_position.append(np.zeros((MAX_PASSAGES_PER_QUERY), dtype=np.int32)+SNIPPET_POSITION_PADDING_VALUE)\n",
    "                continue\n",
    "                \n",
    "            max_snippets_len = min(MAX_PASSAGES_PER_QUERY, len(snippets_per_token))\n",
    "            \n",
    "            ### snippets in matrix format\n",
    "            #pad\n",
    "            snippets_per_token = pad_sequences(snippets_per_token, maxlen = QUERY_CENTRIC_CONTEX, padding=\"post\")\n",
    "            #fill the gaps\n",
    "            _temp = np.zeros((MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), dtype=np.int32)\n",
    "            _temp[:max_snippets_len] = snippets_per_token[:max_snippets_len]\n",
    "            snippets.append(_temp)\n",
    "            \n",
    "            ### snippets_position in matrix format\n",
    "            #pad\n",
    "            snippets_per_token_position = pad_sequences([snippets_per_token_position], maxlen = MAX_PASSAGES_PER_QUERY, padding=\"post\", value=SNIPPET_POSITION_PADDING_VALUE)[0]\n",
    "            snippets_position.append(snippets_per_token_position)\n",
    "            \n",
    "        return snippets, snippets_position\n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test (validation) data generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open /backup/results/fast_method_relevant_results/test_data_deep_models_v2.tar.gz\n"
     ]
    }
   ],
   "source": [
    "path_dl_test = \"/backup/results/fast_method_relevant_results/test_data_deep_models_v2.tar.gz\"\n",
    "\n",
    "\n",
    "\n",
    "tar = tarfile.open(path_dl_test)\n",
    "#open\n",
    "print(\"Open\",path_dl_test)\n",
    "m = tar.getmembers()[0]\n",
    "f = tar.extractfile(m)\n",
    "test_articles_collection = pickle.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TestDataGenerator(object):\n",
    "    def __init__(self, article_collection, tokenizer):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.test_data = article_collection[\"bioasq_data\"] \n",
    "        self.articles = article_collection[\"collection\"]\n",
    "        \n",
    "        self.num_steps = len(self.test_data)\n",
    "        \n",
    "    \n",
    "    def __get_article(self, pmid):\n",
    "        return self.article_map(self.articles[pmid])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        \n",
    "        query = []\n",
    "        query_doc = []\n",
    "        query_doc_position = []\n",
    "        \n",
    "\n",
    "        for query_data in self.test_data:\n",
    "\n",
    "            #tokenized_query = self.tokenizer.texts_to_sequences([query_data[\"query\"]])[0]\n",
    "            tokenized_query = query_data[\"query\"][:MAX_Q_TERM]\n",
    "            #manualy remove the stopwords\n",
    "            #tokenized_query = [ token for token in tokenized_query if token not in biomedical_stop_words_tokens]\n",
    "\n",
    "            #tokenized_query = pad_sequences([tokenized_query], maxlen = MAX_Q_TERM, padding=\"post\")[0]\n",
    "\n",
    "            for doc_pmid in query_data[\"documents\"]:\n",
    "                #positive\n",
    "\n",
    "                tokenized_doc = self.articles[doc_pmid]\n",
    "                doc_snippets, doc_snippets_position = self.__snippet_interaction(tokenized_query, tokenized_doc)\n",
    "\n",
    "                ### add ###\n",
    "\n",
    "                query.append(tokenized_query)\n",
    "\n",
    "                #positive doc\n",
    "                query_doc.append(doc_snippets)\n",
    "                query_doc_position.append(doc_snippets_position)\n",
    "\n",
    "\n",
    "            #missing fill the gap for the missing query_terms\n",
    "\n",
    "            X = [np.array(query), np.array(query_doc), np.array(query_doc_position)]\n",
    "\n",
    "            yield X\n",
    "\n",
    "            #reset\n",
    "            query = []\n",
    "            query_doc = []\n",
    "            query_doc_position = []\n",
    "\n",
    "                \n",
    "    def __snippet_interaction(self, tokenized_query, tokenized_doc, snippet_length=QUERY_CENTRIC_CONTEX):\n",
    "        \n",
    "        snippets = []\n",
    "        snippets_position = [] \n",
    "\n",
    "        half_size = snippet_length//2\n",
    "        \n",
    "        #O(n^2) complexity, probably can do better with better data struct TODO see if is worthit\n",
    "        for query_token in tokenized_query:\n",
    "            \n",
    "            snippets_per_token = []\n",
    "            snippets_per_token_position = []\n",
    "            \n",
    "            if query_token != 0: #jump padded token\n",
    "                \n",
    "                for i,doc_token in enumerate(tokenized_doc):\n",
    "\n",
    "                    if doc_token==query_token:\n",
    "\n",
    "                        lower_index = i-half_size\n",
    "                        lower_index = max(0,lower_index)\n",
    "\n",
    "                        higher_index = i+half_size\n",
    "                        higher_index = min(len(tokenized_doc),higher_index)\n",
    "\n",
    "                        snippets_per_token.append(tokenized_doc[lower_index:higher_index])\n",
    "                        snippets_per_token_position.append(i)\n",
    "\n",
    "            if len(snippets_per_token)==0:\n",
    "                snippets.append(np.zeros((MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), dtype=np.int32))\n",
    "                snippets_position.append(np.zeros((MAX_PASSAGES_PER_QUERY), dtype=np.int32)+SNIPPET_POSITION_PADDING_VALUE)\n",
    "                continue\n",
    "                \n",
    "            max_snippets_len = min(MAX_PASSAGES_PER_QUERY, len(snippets_per_token))\n",
    "            \n",
    "            ### snippets in matrix format\n",
    "            #pad\n",
    "            snippets_per_token = pad_sequences(snippets_per_token, maxlen = QUERY_CENTRIC_CONTEX, padding=\"post\")\n",
    "            #fill the gaps\n",
    "            _temp = np.zeros((MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), dtype=np.int32)\n",
    "            _temp[:max_snippets_len] = snippets_per_token[:max_snippets_len]\n",
    "            snippets.append(_temp)\n",
    "            \n",
    "            ### snippets_position in matrix format\n",
    "            #pad\n",
    "            snippets_per_token_position = pad_sequences([snippets_per_token_position], maxlen = MAX_PASSAGES_PER_QUERY, padding=\"post\",value=SNIPPET_POSITION_PADDING_VALUE)[0]\n",
    "            snippets_position.append(snippets_per_token_position)\n",
    "            \n",
    "        return snippets, snippets_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549\n",
      "82\n",
      "validation size 82 test size 549\n"
     ]
    }
   ],
   "source": [
    "validation_articles_collection = {\"bioasq_data\":[],\"collection\":test_articles_collection[\"collection\"]}\n",
    "\n",
    "_temp = test_articles_collection[\"bioasq_data\"][:]\n",
    "random.shuffle(_temp)\n",
    "print(len(_temp))\n",
    "\n",
    "validation_percentage = 0.15\n",
    "\n",
    "split_index = int(len(_temp)*validation_percentage)\n",
    "print(split_index)\n",
    "\n",
    "validation_articles_collection[\"bioasq_data\"] = _temp[:split_index]\n",
    "#test_articles_collection[\"bioasq_data\"] = _temp[split_index:]\n",
    "\n",
    "print(\"validation size\",len(validation_articles_collection[\"bioasq_data\"]),\"test size\",len(test_articles_collection[\"bioasq_data\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_test_data(data):\n",
    "    data_generator = TestDataGenerator(data, tk)\n",
    "    data_generator = iter(data_generator)\n",
    "\n",
    "    query_results = {}\n",
    "\n",
    "    for i,X in enumerate(data_generator):\n",
    "        print(\"Predict query:\",i,end=\"\\r\")\n",
    "        deep_ranking = document_score_model.predict(X)\n",
    "        deep_ranking = map(lambda x:x[0],deep_ranking.tolist())\n",
    "        bm25_results = data[\"bioasq_data\"][i][\"documents\"]\n",
    "        deep_ranking_pmid = list(zip(bm25_results,deep_ranking))\n",
    "        deep_ranking_pmid.sort(key=lambda x:-x[1])\n",
    "        query_results[data[\"bioasq_data\"][i][\"id\"]] = {\"result\":deep_ranking_pmid,\"goldstandard\":data[\"bioasq_data\"][i][\"positive_pmid\"]}\n",
    "        #print(\"save query results:\",i,end=\"\\r\")\n",
    "        \n",
    "    return query_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate score\n",
    "\n",
    "def validation_score(deep_rank_test_query_results):\n",
    "    id_to_remove = []\n",
    "    for k,v in deep_rank_test_query_results.items():\n",
    "        if len(v[\"goldstandard\"]) == 0:\n",
    "            id_to_remove.append(k)\n",
    "\n",
    "    for k in id_to_remove:\n",
    "        del deep_rank_test_query_results[k]\n",
    "\n",
    "    deep_rank_test_query_results = list(deep_rank_test_query_results.values())\n",
    "\n",
    "    print(\"TEST set, len \",len(deep_rank_test_query_results))\n",
    "\n",
    "    expectations = list(map(lambda x:x[\"goldstandard\"],deep_rank_test_query_results))\n",
    "    predictions = list(map(lambda x:x[\"result\"],deep_rank_test_query_results))\n",
    "\n",
    "    #print(\"Recall:\",f_recall(predictions,expectations,at=1000))\n",
    "    bioasq_map = f_map(predictions,expectations,bioASQ=True)\n",
    "    print(\"MAP @10 bioASQ:\", bioasq_map)\n",
    "    print(\"MAP @25:\",f_map(predictions,expectations, bioASQ=True, at=25))\n",
    "    print(\"MAP @50:\",f_map(predictions,expectations, bioASQ=True, at=50))\n",
    "    print(\"MAP @100:\",f_map(predictions,expectations, bioASQ=True, at=100))\n",
    "    print(\"MAP @200:\",f_map(predictions,expectations, bioASQ=True, at=200))\n",
    "    print(\"MAP @300:\",f_map(predictions,expectations, bioASQ=True, at=300))\n",
    "\n",
    "    print(\"MAP:\",f_map(predictions,expectations, use_len=True))\n",
    "    \n",
    "    print(\"RECALL@10:\",f_recall(predictions,expectations, at=10))\n",
    "    print(\"RECALL@50:\",f_recall(predictions,expectations, at=50))\n",
    "    print(\"RECALL@100:\",f_recall(predictions,expectations, at=100))\n",
    "    return bioasq_map, predictions, expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"query_document_score\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"query_document_score\" during training.\n",
      "WARNING:tensorflow:Output \"query_document_score\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"query_document_score\" during training.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam,  Adadelta\n",
    "\n",
    "#sgd = SGD(lr=0.001)\n",
    "#adam = Adam(lr=0.001)\n",
    "adadelta = Adadelta(lr=2)\n",
    "\n",
    "deepRank_model.compile( optimizer=adadelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict query: 81.0042224 | current max loss: 1.0042224 | current min loss: 1.0042224 | time: 18.907275199890137\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.10538359788359788\n",
      "MAP @25: 0.15268850642525625\n",
      "MAP @50: 0.1783911116589518\n",
      "MAP @100: 0.19325853405067792\n",
      "MAP @200: 0.2014326330889278\n",
      "MAP @300: 0.2044138350813475\n",
      "MAP: 0.13926969239509776\n",
      "RECALL@10: 0.22382204985215787\n",
      "RECALL@50: 0.41523435245251805\n",
      "RECALL@100: 0.5240132376733841\n",
      "Epoach: 0 | avg loss: 0.9125016 | max loss: 1.0042224 | min loss: 0.80067205\n",
      "Epoach: 1 | avg loss: 0.49317193 | max loss: 0.72874445 | min loss: 0.36090243.36090243 | time: 7.26887869834899975\n",
      "Epoach: 2 | avg loss: 0.28394535 | max loss: 0.30443656 | min loss: 0.22263007.22263007 | time: 7.20447111129760785\n",
      "Epoach: 3 | avg loss: 0.27021748 | max loss: 0.3453509 | min loss: 0.219425420.21942542 | time: 7.181874990463257\n",
      "Epoach: 4 | avg loss: 0.22671619 | max loss: 0.29153928 | min loss: 0.179436130.17943613 | time: 7.328411817550659\n",
      "Epoach: 5 | avg loss: 0.20183098 | max loss: 0.24140441 | min loss: 0.1585882515858825 | time: 7.28836703300476142\n",
      "Epoach: 6 | avg loss: 0.17883258 | max loss: 0.21680002 | min loss: 0.145684020.14568402 | time: 7.184903144836426\n",
      "Epoach: 7 | avg loss: 0.17802173 | max loss: 0.21380427 | min loss: 0.1308456 0.1308456 | time: 7.26121187210083755\n",
      "Epoach: 8 | avg loss: 0.16129185 | max loss: 0.18449171 | min loss: 0.143538580.14353858 | time: 7.214230775833135\n",
      "Epoach: 9 | avg loss: 0.17940602 | max loss: 0.2193988 | min loss: 0.139448180.13944818 | time: 7.35244369506835945\n",
      "Step: 7 | loss: 0.13796973 | current max loss: 0.19820029 | current min loss: 0.13796973 | time: 7.414699077606201\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.13848765432098764\n",
      "MAP @25: 0.21805702905989804\n",
      "MAP @50: 0.26683361251990434\n",
      "MAP @100: 0.2891122425549287\n",
      "MAP @200: 0.2990682478903359\n",
      "MAP @300: 0.3034287674776254\n",
      "MAP: 0.2138829599334964\n",
      "RECALL@10: 0.31521366909260395\n",
      "RECALL@50: 0.5639415054155896\n",
      "RECALL@100: 0.670907010858438\n",
      "Epoach: 10 | avg loss: 0.17154312 | max loss: 0.19820029 | min loss: 0.13796973\n",
      "Epoach: 11 | avg loss: 0.14994204 | max loss: 0.21805936 | min loss: 0.08608124.08608124 | time: 7.2892277240753175\n",
      "Epoach: 12 | avg loss: 0.15823033 | max loss: 0.19361591 | min loss: 0.11516501.11516501 | time: 7.312676668167114\n",
      "Epoach: 13 | avg loss: 0.14865342 | max loss: 0.18927157 | min loss: 0.08867377.08867377 | time: 7.149423360824585595\n",
      "Epoach: 14 | avg loss: 0.14407511 | max loss: 0.17260082 | min loss: 0.10460846.10460846 | time: 7.3983585834503175\n",
      "Epoach: 15 | avg loss: 0.1294682 | max loss: 0.16858634 | min loss: 0.094186130.09418613 | time: 7.227109193801881545\n",
      "Epoach: 16 | avg loss: 0.15847948 | max loss: 0.1836209 | min loss: 0.12167572.12167572 | time: 7.171879053115845\n",
      "Epoach: 17 | avg loss: 0.1343056 | max loss: 0.19023322 | min loss: 0.091756366.091756366 | time: 7.3176646232604985\n",
      "Epoach: 18 | avg loss: 0.1224708 | max loss: 0.17449825 | min loss: 0.09055804.09055804 | time: 7.277674436569214945\n",
      "Epoach: 19 | avg loss: 0.1560771 | max loss: 0.23402852 | min loss: 0.093416140.09341614 | time: 7.2064068317413338605\n",
      "Step: 7 | loss: 0.16564412 | current max loss: 0.16564412 | current min loss: 0.11334045 | time: 7.1549603939056415\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.14938810503625316\n",
      "MAP @25: 0.23147504898355428\n",
      "MAP @50: 0.28843859894064394\n",
      "MAP @100: 0.30978129254248044\n",
      "MAP @200: 0.32013804900707904\n",
      "MAP @300: 0.3252427433558781\n",
      "MAP: 0.23115506947332232\n",
      "RECALL@10: 0.33706523308063197\n",
      "RECALL@50: 0.6084816031256814\n",
      "RECALL@100: 0.7109939385716868\n",
      "Epoach: 20 | avg loss: 0.14304489 | max loss: 0.16564412 | min loss: 0.11334045\n",
      "Epoach: 21 | avg loss: 0.13640939 | max loss: 0.16505818 | min loss: 0.11930085.11930085 | time: 7.2430422306060795\n",
      "Epoach: 22 | avg loss: 0.12535563 | max loss: 0.17411429 | min loss: 0.095259696095259696 | time: 7.205635309219366955\n",
      "Epoach: 23 | avg loss: 0.16198543 | max loss: 0.21013002 | min loss: 0.14260782.14260782 | time: 7.3402369022369385\n",
      "Epoach: 24 | avg loss: 0.14705326 | max loss: 0.21715309 | min loss: 0.096290685096290685 | time: 7.3357381820678715\n",
      "Epoach: 25 | avg loss: 0.11794478 | max loss: 0.15313746 | min loss: 0.08104662.08104662 | time: 7.3619906902313236\n",
      "Epoach: 26 | avg loss: 0.14347588 | max loss: 0.2373665 | min loss: 0.109697916109697916 | time: 7.2513625621795657\n",
      "Epoach: 27 | avg loss: 0.1355557 | max loss: 0.18860565 | min loss: 0.101668596.101668596 | time: 7.27151632308967175\n",
      "Epoach: 28 | avg loss: 0.13470176 | max loss: 0.18260312 | min loss: 0.1019487810194878 | time: 7.305899381637573939\n",
      "Epoach: 29 | avg loss: 0.14155304 | max loss: 0.20119151 | min loss: 0.09361894.09361894 | time: 7.2803895473480225\n",
      "Step: 7 | loss: 0.0949727 | current max loss: 0.19834858 | current min loss: 0.0949727 | time: 7.15578317642211997852\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.15306829316088572\n",
      "MAP @25: 0.2358023663593055\n",
      "MAP @50: 0.29495942376672\n",
      "MAP @100: 0.31753922846620775\n",
      "MAP @200: 0.3273482126021353\n",
      "MAP @300: 0.3327947860725136\n",
      "MAP: 0.2283968680163513\n",
      "RECALL@10: 0.35266879961012443\n",
      "RECALL@50: 0.6381910100130662\n",
      "RECALL@100: 0.7341076211370697\n",
      "Epoach: 30 | avg loss: 0.14155135 | max loss: 0.19834858 | min loss: 0.0949727\n",
      "Epoach: 31 | avg loss: 0.13417517 | max loss: 0.15330367 | min loss: 0.11480938.11480938 | time: 7.23323035240173324\n",
      "Epoach: 32 | avg loss: 0.12195169 | max loss: 0.16342108 | min loss: 0.08785638.08785638 | time: 7.2007613182067877\n",
      "Epoach: 33 | avg loss: 0.13282719 | max loss: 0.21276896 | min loss: 0.1014327510143275 | time: 7.2366800308227542511\n",
      "Epoach: 34 | avg loss: 0.118202 | max loss: 0.15498024 | min loss: 0.07243498: 0.07243498 | time: 7.2773785591125491\n",
      "Epoach: 35 | avg loss: 0.13550355 | max loss: 0.2100889 | min loss: 0.08893999588939995 | time: 7.17787504196167579\n",
      "Epoach: 36 | avg loss: 0.12728623 | max loss: 0.18325587 | min loss: 0.077848144077848144 | time: 7.2424316406250427\n",
      "Epoach: 37 | avg loss: 0.14154685 | max loss: 0.20354104 | min loss: 0.114484705114484705 | time: 7.1965279579162677\n",
      "Epoach: 38 | avg loss: 0.12396614 | max loss: 0.20121771 | min loss: 0.08114089.08114089 | time: 7.2359035015106275\n",
      "Epoach: 39 | avg loss: 0.13002037 | max loss: 0.17463444 | min loss: 0.1043766 0.1043766 | time: 7.28255438804626555\n",
      "Step: 7 | loss: 0.12713407 | current max loss: 0.18186021 | current min loss: 0.09330936 | time: 7.149534463882446964\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.1613095238095238\n",
      "MAP @25: 0.23953925857015732\n",
      "MAP @50: 0.3038099068099342\n",
      "MAP @100: 0.32528817271067156\n",
      "MAP @200: 0.3357548856916214\n",
      "MAP @300: 0.340988392054484\n",
      "MAP: 0.237920789142706\n",
      "RECALL@10: 0.3615814245051936\n",
      "RECALL@50: 0.6559265049707834\n",
      "RECALL@100: 0.7430463102287378\n",
      "Epoach: 40 | avg loss: 0.12251069 | max loss: 0.18186021 | min loss: 0.09330936\n",
      "Epoach: 41 | avg loss: 0.13849252 | max loss: 0.18673538 | min loss: 0.09400418.09400418 | time: 7.2914767265319824\n",
      "Epoach: 42 | avg loss: 0.13959584 | max loss: 0.18608947 | min loss: 0.09049299.09049299 | time: 7.1336636543273933\n",
      "Epoach: 43 | avg loss: 0.11447646 | max loss: 0.16573317 | min loss: 0.081478.081478 | time: 7.32727241516113380037\n",
      "Epoach: 44 | avg loss: 0.1403347 | max loss: 0.15892221 | min loss: 0.101163350.10116335 | time: 7.359982728958136\n",
      "Epoach: 45 | avg loss: 0.123007774 | max loss: 0.2004541 | min loss: 0.086272344086272344 | time: 7.166740417480469\n",
      "Epoach: 46 | avg loss: 0.14596887 | max loss: 0.22014281 | min loss: 0.112008944112008944 | time: 7.1658122539520266\n",
      "Epoach: 47 | avg loss: 0.1371493 | max loss: 0.17249142 | min loss: 0.114284820.11428482 | time: 7.2562010288238525\n",
      "Epoach: 48 | avg loss: 0.11318117 | max loss: 0.14699222 | min loss: 0.08392244.08392244 | time: 7.31665062904357915\n",
      "Epoach: 49 | avg loss: 0.12978484 | max loss: 0.17627062 | min loss: 0.096893795.096893795 | time: 7.162693262100224\n",
      "Step: 7 | loss: 0.08817414 | current max loss: 0.16358064 | current min loss: 0.08817414 | time: 7.15329551696777367\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.1643146188516559\n",
      "MAP @25: 0.24593423271982648\n",
      "MAP @50: 0.3102104840135773\n",
      "MAP @100: 0.33230635925296176\n",
      "MAP @200: 0.3427814665032334\n",
      "MAP @300: 0.3479579308877386\n",
      "MAP: 0.24198076876520164\n",
      "RECALL@10: 0.36171367933391446\n",
      "RECALL@50: 0.6519962345903164\n",
      "RECALL@100: 0.7542368186782593\n",
      "Epoach: 50 | avg loss: 0.12024775 | max loss: 0.16358064 | min loss: 0.08817414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoach: 51 | avg loss: 0.12840645 | max loss: 0.23955789 | min loss: 0.054118264054118264 | time: 7.36850380897522358\n",
      "Epoach: 52 | avg loss: 0.14068726 | max loss: 0.19548008 | min loss: 0.104195386104195386 | time: 7.3913023471832275\n",
      "Epoach: 53 | avg loss: 0.11976034 | max loss: 0.14174968 | min loss: 0.061712284.061712284 | time: 7.2530763149261475\n",
      "Epoach: 54 | avg loss: 0.14333203 | max loss: 0.18659239 | min loss: 0.1015752110157521 | time: 7.2962560653686526\n",
      "Epoach: 55 | avg loss: 0.13084026 | max loss: 0.1636992 | min loss: 0.09555291.09555291 | time: 7.1642107963562011\n",
      "Epoach: 56 | avg loss: 0.12226493 | max loss: 0.1592394 | min loss: 0.080581546080581546 | time: 7.2912206649780273\n",
      "Epoach: 57 | avg loss: 0.12206644 | max loss: 0.16975245 | min loss: 0.068161584068161584 | time: 7.18751573562622198\n",
      "Epoach: 58 | avg loss: 0.12992905 | max loss: 0.14667211 | min loss: 0.1023995210239952 | time: 7.2712779045104989\n",
      "Epoach: 59 | avg loss: 0.13541949 | max loss: 0.16244505 | min loss: 0.11245321.11245321 | time: 7.16136670112609954\n",
      "Step: 7 | loss: 0.15698498 | current max loss: 0.16079853 | current min loss: 0.12120643 | time: 7.4757192134857186\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.16457280031354102\n",
      "MAP @25: 0.2492321032561619\n",
      "MAP @50: 0.3159553933596846\n",
      "MAP @100: 0.336264737842954\n",
      "MAP @200: 0.34701345308870346\n",
      "MAP @300: 0.35168472876593976\n",
      "MAP: 0.24156250851654085\n",
      "RECALL@10: 0.36600471642757126\n",
      "RECALL@50: 0.6579597931781442\n",
      "RECALL@100: 0.7589353974494685\n",
      "Epoach: 60 | avg loss: 0.13597706 | max loss: 0.16079853 | min loss: 0.12120643\n",
      "Epoach: 61 | avg loss: 0.11440115 | max loss: 0.13571069 | min loss: 0.097513855097513855 | time: 7.16358876228332545\n",
      "Epoach: 62 | avg loss: 0.11146161 | max loss: 0.1863055 | min loss: 0.061214484061214484 | time: 7.2249200344085693\n",
      "Epoach: 63 | avg loss: 0.12345019 | max loss: 0.16765341 | min loss: 0.092238950.09223895 | time: 7.247416019439697\n",
      "Epoach: 64 | avg loss: 0.12755573 | max loss: 0.19000028 | min loss: 0.07788189.07788189 | time: 7.145911455154419\n",
      "Epoach: 65 | avg loss: 0.14336658 | max loss: 0.17931573 | min loss: 0.11264965.11264965 | time: 7.2433104515075685\n",
      "Epoach: 66 | avg loss: 0.139204 | max loss: 0.17616737 | min loss: 0.11513187 0.11513187 | time: 7.2528200149536136\n",
      "Epoach: 67 | avg loss: 0.123201534 | max loss: 0.16898859 | min loss: 0.0921602209216022 | time: 7.208160877227783555\n",
      "Epoach: 68 | avg loss: 0.13628274 | max loss: 0.16136146 | min loss: 0.1049896810498968 | time: 7.26170039176940935\n",
      "Epoach: 69 | avg loss: 0.1345881 | max loss: 0.1670266 | min loss: 0.096639140.09663914 | time: 7.2773690223693853\n",
      "Step: 7 | loss: 0.115582444 | current max loss: 0.1827197 | current min loss: 0.09472344 | time: 7.337325811386108\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.17336517734665882\n",
      "MAP @25: 0.26805107114744653\n",
      "MAP @50: 0.33273873386647285\n",
      "MAP @100: 0.3524169066156568\n",
      "MAP @200: 0.3629908833120665\n",
      "MAP @300: 0.36815838683158947\n",
      "MAP: 0.24771043493616546\n",
      "RECALL@10: 0.37630904101944407\n",
      "RECALL@50: 0.6692149574199397\n",
      "RECALL@100: 0.7647917185672273\n",
      "\n",
      "Run for the test set\n",
      "TEST set, len  542\n",
      "MAP @10 bioASQ: 0.15782192643355003\n",
      "MAP @25: 0.22792181150291288\n",
      "MAP @50: 0.26919149118783053\n",
      "MAP @100: 0.289548409148247\n",
      "MAP @200: 0.2983801819770337\n",
      "MAP @300: 0.3016172626865656\n",
      "MAP: 0.27438177410612613\n",
      "RECALL@10: 0.3919045955506571\n",
      "RECALL@50: 0.6726866010422436\n",
      "RECALL@100: 0.7669971911766049\n",
      "Epoach: 70 | avg loss: 0.13168631 | max loss: 0.1827197 | min loss: 0.09472344\n",
      "Epoach: 71 | avg loss: 0.13559842 | max loss: 0.2112328 | min loss: 0.061316475061316475 | time: 7.27663826942443851\n",
      "Epoach: 72 | avg loss: 0.113088205 | max loss: 0.14079547 | min loss: 0.0718549807185498 | time: 7.1466455459594734\n",
      "Epoach: 73 | avg loss: 0.11908828 | max loss: 0.1632543 | min loss: 0.0918213309182133 | time: 7.18457722663879402709\n",
      "Epoach: 74 | avg loss: 0.1339307 | max loss: 0.21749558 | min loss: 0.0698912740.069891274 | time: 7.182408094406128\n",
      "Epoach: 75 | avg loss: 0.11574526 | max loss: 0.15864204 | min loss: 0.083352044083352044 | time: 7.3618166446685794\n",
      "Epoach: 76 | avg loss: 0.12770608 | max loss: 0.18351571 | min loss: 0.0715709607157096 | time: 7.3097026348114016\n",
      "Epoach: 77 | avg loss: 0.12679563 | max loss: 0.18082902 | min loss: 0.07872251.07872251 | time: 7.155915260314941\n",
      "Epoach: 78 | avg loss: 0.117759705 | max loss: 0.15956298 | min loss: 0.080052234080052234 | time: 7.333746433258057\n",
      "Epoach: 79 | avg loss: 0.11549167 | max loss: 0.13695568 | min loss: 0.10073331.10073331 | time: 7.31000137329101651\n",
      "Step: 7 | loss: 0.1475746 | current max loss: 0.17734766 | current min loss: 0.09957172 | time: 7.041733503341675354\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.1749514991181658\n",
      "MAP @25: 0.26842617817462777\n",
      "MAP @50: 0.3337965955140371\n",
      "MAP @100: 0.3532953639252208\n",
      "MAP @200: 0.36407878012023587\n",
      "MAP @300: 0.3692383206547756\n",
      "MAP: 0.2504284713713804\n",
      "RECALL@10: 0.3873874413571035\n",
      "RECALL@50: 0.6711661267274311\n",
      "RECALL@100: 0.7652742866779325\n",
      "\n",
      "Run for the test set\n",
      "TEST set, len  542\n",
      "MAP @10 bioASQ: 0.15880498447841618\n",
      "MAP @25: 0.22868956028074536\n",
      "MAP @50: 0.2708445298382649\n",
      "MAP @100: 0.2909245459080687\n",
      "MAP @200: 0.2997859717252131\n",
      "MAP @300: 0.30307516992204137\n",
      "MAP: 0.275901465776052\n",
      "RECALL@10: 0.3922446879226974\n",
      "RECALL@50: 0.675784216152063\n",
      "RECALL@100: 0.7712119303275891\n",
      "Epoach: 80 | avg loss: 0.13038842 | max loss: 0.17734766 | min loss: 0.09957172\n",
      "Epoach: 81 | avg loss: 0.13132675 | max loss: 0.1911749 | min loss: 0.073122166073122166 | time: 7.2100250720977787\n",
      "Epoach: 82 | avg loss: 0.13073713 | max loss: 0.16726801 | min loss: 0.10106676.10106676 | time: 7.2767579555511475\n",
      "Epoach: 83 | avg loss: 0.1325604 | max loss: 0.169622 | min loss: 0.095894344.095894344 | time: 7.226557016372681958\n",
      "Epoach: 84 | avg loss: 0.1449853 | max loss: 0.1935093 | min loss: 0.10641447.10641447 | time: 7.27042508125305298\n",
      "Epoach: 85 | avg loss: 0.1231386 | max loss: 0.17995945 | min loss: 0.087629296.087629296 | time: 7.1082403659820569\n",
      "Epoach: 86 | avg loss: 0.12449381 | max loss: 0.15955055 | min loss: 0.090039834090039834 | time: 7.1912162303924564\n",
      "Epoach: 87 | avg loss: 0.12904318 | max loss: 0.169639 | min loss: 0.091004804091004804 | time: 7.4999349117279058\n",
      "Epoach: 88 | avg loss: 0.11352096 | max loss: 0.2033876 | min loss: 0.05610793.05610793 | time: 7.19042015075683685\n",
      "Epoach: 89 | avg loss: 0.09983999 | max loss: 0.12355232 | min loss: 0.06331807.06331807 | time: 7.2659566402435315\n",
      "Step: 7 | loss: 0.13360478 | current max loss: 0.14776096 | current min loss: 0.08469281 | time: 7.3966729640960694\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.1703880070546737\n",
      "MAP @25: 0.26635147449700913\n",
      "MAP @50: 0.3304672714953765\n",
      "MAP @100: 0.3502854601662446\n",
      "MAP @200: 0.36126563053267685\n",
      "MAP @300: 0.36642203313154825\n",
      "MAP: 0.24642910103074928\n",
      "RECALL@10: 0.38403602434144474\n",
      "RECALL@50: 0.6702402008015051\n",
      "RECALL@100: 0.7638117323698499\n",
      "Epoach: 90 | avg loss: 0.118686914 | max loss: 0.14776096 | min loss: 0.08469281\n",
      "Epoach: 91 | avg loss: 0.13386112 | max loss: 0.16763479 | min loss: 0.099743865099743865 | time: 7.37706422805786119\n",
      "Epoach: 92 | avg loss: 0.1225351 | max loss: 0.18580732 | min loss: 0.071352030.07135203 | time: 7.4048533439636234\n",
      "Epoach: 93 | avg loss: 0.13260387 | max loss: 0.15698902 | min loss: 0.09888101.09888101 | time: 7.3830165863037115\n",
      "Epoach: 94 | avg loss: 0.12662022 | max loss: 0.1549881 | min loss: 0.101264544101264544 | time: 7.3896706104278564\n",
      "Epoach: 95 | avg loss: 0.12595809 | max loss: 0.14741984 | min loss: 0.090201646090201646 | time: 7.2925035953521736\n",
      "Epoach: 96 | avg loss: 0.120436504 | max loss: 0.1749489 | min loss: 0.0824854152485415 | time: 7.28966999053955161565\n",
      "Epoach: 97 | avg loss: 0.11668563 | max loss: 0.14204536 | min loss: 0.09516583495165834 | time: 7.262192487716675774\n",
      "Epoach: 98 | avg loss: 0.14738703 | max loss: 0.22028753 | min loss: 0.08594997.08594997 | time: 7.242191314697266\n",
      "Epoach: 99 | avg loss: 0.13169128 | max loss: 0.19685341 | min loss: 0.09337724.09337724 | time: 7.2958345413208014\n",
      "Step: 7 | loss: 0.1101003 | current max loss: 0.15753786 | current min loss: 0.07659926 | time: 7.23682522773742735\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.1709705075445816\n",
      "MAP @25: 0.2686048616879495\n",
      "MAP @50: 0.3332439235567237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP @100: 0.3530563247790597\n",
      "MAP @200: 0.3635979929695319\n",
      "MAP @300: 0.36880648712845965\n",
      "MAP: 0.24753113343438837\n",
      "RECALL@10: 0.38414152096396736\n",
      "RECALL@50: 0.6703620086333401\n",
      "RECALL@100: 0.7665870032610629\n",
      "Epoach: 100 | avg loss: 0.11215229 | max loss: 0.15753786 | min loss: 0.07659926\n",
      "Epoach: 101 | avg loss: 0.111854926 | max loss: 0.13560629 | min loss: 0.0813896661389666 | time: 7.21495342254638793\n",
      "Epoach: 102 | avg loss: 0.12348192 | max loss: 0.17242576 | min loss: 0.089476498947649 | time: 7.232814311981201589\n",
      "Epoach: 103 | avg loss: 0.13245863 | max loss: 0.17178094 | min loss: 0.116066641606664 | time: 7.2380185127258324\n",
      "Epoach: 104 | avg loss: 0.14003517 | max loss: 0.19332692 | min loss: 0.09358653493586534 | time: 7.2809455394744871\n",
      "Epoach: 105 | avg loss: 0.1171598 | max loss: 0.16716018 | min loss: 0.072916925.072916925 | time: 7.149568557739258\n",
      "Epoach: 106 | avg loss: 0.10621229 | max loss: 0.16023287 | min loss: 0.0686478606864786 | time: 7.164447069168091\n",
      "Epoach: 107 | avg loss: 0.11307475 | max loss: 0.15092917 | min loss: 0.08166652.08166652 | time: 7.337155342102051\n",
      "Epoach: 108 | avg loss: 0.12866719 | max loss: 0.17475227 | min loss: 0.08708335587083355 | time: 7.29993462562561555\n",
      "Epoach: 109 | avg loss: 0.113989905 | max loss: 0.18569195 | min loss: 0.0707639307076393 | time: 7.3498919010162355\n",
      "Step: 7 | loss: 0.097429104 | current max loss: 0.13771963 | current min loss: 0.097429104 | time: 7.2044262886047361\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.17047717029198511\n",
      "MAP @25: 0.267550768300964\n",
      "MAP @50: 0.330360573375772\n",
      "MAP @100: 0.352005829205955\n",
      "MAP @200: 0.36200318036054485\n",
      "MAP @300: 0.36687981036782624\n",
      "MAP: 0.2506476244096526\n",
      "RECALL@10: 0.3861991341326917\n",
      "RECALL@50: 0.6703331645620982\n",
      "RECALL@100: 0.7687892883618985\n",
      "Epoach: 110 | avg loss: 0.11758676 | max loss: 0.13771963 | min loss: 0.097429104\n",
      "Epoach: 111 | avg loss: 0.1223331 | max loss: 0.19985129 | min loss: 0.06259223.06259223 | time: 7.288471698760986\n",
      "Epoach: 112 | avg loss: 0.10697573 | max loss: 0.13267756 | min loss: 0.069434716943471 | time: 7.24559187889099173\n",
      "Epoach: 113 | avg loss: 0.123114064 | max loss: 0.16955033 | min loss: 0.0807559040755904 | time: 7.2184584140777592\n",
      "Epoach: 114 | avg loss: 0.11844236 | max loss: 0.1574506 | min loss: 0.0799451307994513 | time: 7.22785234451293955\n",
      "Epoach: 115 | avg loss: 0.10949586 | max loss: 0.18929581 | min loss: 0.0701660807016608 | time: 7.280533790588379405\n",
      "Epoach: 116 | avg loss: 0.12086545 | max loss: 0.15624791 | min loss: 0.09946294.09946294 | time: 7.299308776855469\n",
      "Epoach: 117 | avg loss: 0.11320803 | max loss: 0.14736357 | min loss: 0.0741631607416316 | time: 6.96538400650024423\n",
      "Epoach: 118 | avg loss: 0.120623484 | max loss: 0.19713645 | min loss: 0.080766598076659 | time: 7.3359737396240232\n",
      "Epoach: 119 | avg loss: 0.12111886 | max loss: 0.14347509 | min loss: 0.0935353709353537 | time: 7.3008763790130615\n",
      "Step: 7 | loss: 0.15406655 | current max loss: 0.15406655 | current min loss: 0.072138995 | time: 7.3216869831085205\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.17591269841269846\n",
      "MAP @25: 0.2735893709723123\n",
      "MAP @50: 0.33746507473845055\n",
      "MAP @100: 0.3596669022977783\n",
      "MAP @200: 0.36963622990540523\n",
      "MAP @300: 0.37400506851527177\n",
      "MAP: 0.25440470610000404\n",
      "RECALL@10: 0.39031358107121267\n",
      "RECALL@50: 0.6798924813769324\n",
      "RECALL@100: 0.7738490817990719\n",
      "\n",
      "Run for the test set\n",
      "TEST set, len  542\n",
      "MAP @10 bioASQ: 0.1614991507057926\n",
      "MAP @25: 0.23298677200462994\n",
      "MAP @50: 0.27456449138806016\n",
      "MAP @100: 0.29540617108905387\n",
      "MAP @200: 0.30406502532902824\n",
      "MAP @300: 0.3072650632330773\n",
      "MAP: 0.2801333955368785\n",
      "RECALL@10: 0.40121147802365026\n",
      "RECALL@50: 0.6813916919168387\n",
      "RECALL@100: 0.781787529890119\n",
      "Epoach: 120 | avg loss: 0.109308794 | max loss: 0.15406655 | min loss: 0.072138995\n",
      "Epoach: 121 | avg loss: 0.12260577 | max loss: 0.16768783 | min loss: 0.1031397610313976 | time: 7.27727675437927252\n",
      "Epoach: 122 | avg loss: 0.12293301 | max loss: 0.16877489 | min loss: 0.0915998609159986 | time: 7.314895868301392544\n",
      "Epoach: 123 | avg loss: 0.126827 | max loss: 0.15773126 | min loss: 0.095549360.09554936 | time: 7.18324589729309185\n",
      "Epoach: 124 | avg loss: 0.115047805 | max loss: 0.16813481 | min loss: 0.078836777883677 | time: 7.1987066268920965\n",
      "Epoach: 125 | avg loss: 0.121839166 | max loss: 0.18360202 | min loss: 0.05921337859213378 | time: 7.103651285171509\n",
      "Epoach: 126 | avg loss: 0.12293114 | max loss: 0.1826544 | min loss: 0.0762528507625285 | time: 7.2773101329803472\n",
      "Epoach: 127 | avg loss: 0.12642208 | max loss: 0.16227275 | min loss: 0.08599813.08599813 | time: 6.9433057308197026\n",
      "Epoach: 128 | avg loss: 0.11752176 | max loss: 0.15461151 | min loss: 0.0899822908998229 | time: 7.206106901168823326\n",
      "Epoach: 129 | avg loss: 0.1406187 | max loss: 0.21687658 | min loss: 0.086261430.08626143 | time: 7.154012441635132\n",
      "Step: 7 | loss: 0.12647486 | current max loss: 0.13444234 | current min loss: 0.08959322 | time: 7.1170551776885995385\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.1748324514991182\n",
      "MAP @25: 0.272941813195814\n",
      "MAP @50: 0.334768667666217\n",
      "MAP @100: 0.3587263823162961\n",
      "MAP @200: 0.36856763169489093\n",
      "MAP @300: 0.3730728168673721\n",
      "MAP: 0.25275619256936455\n",
      "RECALL@10: 0.39049930070137673\n",
      "RECALL@50: 0.6743277065099824\n",
      "RECALL@100: 0.7738490817990719\n",
      "Epoach: 130 | avg loss: 0.109535694 | max loss: 0.13444234 | min loss: 0.08959322\n",
      "Epoach: 131 | avg loss: 0.12030397 | max loss: 0.15602115 | min loss: 0.0702851207028512 | time: 7.2698318958282478\n",
      "Epoach: 132 | avg loss: 0.12714842 | max loss: 0.15507792 | min loss: 0.0899807108998071 | time: 7.0918672084808355\n",
      "Epoach: 133 | avg loss: 0.12981841 | max loss: 0.15516022 | min loss: 0.09725645.09725645 | time: 7.2541887760162357\n",
      "Epoach: 134 | avg loss: 0.12710814 | max loss: 0.18999387 | min loss: 0.0918492809184928 | time: 7.283825397491455224\n",
      "Epoach: 135 | avg loss: 0.12384147 | max loss: 0.15293159 | min loss: 0.08190185681901856 | time: 7.3303263187408458\n",
      "Epoach: 136 | avg loss: 0.10275482 | max loss: 0.12628697 | min loss: 0.07055084.07055084 | time: 7.28356409072876\n",
      "Epoach: 137 | avg loss: 0.10592739 | max loss: 0.1614766 | min loss: 0.0674236.0674236 | time: 7.36288952827453609\n",
      "Epoach: 138 | avg loss: 0.12726012 | max loss: 0.1929503 | min loss: 0.0894579908945799 | time: 7.1230063438415537\n",
      "Epoach: 139 | avg loss: 0.109837726 | max loss: 0.12060818 | min loss: 0.091730719173071 | time: 7.23902082443237345\n",
      "Step: 7 | loss: 0.10445147 | current max loss: 0.16101182 | current min loss: 0.07354873 | time: 7.325538873672485\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.17419067215363515\n",
      "MAP @25: 0.27970944560178695\n",
      "MAP @50: 0.34002267214861004\n",
      "MAP @100: 0.361842938151953\n",
      "MAP @200: 0.3719552581346292\n",
      "MAP @300: 0.37655106774936364\n",
      "MAP: 0.25273146924239515\n",
      "RECALL@10: 0.38647320995753376\n",
      "RECALL@50: 0.6855000880408587\n",
      "RECALL@100: 0.7700106979606881\n",
      "Epoach: 140 | avg loss: 0.12333746 | max loss: 0.16101182 | min loss: 0.07354873\n",
      "Epoach: 141 | avg loss: 0.11537984 | max loss: 0.14525847 | min loss: 0.0758746507587465 | time: 7.4076721668243417\n",
      "Epoach: 142 | avg loss: 0.12565064 | max loss: 0.1590651 | min loss: 0.09424418.09424418 | time: 7.3040664196014495\n",
      "Epoach: 143 | avg loss: 0.1250897 | max loss: 0.18472086 | min loss: 0.082473840.08247384 | time: 7.175346851348877\n",
      "Epoach: 144 | avg loss: 0.11389785 | max loss: 0.15103066 | min loss: 0.08437048.08437048 | time: 7.2508127689361572\n",
      "Epoach: 145 | avg loss: 0.123646125 | max loss: 0.15189546 | min loss: 0.095503679550367 | time: 7.1673316955566415\n",
      "Epoach: 146 | avg loss: 0.123359665 | max loss: 0.14818111 | min loss: 0.0664836206648362 | time: 7.1775548458099365\n",
      "Epoach: 147 | avg loss: 0.12951896 | max loss: 0.16996779 | min loss: 0.0997890409978904 | time: 7.16318607330322395\n",
      "Epoach: 148 | avg loss: 0.12955004 | max loss: 0.15969898 | min loss: 0.085543476085543476 | time: 7.202609539031982\n",
      "Epoach: 149 | avg loss: 0.13576128 | max loss: 0.16207124 | min loss: 0.0982986209829862 | time: 7.2414610385894775\n",
      "Step: 7 | loss: 0.08349776 | current max loss: 0.15659514 | current min loss: 0.08349776 | time: 7.276780128479004175\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.1743920242994317\n",
      "MAP @25: 0.2743560468939042\n",
      "MAP @50: 0.336635908204862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP @100: 0.3595678943826437\n",
      "MAP @200: 0.3693039234196437\n",
      "MAP @300: 0.3741783407949904\n",
      "MAP: 0.24899243907276575\n",
      "RECALL@10: 0.3840735140124321\n",
      "RECALL@50: 0.677707812252005\n",
      "RECALL@100: 0.7699035547276566\n",
      "Epoach: 150 | avg loss: 0.122980796 | max loss: 0.15659514 | min loss: 0.08349776\n",
      "Epoach: 151 | avg loss: 0.12964933 | max loss: 0.19005033 | min loss: 0.0899069408990694 | time: 7.295185327529907\n",
      "Epoach: 152 | avg loss: 0.12818004 | max loss: 0.16179803 | min loss: 0.0758545707585457 | time: 7.2142586708068855\n",
      "Epoach: 153 | avg loss: 0.1289148 | max loss: 0.19278608 | min loss: 0.07356605.07356605 | time: 7.2738566398620605\n",
      "Epoach: 154 | avg loss: 0.1226775 | max loss: 0.14730117 | min loss: 0.08831321.08831321 | time: 7.2197029590606695\n",
      "Epoach: 155 | avg loss: 0.12865415 | max loss: 0.20567371 | min loss: 0.0814238108142381 | time: 7.12075734138488826\n",
      "Epoach: 156 | avg loss: 0.115246035 | max loss: 0.1301985 | min loss: 0.093526869352686 | time: 7.2388451099395752\n",
      "Epoach: 157 | avg loss: 0.13291025 | max loss: 0.17828828 | min loss: 0.0842052808420528 | time: 7.135978698730469\n",
      "Epoach: 158 | avg loss: 0.10686029 | max loss: 0.1505965 | min loss: 0.0520909105209091 | time: 7.20869684219360354\n",
      "Epoach: 159 | avg loss: 0.12029894 | max loss: 0.16357799 | min loss: 0.079245865079245865 | time: 7.0976033210754395\n",
      "Step: 7 | loss: 0.12447993 | current max loss: 0.16216093 | current min loss: 0.08046014 | time: 7.3444659709930425\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.1758289241622575\n",
      "MAP @25: 0.2759837182473286\n",
      "MAP @50: 0.3381221195286773\n",
      "MAP @100: 0.36119755096157163\n",
      "MAP @200: 0.37098732121064015\n",
      "MAP @300: 0.37560189104243363\n",
      "MAP: 0.25074564098930546\n",
      "RECALL@10: 0.3839853305909154\n",
      "RECALL@50: 0.6785322338493736\n",
      "RECALL@100: 0.7712564078778955\n",
      "Epoach: 160 | avg loss: 0.11097397 | max loss: 0.16216093 | min loss: 0.08046014\n",
      "Epoach: 161 | avg loss: 0.13203102 | max loss: 0.17208955 | min loss: 0.09066202590662025 | time: 7.09327673912048364\n",
      "Epoach: 162 | avg loss: 0.13087589 | max loss: 0.17522132 | min loss: 0.0975946109759461 | time: 7.3328332901000985\n",
      "Epoach: 163 | avg loss: 0.113101825 | max loss: 0.14180225 | min loss: 0.076393677639367 | time: 7.23366856575012255\n",
      "Epoach: 164 | avg loss: 0.12540674 | max loss: 0.168939 | min loss: 0.0829293608292936 | time: 7.23978900909423834\n",
      "Epoach: 165 | avg loss: 0.1214287 | max loss: 0.14461343 | min loss: 0.080453254080453254 | time: 7.32610368728637705\n",
      "Epoach: 166 | avg loss: 0.11146188 | max loss: 0.14207396 | min loss: 0.060699295060699295 | time: 7.181594133377075\n",
      "Epoach: 167 | avg loss: 0.12125977 | max loss: 0.13797322 | min loss: 0.0824002.0824002 | time: 7.3133394718170172\n",
      "Epoach: 168 | avg loss: 0.13219182 | max loss: 0.20073168 | min loss: 0.0763162.0763162 | time: 7.3257324695587165\n",
      "Epoach: 169 | avg loss: 0.1086783 | max loss: 0.16583434 | min loss: 0.070133880.07013388 | time: 7.2546961307525635\n",
      "Step: 7 | loss: 0.099920765 | current max loss: 0.16573715 | current min loss: 0.09609317 | time: 7.220130443572998\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.1838281403096218\n",
      "MAP @25: 0.29617214293514005\n",
      "MAP @50: 0.35873239195324347\n",
      "MAP @100: 0.38115806551059545\n",
      "MAP @200: 0.3903410574899741\n",
      "MAP @300: 0.3948225665894048\n",
      "MAP: 0.2535453633980681\n",
      "RECALL@10: 0.38274844450314033\n",
      "RECALL@50: 0.6970525780980601\n",
      "RECALL@100: 0.7733320461113818\n",
      "\n",
      "Run for the test set\n",
      "TEST set, len  542\n",
      "MAP @10 bioASQ: 0.16854075147894318\n",
      "MAP @25: 0.24549095310454266\n",
      "MAP @50: 0.286464111617804\n",
      "MAP @100: 0.30734524431270127\n",
      "MAP @200: 0.31620662848061404\n",
      "MAP @300: 0.319445862767345\n",
      "MAP: 0.2821870322085701\n",
      "RECALL@10: 0.40446242900297\n",
      "RECALL@50: 0.6895746641349656\n",
      "RECALL@100: 0.779370244656268\n",
      "Epoach: 170 | avg loss: 0.1141091 | max loss: 0.16573715 | min loss: 0.09609317\n",
      "Epoach: 171 | avg loss: 0.12821916 | max loss: 0.17293437 | min loss: 0.0741436607414366 | time: 7.1773002147674565\n",
      "Epoach: 172 | avg loss: 0.11087568 | max loss: 0.1592532 | min loss: 0.07464691.07464691 | time: 7.009093523025513\n",
      "Epoach: 173 | avg loss: 0.09883292 | max loss: 0.12565236 | min loss: 0.06640135.06640135 | time: 7.2761783599853525\n",
      "Epoach: 174 | avg loss: 0.10525979 | max loss: 0.14815989 | min loss: 0.05946505459465054 | time: 7.2450282573699951\n",
      "Epoach: 175 | avg loss: 0.1322105 | max loss: 0.16512683 | min loss: 0.08207034.08207034 | time: 7.222728967666626\n",
      "Epoach: 176 | avg loss: 0.11982639 | max loss: 0.16292572 | min loss: 0.0935387609353876 | time: 7.22682523727417845\n",
      "Epoach: 177 | avg loss: 0.101029575 | max loss: 0.14481463 | min loss: 0.0556279835627983 | time: 7.1545882225036621\n",
      "Epoach: 178 | avg loss: 0.11167384 | max loss: 0.15102671 | min loss: 0.0786836307868363 | time: 7.28417372703552254\n",
      "Epoach: 179 | avg loss: 0.11237638 | max loss: 0.14226015 | min loss: 0.0943222809432228 | time: 7.31845903396606454\n",
      "Step: 7 | loss: 0.117074564 | current max loss: 0.16782479 | current min loss: 0.10850035 | time: 7.2261621952056885\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.18716980207720954\n",
      "MAP @25: 0.29943113546055106\n",
      "MAP @50: 0.3618374526134501\n",
      "MAP @100: 0.38375708265567093\n",
      "MAP @200: 0.39305429405824927\n",
      "MAP @300: 0.39750699551372887\n",
      "MAP: 0.2541565257692609\n",
      "RECALL@10: 0.3869469551831324\n",
      "RECALL@50: 0.6971574208940085\n",
      "RECALL@100: 0.7691611249813964\n",
      "\n",
      "Run for the test set\n",
      "TEST set, len  542\n",
      "MAP @10 bioASQ: 0.17016385520998062\n",
      "MAP @25: 0.24710556510640566\n",
      "MAP @50: 0.28754408992439257\n",
      "MAP @100: 0.30834555634962835\n",
      "MAP @200: 0.317320420543861\n",
      "MAP @300: 0.3206184907056666\n",
      "MAP: 0.2829476674569666\n",
      "RECALL@10: 0.40740116189760406\n",
      "RECALL@50: 0.6884269585595749\n",
      "RECALL@100: 0.7785417745176165\n",
      "Epoach: 180 | avg loss: 0.1414197 | max loss: 0.16782479 | min loss: 0.10850035\n",
      "Epoach: 181 | avg loss: 0.09695637 | max loss: 0.11140604 | min loss: 0.0866541908665419 | time: 7.298676967620855564\n",
      "Epoach: 182 | avg loss: 0.11757391 | max loss: 0.16657232 | min loss: 0.06149036861490368 | time: 7.3261213302612305\n",
      "Epoach: 183 | avg loss: 0.13866308 | max loss: 0.1737435 | min loss: 0.1096274710962747 | time: 7.09432530403137295\n",
      "Epoach: 184 | avg loss: 0.13055918 | max loss: 0.16869166 | min loss: 0.07114840711484 | time: 7.2352845668792725205\n",
      "Epoach: 185 | avg loss: 0.104478486 | max loss: 0.14919567 | min loss: 0.0734873043487304 | time: 7.34447431564331055\n",
      "Epoach: 186 | avg loss: 0.11458882 | max loss: 0.17993923 | min loss: 0.05375253453752534 | time: 7.28704619407653889\n",
      "Epoach: 187 | avg loss: 0.11970785 | max loss: 0.18487547 | min loss: 0.077528276077528276 | time: 7.2065038681030275\n",
      "Epoach: 188 | avg loss: 0.11617205 | max loss: 0.15852627 | min loss: 0.074680237468023 | time: 7.1911163330078125\n",
      "Epoach: 189 | avg loss: 0.09444153 | max loss: 0.11352303 | min loss: 0.08031893.08031893 | time: 7.17886900901794435\n",
      "Step: 7 | loss: 0.12551218 | current max loss: 0.15477467 | current min loss: 0.062117264 | time: 7.4304261207580575\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.17523858514599258\n",
      "MAP @25: 0.2767644488939802\n",
      "MAP @50: 0.33975633931736177\n",
      "MAP @100: 0.3630331550115656\n",
      "MAP @200: 0.3729619684630158\n",
      "MAP @300: 0.3780729069799826\n",
      "MAP: 0.2502556451111925\n",
      "RECALL@10: 0.3844510682038688\n",
      "RECALL@50: 0.6812877296496732\n",
      "RECALL@100: 0.7710269048706148\n",
      "Epoach: 190 | avg loss: 0.10527942 | max loss: 0.15477467 | min loss: 0.062117264\n",
      "Epoach: 191 | avg loss: 0.11661269 | max loss: 0.13732353 | min loss: 0.09819826.09819826 | time: 7.18838930130004914\n",
      "Epoach: 192 | avg loss: 0.107770994 | max loss: 0.14458206 | min loss: 0.07511151675111516 | time: 7.193532705307007\n",
      "Epoach: 193 | avg loss: 0.13401124 | max loss: 0.14654237 | min loss: 0.101624765101624765 | time: 7.3642706871032715\n",
      "Epoach: 194 | avg loss: 0.112958066 | max loss: 0.14322403 | min loss: 0.0868439756843975 | time: 7.2860314846038823\n",
      "Epoach: 195 | avg loss: 0.119806625 | max loss: 0.16294594 | min loss: 0.0847752244775224 | time: 7.1899924278259283\n",
      "Epoach: 196 | avg loss: 0.120760374 | max loss: 0.14149414 | min loss: 0.08335283352 | time: 7.29563713073730562525\n",
      "Epoach: 197 | avg loss: 0.10520836 | max loss: 0.14665222 | min loss: 0.0646982506469825 | time: 7.234529733657837255\n",
      "Epoach: 198 | avg loss: 0.1084027 | max loss: 0.14358954 | min loss: 0.0698500306985003 | time: 7.32360076904296942545\n",
      "Epoach: 199 | avg loss: 0.114197634 | max loss: 0.15991473 | min loss: 0.075209117520911 | time: 7.198521852493286362\n",
      "Step: 7 | loss: 0.097995624 | current max loss: 0.15523131 | current min loss: 0.072796576 | time: 7.2503602504730225\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.1798780129335685\n",
      "MAP @25: 0.286648038321772\n",
      "MAP @50: 0.3530016284252338\n",
      "MAP @100: 0.3732163581689907\n",
      "MAP @200: 0.38383000544667634\n",
      "MAP @300: 0.3883919574348255\n",
      "MAP: 0.254877340546946\n",
      "RECALL@10: 0.38714537928764026\n",
      "RECALL@50: 0.6922670245512672\n",
      "RECALL@100: 0.7701625670691966\n",
      "Epoach: 200 | avg loss: 0.10492382 | max loss: 0.15523131 | min loss: 0.072796576\n",
      "Epoach: 201 | avg loss: 0.12303525 | max loss: 0.1493389 | min loss: 0.095428689542868 | time: 7.163046360015869614\n",
      "Epoach: 202 | avg loss: 0.10416442 | max loss: 0.14659397 | min loss: 0.06719138567191385 | time: 7.3001251220703125\n",
      "Epoach: 203 | avg loss: 0.12344153 | max loss: 0.17316853 | min loss: 0.08881657688816576 | time: 7.18980312347412155\n",
      "Epoach: 204 | avg loss: 0.12424375 | max loss: 0.16472733 | min loss: 0.1005080210050802 | time: 7.2322471141815186\n",
      "Epoach: 205 | avg loss: 0.106951326 | max loss: 0.14351846 | min loss: 0.0792236559223655 | time: 7.332307815551758455\n",
      "Epoach: 206 | avg loss: 0.11699806 | max loss: 0.16703494 | min loss: 0.0811772308117723 | time: 7.2584180831909185\n",
      "Epoach: 207 | avg loss: 0.0951813 | max loss: 0.14940201 | min loss: 0.0742690107426901 | time: 7.390532255172729553\n",
      "Epoach: 208 | avg loss: 0.11451972 | max loss: 0.17207997 | min loss: 0.0643375706433757 | time: 7.3992762565612794\n",
      "Epoach: 209 | avg loss: 0.13291 | max loss: 0.19992332 | min loss: 0.086377890.08637789 | time: 7.39583778381347795\n",
      "Step: 7 | loss: 0.098667756 | current max loss: 0.14802535 | current min loss: 0.098667756 | time: 7.382756948471069\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.18125367430922992\n",
      "MAP @25: 0.2979336021099007\n",
      "MAP @50: 0.3588152411179047\n",
      "MAP @100: 0.3797265124264823\n",
      "MAP @200: 0.390203866222824\n",
      "MAP @300: 0.39481871616374703\n",
      "MAP: 0.2509985266799686\n",
      "RECALL@10: 0.3809318448376246\n",
      "RECALL@50: 0.6921272652461344\n",
      "RECALL@100: 0.7666760209355918\n",
      "Epoach: 210 | avg loss: 0.12175794 | max loss: 0.14802535 | min loss: 0.098667756\n",
      "Epoach: 211 | avg loss: 0.11727217 | max loss: 0.17722708 | min loss: 0.06116619761166197 | time: 7.2971827983856272\n",
      "Epoach: 212 | avg loss: 0.117636725 | max loss: 0.15565184 | min loss: 0.06225428225428 | time: 7.2170441150665285753\n",
      "Epoach: 213 | avg loss: 0.11826941 | max loss: 0.16345349 | min loss: 0.0940131609401316 | time: 7.262212038040161\n",
      "Epoach: 214 | avg loss: 0.1290701 | max loss: 0.16884302 | min loss: 0.088032596088032596 | time: 7.1961178779602057\n",
      "Epoach: 215 | avg loss: 0.12080053 | max loss: 0.15512854 | min loss: 0.0991724809917248 | time: 7.050011634826663\n",
      "Epoach: 216 | avg loss: 0.11155695 | max loss: 0.12991287 | min loss: 0.0808147808081478 | time: 7.3600037097930915\n",
      "Epoach: 217 | avg loss: 0.11507916 | max loss: 0.144011 | min loss: 0.0794079707940797 | time: 7.23198246955871659327\n",
      "Epoach: 218 | avg loss: 0.11718708 | max loss: 0.14289516 | min loss: 0.07184771471847714 | time: 7.3045773506164555\n",
      "Epoach: 219 | avg loss: 0.10799653 | max loss: 0.14443253 | min loss: 0.0721708707217087 | time: 7.473936319351196936\n",
      "Step: 7 | loss: 0.13864726 | current max loss: 0.16691649 | current min loss: 0.09280909 | time: 7.3152041435241795\n",
      "Predict query: 81\n",
      "TEST set, len  81\n",
      "MAP @10 bioASQ: 0.18012884577699392\n",
      "MAP @25: 0.2881033581322253\n",
      "MAP @50: 0.35050843272427357\n",
      "MAP @100: 0.3728636351669482\n",
      "MAP @200: 0.3825065978336262\n",
      "MAP @300: 0.3874132583153793\n",
      "MAP: 0.2530875676784894\n",
      "RECALL@10: 0.3884558660195829\n",
      "RECALL@50: 0.6853351550752427\n",
      "RECALL@100: 0.7715650498532042\n",
      "Epoach: 220 | avg loss: 0.1335946 | max loss: 0.16691649 | min loss: 0.09280909\n",
      "Epoach: 221 | avg loss: 0.14142254 | max loss: 0.21940087 | min loss: 0.0873632208736322 | time: 7.29624390602111823\n",
      "Epoach: 222 | avg loss: 0.104749665 | max loss: 0.1512545 | min loss: 0.055271085527108 | time: 7.2712104320526128\n",
      "Epoach: 223 | avg loss: 0.11842991 | max loss: 0.14986642 | min loss: 0.09034414690344146 | time: 7.3325297832489015\n",
      "Epoach: 224 | avg loss: 0.11505957 | max loss: 0.1683234 | min loss: 0.0796950.079695 | time: 7.22757768630981451144\n",
      "Epoach: 225 | avg loss: 0.11308734 | max loss: 0.1730702 | min loss: 0.0741779907417799 | time: 7.116195201873779\n",
      "Epoach: 226 | avg loss: 0.14186764 | max loss: 0.17312224 | min loss: 0.1103255611032556 | time: 7.2430531978607185\n",
      "Epoach: 227 | avg loss: 0.11562839 | max loss: 0.15921995 | min loss: 0.0829279608292796 | time: 7.128879547119141\n",
      "Epoach: 228 | avg loss: 0.12553455 | max loss: 0.15673241 | min loss: 0.107771024107771024 | time: 7.2180964946746833\n",
      "Epoach: 229 | avg loss: 0.118998006 | max loss: 0.16905858 | min loss: 0.067745696774569 | time: 7.1966078281402591\n",
      "Step: 7 | loss: 0.12966877 | current max loss: 0.1345713 | current min loss: 0.08405525 | time: 7.1229972839355475\n",
      "Predict query: 8\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-da033c60b7d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoach\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mvalidate_query_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_articles_collection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mbio_map_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_query_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ff3fef10a7fd>\u001b[0m in \u001b[0;36mvalidate_test_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predict query:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdeep_ranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_score_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mdeep_ranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdeep_ranking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbm25_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bioasq_data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"documents\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1476\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m       return training_arrays.predict_loop(\n\u001b[0;32m-> 1478\u001b[0;31m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, inputs, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models.generic_model import ModelAPI, f_recall, f_map\n",
    "\n",
    "gen = TrainDataGenerator(train_articles_collection, tk, 256)\n",
    "\n",
    "gen_iter = iter(gen)\n",
    "\n",
    "loss = []\n",
    "\n",
    "for i,line in enumerate(loss):\n",
    "    \n",
    "    print(\"Epoach:\",i,\"| avg loss:\",np.mean(loss[i]),\"| max loss:\",np.max(loss[i]),\"| min loss:\",np.min(loss[i]))\n",
    "\n",
    "import time\n",
    "\n",
    "max_bio_map_val = 0.166\n",
    "max_bio_map_test = 0\n",
    "\n",
    "for epoach in range(0,500):\n",
    "    loss_per_epoach = []\n",
    "    for step in range(len(gen)):\n",
    "        X = next(gen_iter)\n",
    "        \n",
    "        start = time.time()\n",
    "        loss_per_epoach.append(deepRank_model.train_on_batch(X))\n",
    "        print(\"Step:\",step,\"| loss:\",loss_per_epoach[-1],\"| current max loss:\",np.max(loss_per_epoach),\"| current min loss:\",np.min(loss_per_epoach),\"| time:\",time.time()-start,end=\"\\r\")\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    if epoach%10==0:\n",
    "        print(\"\")\n",
    "        validate_query_results = validate_test_data(validation_articles_collection)\n",
    "        print(\"\")\n",
    "        bio_map_val, _, _ = validation_score(validate_query_results)\n",
    "        if bio_map_val >= max_bio_map_val:\n",
    "            max_bio_map_val = bio_map_val\n",
    "            print(\"\")\n",
    "            print(\"Run for the test set\")\n",
    "            test_query_results = validate_test_data(test_articles_collection)\n",
    "            bio_map_test, _, _ = validation_score(test_query_results)\n",
    "\n",
    "            if bio_map_test >= max_bio_map_test:\n",
    "                max_bio_map_test = bio_map_test\n",
    "                \n",
    "                deepRank_model.save_weights(\"deep_rank_v6_weights.h5\")\n",
    "                #deepRank_model.save(\"deep_rank_model.h5\")\n",
    "                \n",
    "    loss.append(loss_per_epoach)\n",
    "    print(\"\",end=\"\\r\")#clear the line\n",
    "    print(\"Epoach:\",epoach,\"| avg loss:\",np.mean(loss[-1]),\"| max loss:\",np.max(loss[-1]),\"| min loss:\",np.min(loss[-1]))\n",
    "#deepRank_model.fit_generator(gen_iter, steps_per_epoch=len(gen), verbose=1, epochs=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST set, len  542\n",
      "MAP @10 bioASQ: 0.16169177941779414\n",
      "MAP @25: 0.23833417521847772\n",
      "MAP @50: 0.2799313708463157\n",
      "MAP @100: 0.3007881269784635\n",
      "MAP @200: 0.3094011205079565\n",
      "MAP @300: 0.31247589311468943\n",
      "MAP: 0.27747706690042545\n",
      "RECALL@10: 0.39841202864441505\n",
      "RECALL@50: 0.694037633683637\n",
      "RECALL@100: 0.7848605542414242\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_ = validation_score(validate_test_data(test_articles_collection))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoach: 0 | avg loss: 0.9125016 | max loss: 1.0042224 | min loss: 0.80067205\n",
      "Epoach: 1 | avg loss: 0.49317193 | max loss: 0.72874445 | min loss: 0.36090243\n",
      "Epoach: 2 | avg loss: 0.28394535 | max loss: 0.30443656 | min loss: 0.22263007\n",
      "Epoach: 3 | avg loss: 0.27021748 | max loss: 0.3453509 | min loss: 0.21942542\n",
      "Epoach: 4 | avg loss: 0.22671619 | max loss: 0.29153928 | min loss: 0.17943613\n",
      "Epoach: 5 | avg loss: 0.20183098 | max loss: 0.24140441 | min loss: 0.15858825\n",
      "Epoach: 6 | avg loss: 0.17883258 | max loss: 0.21680002 | min loss: 0.14568402\n",
      "Epoach: 7 | avg loss: 0.17802173 | max loss: 0.21380427 | min loss: 0.1308456\n",
      "Epoach: 8 | avg loss: 0.16129185 | max loss: 0.18449171 | min loss: 0.14353858\n",
      "Epoach: 9 | avg loss: 0.17940602 | max loss: 0.2193988 | min loss: 0.13944818\n",
      "Epoach: 10 | avg loss: 0.17154312 | max loss: 0.19820029 | min loss: 0.13796973\n",
      "Epoach: 11 | avg loss: 0.14994204 | max loss: 0.21805936 | min loss: 0.08608124\n",
      "Epoach: 12 | avg loss: 0.15823033 | max loss: 0.19361591 | min loss: 0.11516501\n",
      "Epoach: 13 | avg loss: 0.14865342 | max loss: 0.18927157 | min loss: 0.08867377\n",
      "Epoach: 14 | avg loss: 0.14407511 | max loss: 0.17260082 | min loss: 0.10460846\n",
      "Epoach: 15 | avg loss: 0.1294682 | max loss: 0.16858634 | min loss: 0.09418613\n",
      "Epoach: 16 | avg loss: 0.15847948 | max loss: 0.1836209 | min loss: 0.12167572\n",
      "Epoach: 17 | avg loss: 0.1343056 | max loss: 0.19023322 | min loss: 0.091756366\n",
      "Epoach: 18 | avg loss: 0.1224708 | max loss: 0.17449825 | min loss: 0.09055804\n",
      "Epoach: 19 | avg loss: 0.1560771 | max loss: 0.23402852 | min loss: 0.09341614\n",
      "Epoach: 20 | avg loss: 0.14304489 | max loss: 0.16564412 | min loss: 0.11334045\n",
      "Epoach: 21 | avg loss: 0.13640939 | max loss: 0.16505818 | min loss: 0.11930085\n",
      "Epoach: 22 | avg loss: 0.12535563 | max loss: 0.17411429 | min loss: 0.095259696\n",
      "Epoach: 23 | avg loss: 0.16198543 | max loss: 0.21013002 | min loss: 0.14260782\n",
      "Epoach: 24 | avg loss: 0.14705326 | max loss: 0.21715309 | min loss: 0.096290685\n",
      "Epoach: 25 | avg loss: 0.11794478 | max loss: 0.15313746 | min loss: 0.08104662\n",
      "Epoach: 26 | avg loss: 0.14347588 | max loss: 0.2373665 | min loss: 0.109697916\n",
      "Epoach: 27 | avg loss: 0.1355557 | max loss: 0.18860565 | min loss: 0.101668596\n",
      "Epoach: 28 | avg loss: 0.13470176 | max loss: 0.18260312 | min loss: 0.10194878\n",
      "Epoach: 29 | avg loss: 0.14155304 | max loss: 0.20119151 | min loss: 0.09361894\n",
      "Epoach: 30 | avg loss: 0.14155135 | max loss: 0.19834858 | min loss: 0.0949727\n",
      "Epoach: 31 | avg loss: 0.13417517 | max loss: 0.15330367 | min loss: 0.11480938\n",
      "Epoach: 32 | avg loss: 0.12195169 | max loss: 0.16342108 | min loss: 0.08785638\n",
      "Epoach: 33 | avg loss: 0.13282719 | max loss: 0.21276896 | min loss: 0.10143275\n",
      "Epoach: 34 | avg loss: 0.118202 | max loss: 0.15498024 | min loss: 0.07243498\n",
      "Epoach: 35 | avg loss: 0.13550355 | max loss: 0.2100889 | min loss: 0.088939995\n",
      "Epoach: 36 | avg loss: 0.12728623 | max loss: 0.18325587 | min loss: 0.077848144\n",
      "Epoach: 37 | avg loss: 0.14154685 | max loss: 0.20354104 | min loss: 0.114484705\n",
      "Epoach: 38 | avg loss: 0.12396614 | max loss: 0.20121771 | min loss: 0.08114089\n",
      "Epoach: 39 | avg loss: 0.13002037 | max loss: 0.17463444 | min loss: 0.1043766\n",
      "Epoach: 40 | avg loss: 0.12251069 | max loss: 0.18186021 | min loss: 0.09330936\n",
      "Epoach: 41 | avg loss: 0.13849252 | max loss: 0.18673538 | min loss: 0.09400418\n",
      "Epoach: 42 | avg loss: 0.13959584 | max loss: 0.18608947 | min loss: 0.09049299\n",
      "Epoach: 43 | avg loss: 0.11447646 | max loss: 0.16573317 | min loss: 0.081478\n",
      "Epoach: 44 | avg loss: 0.1403347 | max loss: 0.15892221 | min loss: 0.10116335\n",
      "Epoach: 45 | avg loss: 0.123007774 | max loss: 0.2004541 | min loss: 0.086272344\n",
      "Epoach: 46 | avg loss: 0.14596887 | max loss: 0.22014281 | min loss: 0.112008944\n",
      "Epoach: 47 | avg loss: 0.1371493 | max loss: 0.17249142 | min loss: 0.11428482\n",
      "Epoach: 48 | avg loss: 0.11318117 | max loss: 0.14699222 | min loss: 0.08392244\n",
      "Epoach: 49 | avg loss: 0.12978484 | max loss: 0.17627062 | min loss: 0.096893795\n",
      "Epoach: 50 | avg loss: 0.12024775 | max loss: 0.16358064 | min loss: 0.08817414\n",
      "Epoach: 51 | avg loss: 0.12840645 | max loss: 0.23955789 | min loss: 0.054118264\n",
      "Epoach: 52 | avg loss: 0.14068726 | max loss: 0.19548008 | min loss: 0.104195386\n",
      "Epoach: 53 | avg loss: 0.11976034 | max loss: 0.14174968 | min loss: 0.061712284\n",
      "Epoach: 54 | avg loss: 0.14333203 | max loss: 0.18659239 | min loss: 0.10157521\n",
      "Epoach: 55 | avg loss: 0.13084026 | max loss: 0.1636992 | min loss: 0.09555291\n",
      "Epoach: 56 | avg loss: 0.12226493 | max loss: 0.1592394 | min loss: 0.080581546\n",
      "Epoach: 57 | avg loss: 0.12206644 | max loss: 0.16975245 | min loss: 0.068161584\n",
      "Epoach: 58 | avg loss: 0.12992905 | max loss: 0.14667211 | min loss: 0.10239952\n",
      "Epoach: 59 | avg loss: 0.13541949 | max loss: 0.16244505 | min loss: 0.11245321\n",
      "Epoach: 60 | avg loss: 0.13597706 | max loss: 0.16079853 | min loss: 0.12120643\n",
      "Epoach: 61 | avg loss: 0.11440115 | max loss: 0.13571069 | min loss: 0.097513855\n",
      "Epoach: 62 | avg loss: 0.11146161 | max loss: 0.1863055 | min loss: 0.061214484\n",
      "Epoach: 63 | avg loss: 0.12345019 | max loss: 0.16765341 | min loss: 0.09223895\n",
      "Epoach: 64 | avg loss: 0.12755573 | max loss: 0.19000028 | min loss: 0.07788189\n",
      "Epoach: 65 | avg loss: 0.14336658 | max loss: 0.17931573 | min loss: 0.11264965\n",
      "Epoach: 66 | avg loss: 0.139204 | max loss: 0.17616737 | min loss: 0.11513187\n",
      "Epoach: 67 | avg loss: 0.123201534 | max loss: 0.16898859 | min loss: 0.09216022\n",
      "Epoach: 68 | avg loss: 0.13628274 | max loss: 0.16136146 | min loss: 0.10498968\n",
      "Epoach: 69 | avg loss: 0.1345881 | max loss: 0.1670266 | min loss: 0.09663914\n",
      "Epoach: 70 | avg loss: 0.13168631 | max loss: 0.1827197 | min loss: 0.09472344\n",
      "Epoach: 71 | avg loss: 0.13559842 | max loss: 0.2112328 | min loss: 0.061316475\n",
      "Epoach: 72 | avg loss: 0.113088205 | max loss: 0.14079547 | min loss: 0.07185498\n",
      "Epoach: 73 | avg loss: 0.11908828 | max loss: 0.1632543 | min loss: 0.09182133\n",
      "Epoach: 74 | avg loss: 0.1339307 | max loss: 0.21749558 | min loss: 0.069891274\n",
      "Epoach: 75 | avg loss: 0.11574526 | max loss: 0.15864204 | min loss: 0.083352044\n",
      "Epoach: 76 | avg loss: 0.12770608 | max loss: 0.18351571 | min loss: 0.07157096\n",
      "Epoach: 77 | avg loss: 0.12679563 | max loss: 0.18082902 | min loss: 0.07872251\n",
      "Epoach: 78 | avg loss: 0.117759705 | max loss: 0.15956298 | min loss: 0.080052234\n",
      "Epoach: 79 | avg loss: 0.11549167 | max loss: 0.13695568 | min loss: 0.10073331\n",
      "Epoach: 80 | avg loss: 0.13038842 | max loss: 0.17734766 | min loss: 0.09957172\n",
      "Epoach: 81 | avg loss: 0.13132675 | max loss: 0.1911749 | min loss: 0.073122166\n",
      "Epoach: 82 | avg loss: 0.13073713 | max loss: 0.16726801 | min loss: 0.10106676\n",
      "Epoach: 83 | avg loss: 0.1325604 | max loss: 0.169622 | min loss: 0.095894344\n",
      "Epoach: 84 | avg loss: 0.1449853 | max loss: 0.1935093 | min loss: 0.10641447\n",
      "Epoach: 85 | avg loss: 0.1231386 | max loss: 0.17995945 | min loss: 0.087629296\n",
      "Epoach: 86 | avg loss: 0.12449381 | max loss: 0.15955055 | min loss: 0.090039834\n",
      "Epoach: 87 | avg loss: 0.12904318 | max loss: 0.169639 | min loss: 0.091004804\n",
      "Epoach: 88 | avg loss: 0.11352096 | max loss: 0.2033876 | min loss: 0.05610793\n",
      "Epoach: 89 | avg loss: 0.09983999 | max loss: 0.12355232 | min loss: 0.06331807\n",
      "Epoach: 90 | avg loss: 0.118686914 | max loss: 0.14776096 | min loss: 0.08469281\n",
      "Epoach: 91 | avg loss: 0.13386112 | max loss: 0.16763479 | min loss: 0.099743865\n",
      "Epoach: 92 | avg loss: 0.1225351 | max loss: 0.18580732 | min loss: 0.07135203\n",
      "Epoach: 93 | avg loss: 0.13260387 | max loss: 0.15698902 | min loss: 0.09888101\n",
      "Epoach: 94 | avg loss: 0.12662022 | max loss: 0.1549881 | min loss: 0.101264544\n",
      "Epoach: 95 | avg loss: 0.12595809 | max loss: 0.14741984 | min loss: 0.090201646\n",
      "Epoach: 96 | avg loss: 0.120436504 | max loss: 0.1749489 | min loss: 0.082485415\n",
      "Epoach: 97 | avg loss: 0.11668563 | max loss: 0.14204536 | min loss: 0.095165834\n",
      "Epoach: 98 | avg loss: 0.14738703 | max loss: 0.22028753 | min loss: 0.08594997\n",
      "Epoach: 99 | avg loss: 0.13169128 | max loss: 0.19685341 | min loss: 0.09337724\n",
      "Epoach: 100 | avg loss: 0.11215229 | max loss: 0.15753786 | min loss: 0.07659926\n",
      "Epoach: 101 | avg loss: 0.111854926 | max loss: 0.13560629 | min loss: 0.081389666\n",
      "Epoach: 102 | avg loss: 0.12348192 | max loss: 0.17242576 | min loss: 0.08947649\n",
      "Epoach: 103 | avg loss: 0.13245863 | max loss: 0.17178094 | min loss: 0.11606664\n",
      "Epoach: 104 | avg loss: 0.14003517 | max loss: 0.19332692 | min loss: 0.093586534\n",
      "Epoach: 105 | avg loss: 0.1171598 | max loss: 0.16716018 | min loss: 0.072916925\n",
      "Epoach: 106 | avg loss: 0.10621229 | max loss: 0.16023287 | min loss: 0.06864786\n",
      "Epoach: 107 | avg loss: 0.11307475 | max loss: 0.15092917 | min loss: 0.08166652\n",
      "Epoach: 108 | avg loss: 0.12866719 | max loss: 0.17475227 | min loss: 0.087083355\n",
      "Epoach: 109 | avg loss: 0.113989905 | max loss: 0.18569195 | min loss: 0.07076393\n",
      "Epoach: 110 | avg loss: 0.11758676 | max loss: 0.13771963 | min loss: 0.097429104\n",
      "Epoach: 111 | avg loss: 0.1223331 | max loss: 0.19985129 | min loss: 0.06259223\n",
      "Epoach: 112 | avg loss: 0.10697573 | max loss: 0.13267756 | min loss: 0.06943471\n",
      "Epoach: 113 | avg loss: 0.123114064 | max loss: 0.16955033 | min loss: 0.080755904\n",
      "Epoach: 114 | avg loss: 0.11844236 | max loss: 0.1574506 | min loss: 0.07994513\n",
      "Epoach: 115 | avg loss: 0.10949586 | max loss: 0.18929581 | min loss: 0.07016608\n",
      "Epoach: 116 | avg loss: 0.12086545 | max loss: 0.15624791 | min loss: 0.09946294\n",
      "Epoach: 117 | avg loss: 0.11320803 | max loss: 0.14736357 | min loss: 0.07416316\n",
      "Epoach: 118 | avg loss: 0.120623484 | max loss: 0.19713645 | min loss: 0.08076659\n",
      "Epoach: 119 | avg loss: 0.12111886 | max loss: 0.14347509 | min loss: 0.09353537\n",
      "Epoach: 120 | avg loss: 0.109308794 | max loss: 0.15406655 | min loss: 0.072138995\n",
      "Epoach: 121 | avg loss: 0.12260577 | max loss: 0.16768783 | min loss: 0.10313976\n",
      "Epoach: 122 | avg loss: 0.12293301 | max loss: 0.16877489 | min loss: 0.09159986\n",
      "Epoach: 123 | avg loss: 0.126827 | max loss: 0.15773126 | min loss: 0.09554936\n",
      "Epoach: 124 | avg loss: 0.115047805 | max loss: 0.16813481 | min loss: 0.07883677\n",
      "Epoach: 125 | avg loss: 0.121839166 | max loss: 0.18360202 | min loss: 0.059213378\n",
      "Epoach: 126 | avg loss: 0.12293114 | max loss: 0.1826544 | min loss: 0.07625285\n",
      "Epoach: 127 | avg loss: 0.12642208 | max loss: 0.16227275 | min loss: 0.08599813\n",
      "Epoach: 128 | avg loss: 0.11752176 | max loss: 0.15461151 | min loss: 0.08998229\n",
      "Epoach: 129 | avg loss: 0.1406187 | max loss: 0.21687658 | min loss: 0.08626143\n",
      "Epoach: 130 | avg loss: 0.109535694 | max loss: 0.13444234 | min loss: 0.08959322\n",
      "Epoach: 131 | avg loss: 0.12030397 | max loss: 0.15602115 | min loss: 0.07028512\n",
      "Epoach: 132 | avg loss: 0.12714842 | max loss: 0.15507792 | min loss: 0.08998071\n",
      "Epoach: 133 | avg loss: 0.12981841 | max loss: 0.15516022 | min loss: 0.09725645\n",
      "Epoach: 134 | avg loss: 0.12710814 | max loss: 0.18999387 | min loss: 0.09184928\n",
      "Epoach: 135 | avg loss: 0.12384147 | max loss: 0.15293159 | min loss: 0.081901856\n",
      "Epoach: 136 | avg loss: 0.10275482 | max loss: 0.12628697 | min loss: 0.07055084\n",
      "Epoach: 137 | avg loss: 0.10592739 | max loss: 0.1614766 | min loss: 0.0674236\n",
      "Epoach: 138 | avg loss: 0.12726012 | max loss: 0.1929503 | min loss: 0.08945799\n",
      "Epoach: 139 | avg loss: 0.109837726 | max loss: 0.12060818 | min loss: 0.09173071\n",
      "Epoach: 140 | avg loss: 0.12333746 | max loss: 0.16101182 | min loss: 0.07354873\n",
      "Epoach: 141 | avg loss: 0.11537984 | max loss: 0.14525847 | min loss: 0.07587465\n",
      "Epoach: 142 | avg loss: 0.12565064 | max loss: 0.1590651 | min loss: 0.09424418\n",
      "Epoach: 143 | avg loss: 0.1250897 | max loss: 0.18472086 | min loss: 0.08247384\n",
      "Epoach: 144 | avg loss: 0.11389785 | max loss: 0.15103066 | min loss: 0.08437048\n",
      "Epoach: 145 | avg loss: 0.123646125 | max loss: 0.15189546 | min loss: 0.09550367\n",
      "Epoach: 146 | avg loss: 0.123359665 | max loss: 0.14818111 | min loss: 0.06648362\n",
      "Epoach: 147 | avg loss: 0.12951896 | max loss: 0.16996779 | min loss: 0.09978904\n",
      "Epoach: 148 | avg loss: 0.12955004 | max loss: 0.15969898 | min loss: 0.085543476\n",
      "Epoach: 149 | avg loss: 0.13576128 | max loss: 0.16207124 | min loss: 0.09829862\n",
      "Epoach: 150 | avg loss: 0.122980796 | max loss: 0.15659514 | min loss: 0.08349776\n",
      "Epoach: 151 | avg loss: 0.12964933 | max loss: 0.19005033 | min loss: 0.08990694\n",
      "Epoach: 152 | avg loss: 0.12818004 | max loss: 0.16179803 | min loss: 0.07585457\n",
      "Epoach: 153 | avg loss: 0.1289148 | max loss: 0.19278608 | min loss: 0.07356605\n",
      "Epoach: 154 | avg loss: 0.1226775 | max loss: 0.14730117 | min loss: 0.08831321\n",
      "Epoach: 155 | avg loss: 0.12865415 | max loss: 0.20567371 | min loss: 0.08142381\n",
      "Epoach: 156 | avg loss: 0.115246035 | max loss: 0.1301985 | min loss: 0.09352686\n",
      "Epoach: 157 | avg loss: 0.13291025 | max loss: 0.17828828 | min loss: 0.08420528\n",
      "Epoach: 158 | avg loss: 0.10686029 | max loss: 0.1505965 | min loss: 0.05209091\n",
      "Epoach: 159 | avg loss: 0.12029894 | max loss: 0.16357799 | min loss: 0.079245865\n",
      "Epoach: 160 | avg loss: 0.11097397 | max loss: 0.16216093 | min loss: 0.08046014\n",
      "Epoach: 161 | avg loss: 0.13203102 | max loss: 0.17208955 | min loss: 0.090662025\n",
      "Epoach: 162 | avg loss: 0.13087589 | max loss: 0.17522132 | min loss: 0.09759461\n",
      "Epoach: 163 | avg loss: 0.113101825 | max loss: 0.14180225 | min loss: 0.07639367\n",
      "Epoach: 164 | avg loss: 0.12540674 | max loss: 0.168939 | min loss: 0.08292936\n",
      "Epoach: 165 | avg loss: 0.1214287 | max loss: 0.14461343 | min loss: 0.080453254\n",
      "Epoach: 166 | avg loss: 0.11146188 | max loss: 0.14207396 | min loss: 0.060699295\n",
      "Epoach: 167 | avg loss: 0.12125977 | max loss: 0.13797322 | min loss: 0.0824002\n",
      "Epoach: 168 | avg loss: 0.13219182 | max loss: 0.20073168 | min loss: 0.0763162\n",
      "Epoach: 169 | avg loss: 0.1086783 | max loss: 0.16583434 | min loss: 0.07013388\n",
      "Epoach: 170 | avg loss: 0.1141091 | max loss: 0.16573715 | min loss: 0.09609317\n",
      "Epoach: 171 | avg loss: 0.12821916 | max loss: 0.17293437 | min loss: 0.07414366\n",
      "Epoach: 172 | avg loss: 0.11087568 | max loss: 0.1592532 | min loss: 0.07464691\n",
      "Epoach: 173 | avg loss: 0.09883292 | max loss: 0.12565236 | min loss: 0.06640135\n",
      "Epoach: 174 | avg loss: 0.10525979 | max loss: 0.14815989 | min loss: 0.059465054\n",
      "Epoach: 175 | avg loss: 0.1322105 | max loss: 0.16512683 | min loss: 0.08207034\n",
      "Epoach: 176 | avg loss: 0.11982639 | max loss: 0.16292572 | min loss: 0.09353876\n",
      "Epoach: 177 | avg loss: 0.101029575 | max loss: 0.14481463 | min loss: 0.055627983\n",
      "Epoach: 178 | avg loss: 0.11167384 | max loss: 0.15102671 | min loss: 0.07868363\n",
      "Epoach: 179 | avg loss: 0.11237638 | max loss: 0.14226015 | min loss: 0.09432228\n",
      "Epoach: 180 | avg loss: 0.1414197 | max loss: 0.16782479 | min loss: 0.10850035\n",
      "Epoach: 181 | avg loss: 0.09695637 | max loss: 0.11140604 | min loss: 0.08665419\n",
      "Epoach: 182 | avg loss: 0.11757391 | max loss: 0.16657232 | min loss: 0.061490368\n",
      "Epoach: 183 | avg loss: 0.13866308 | max loss: 0.1737435 | min loss: 0.10962747\n",
      "Epoach: 184 | avg loss: 0.13055918 | max loss: 0.16869166 | min loss: 0.0711484\n",
      "Epoach: 185 | avg loss: 0.104478486 | max loss: 0.14919567 | min loss: 0.073487304\n",
      "Epoach: 186 | avg loss: 0.11458882 | max loss: 0.17993923 | min loss: 0.053752534\n",
      "Epoach: 187 | avg loss: 0.11970785 | max loss: 0.18487547 | min loss: 0.077528276\n",
      "Epoach: 188 | avg loss: 0.11617205 | max loss: 0.15852627 | min loss: 0.07468023\n",
      "Epoach: 189 | avg loss: 0.09444153 | max loss: 0.11352303 | min loss: 0.08031893\n",
      "Epoach: 190 | avg loss: 0.10527942 | max loss: 0.15477467 | min loss: 0.062117264\n",
      "Epoach: 191 | avg loss: 0.11661269 | max loss: 0.13732353 | min loss: 0.09819826\n",
      "Epoach: 192 | avg loss: 0.107770994 | max loss: 0.14458206 | min loss: 0.075111516\n",
      "Epoach: 193 | avg loss: 0.13401124 | max loss: 0.14654237 | min loss: 0.101624765\n",
      "Epoach: 194 | avg loss: 0.112958066 | max loss: 0.14322403 | min loss: 0.086843975\n",
      "Epoach: 195 | avg loss: 0.119806625 | max loss: 0.16294594 | min loss: 0.084775224\n",
      "Epoach: 196 | avg loss: 0.120760374 | max loss: 0.14149414 | min loss: 0.083352\n",
      "Epoach: 197 | avg loss: 0.10520836 | max loss: 0.14665222 | min loss: 0.06469825\n",
      "Epoach: 198 | avg loss: 0.1084027 | max loss: 0.14358954 | min loss: 0.06985003\n",
      "Epoach: 199 | avg loss: 0.114197634 | max loss: 0.15991473 | min loss: 0.07520911\n",
      "Epoach: 200 | avg loss: 0.10492382 | max loss: 0.15523131 | min loss: 0.072796576\n",
      "Epoach: 201 | avg loss: 0.12303525 | max loss: 0.1493389 | min loss: 0.09542868\n",
      "Epoach: 202 | avg loss: 0.10416442 | max loss: 0.14659397 | min loss: 0.067191385\n",
      "Epoach: 203 | avg loss: 0.12344153 | max loss: 0.17316853 | min loss: 0.088816576\n",
      "Epoach: 204 | avg loss: 0.12424375 | max loss: 0.16472733 | min loss: 0.10050802\n",
      "Epoach: 205 | avg loss: 0.106951326 | max loss: 0.14351846 | min loss: 0.079223655\n",
      "Epoach: 206 | avg loss: 0.11699806 | max loss: 0.16703494 | min loss: 0.08117723\n",
      "Epoach: 207 | avg loss: 0.0951813 | max loss: 0.14940201 | min loss: 0.07426901\n",
      "Epoach: 208 | avg loss: 0.11451972 | max loss: 0.17207997 | min loss: 0.06433757\n",
      "Epoach: 209 | avg loss: 0.13291 | max loss: 0.19992332 | min loss: 0.08637789\n",
      "Epoach: 210 | avg loss: 0.12175794 | max loss: 0.14802535 | min loss: 0.098667756\n",
      "Epoach: 211 | avg loss: 0.11727217 | max loss: 0.17722708 | min loss: 0.061166197\n",
      "Epoach: 212 | avg loss: 0.117636725 | max loss: 0.15565184 | min loss: 0.06225428\n",
      "Epoach: 213 | avg loss: 0.11826941 | max loss: 0.16345349 | min loss: 0.09401316\n",
      "Epoach: 214 | avg loss: 0.1290701 | max loss: 0.16884302 | min loss: 0.088032596\n",
      "Epoach: 215 | avg loss: 0.12080053 | max loss: 0.15512854 | min loss: 0.09917248\n",
      "Epoach: 216 | avg loss: 0.11155695 | max loss: 0.12991287 | min loss: 0.08081478\n",
      "Epoach: 217 | avg loss: 0.11507916 | max loss: 0.144011 | min loss: 0.07940797\n",
      "Epoach: 218 | avg loss: 0.11718708 | max loss: 0.14289516 | min loss: 0.071847714\n",
      "Epoach: 219 | avg loss: 0.10799653 | max loss: 0.14443253 | min loss: 0.07217087\n",
      "Epoach: 220 | avg loss: 0.1335946 | max loss: 0.16691649 | min loss: 0.09280909\n",
      "Epoach: 221 | avg loss: 0.14142254 | max loss: 0.21940087 | min loss: 0.08736322\n",
      "Epoach: 222 | avg loss: 0.104749665 | max loss: 0.1512545 | min loss: 0.05527108\n",
      "Epoach: 223 | avg loss: 0.11842991 | max loss: 0.14986642 | min loss: 0.090344146\n",
      "Epoach: 224 | avg loss: 0.11505957 | max loss: 0.1683234 | min loss: 0.079695\n",
      "Epoach: 225 | avg loss: 0.11308734 | max loss: 0.1730702 | min loss: 0.07417799\n",
      "Epoach: 226 | avg loss: 0.14186764 | max loss: 0.17312224 | min loss: 0.11032556\n",
      "Epoach: 227 | avg loss: 0.11562839 | max loss: 0.15921995 | min loss: 0.08292796\n",
      "Epoach: 228 | avg loss: 0.12553455 | max loss: 0.15673241 | min loss: 0.107771024\n",
      "Epoach: 229 | avg loss: 0.118998006 | max loss: 0.16905858 | min loss: 0.06774569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 | loss: 0.11017014 | current max loss: 0.11017014 | current min loss: 0.11017014 | time: 7.198953151702881\n",
      "Run for the test set\n",
      "TEST set, len  542\n",
      "MAP @10 bioASQ: 0.15984449130205572\n",
      "MAP @25: 0.23606571372261492\n",
      "MAP @50: 0.27730569437429264\n",
      "MAP @100: 0.29791470334666664\n",
      "MAP @200: 0.306586992229299\n",
      "MAP @300: 0.30988258322266754\n",
      "MAP: 0.2777642714465491\n",
      "RECALL@10: 0.3993093253566211\n",
      "RECALL@50: 0.6919821718259653\n",
      "RECALL@100: 0.7841027630008729\n",
      "Step: 1 | loss: 0.13290612 | current max loss: 0.13290612 | current min loss: 0.11017014 | time: 7.3045594692230225\n",
      "Run for the test set\n",
      "TEST set, len  542\n",
      "MAP @10 bioASQ: 0.16175386575294323\n",
      "MAP @25: 0.23877232985998448\n",
      "MAP @50: 0.279995892313945\n",
      "MAP @100: 0.3006779991899093\n",
      "MAP @200: 0.30938276512018537\n",
      "MAP @300: 0.3124854489296801\n",
      "MAP: 0.27772919079815606\n",
      "RECALL@10: 0.40005204504457903\n",
      "RECALL@50: 0.6942410418181969\n",
      "RECALL@100: 0.7835968498027603\n",
      "Step: 2 | loss: 0.07845879 | current max loss: 0.13290612 | current min loss: 0.07845879 | time: 7.303868055343628\n",
      "Run for the test set\n",
      "TEST set, len  542\n",
      "MAP @10 bioASQ: 0.1613373455163123\n",
      "MAP @25: 0.23846586801448325\n",
      "MAP @50: 0.279978418765104\n",
      "MAP @100: 0.3004636794506387\n",
      "MAP @200: 0.30924347254753426\n",
      "MAP @300: 0.31236016287733\n",
      "MAP: 0.2775077401703811\n",
      "RECALL@10: 0.3992204122964334\n",
      "RECALL@50: 0.6940752575516586\n",
      "RECALL@100: 0.7834255601480759\n",
      "Step: 3 | loss: 0.11763516 | current max loss: 0.13290612 | current min loss: 0.07845879 | time: 7.279842376708984\n",
      "Run for the test set\n",
      "TEST set, len  542\n",
      "MAP @10 bioASQ: 0.16164162713055702\n",
      "MAP @25: 0.2377010033066851\n",
      "MAP @50: 0.27969673032222697\n",
      "MAP @100: 0.3003207660474627\n",
      "MAP @200: 0.3089268438458655\n",
      "MAP @300: 0.3120612848563396\n",
      "MAP: 0.2791767071933785\n",
      "RECALL@10: 0.4010479302638446\n",
      "RECALL@50: 0.6935706556574737\n",
      "RECALL@100: 0.7842148980415133\n",
      "Step: 5 | loss: 0.09737709 | current max loss: 0.13290612 | current min loss: 0.07845879 | time: 7.234145641326904\n",
      "Run for the test set\n",
      "TEST set, len  542\n",
      "MAP @10 bioASQ: 0.16217785392139633\n",
      "MAP @25: 0.23870087589303493\n",
      "MAP @50: 0.2802366159179873\n",
      "MAP @100: 0.30099127117433844\n",
      "MAP @200: 0.3096647696386653\n",
      "MAP @300: 0.31269349853181366\n",
      "MAP: 0.27850809910056995\n",
      "RECALL@10: 0.40265056250702635\n",
      "RECALL@50: 0.6947928133133555\n",
      "RECALL@100: 0.7847253726011463\n",
      "Step: 6 | loss: 0.11668097 | current max loss: 0.13290612 | current min loss: 0.07845879 | time: 7.195175647735596\n",
      "Run for the test set\n",
      "TEST set, len  542\n",
      "MAP @10 bioASQ: 0.16263127452703088\n",
      "MAP @25: 0.23964613694108033\n",
      "MAP @50: 0.2809354590318845\n",
      "MAP @100: 0.3015976498541675\n",
      "MAP @200: 0.3102829662345154\n",
      "MAP @300: 0.31335684334419567\n",
      "MAP: 0.27974583371262846\n",
      "RECALL@10: 0.40158078532959124\n",
      "RECALL@50: 0.6949494551048231\n",
      "RECALL@100: 0.7847381852292725\n",
      "Step: 7 | loss: 0.11858338 | current max loss: 0.13290612 | current min loss: 0.07845879 | time: 7.335415840148926\n",
      "Run for the test set\n",
      "Predict query: 236\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0ae88280fa04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Run for the test set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtest_query_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_articles_collection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mbio_map_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_query_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ff3fef10a7fd>\u001b[0m in \u001b[0;36mvalidate_test_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predict query:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdeep_ranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_score_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mdeep_ranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdeep_ranking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbm25_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bioasq_data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"documents\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1476\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m       return training_arrays.predict_loop(\n\u001b[0;32m-> 1478\u001b[0;31m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, inputs, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen = TrainDataGenerator(train_articles_collection, tk, 256)\n",
    "\n",
    "gen_iter = iter(gen)\n",
    "\n",
    "\n",
    "for i,line in enumerate(loss):\n",
    "    \n",
    "    print(\"Epoach:\",i,\"| avg loss:\",np.mean(loss[i]),\"| max loss:\",np.max(loss[i]),\"| min loss:\",np.min(loss[i]))\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "for epoach in range(250,300):\n",
    "    loss_per_epoach = []\n",
    "    for step in range(len(gen)):\n",
    "        X = next(gen_iter)\n",
    "        \n",
    "        start = time.time()\n",
    "        loss_per_epoach.append(deepRank_model.train_on_batch(X))\n",
    "        print(\"Step:\",step,\"| loss:\",loss_per_epoach[-1],\"| current max loss:\",np.max(loss_per_epoach),\"| current min loss:\",np.min(loss_per_epoach),\"| time:\",time.time()-start,end=\"\\r\")\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Run for the test set\")\n",
    "        test_query_results = validate_test_data(test_articles_collection)\n",
    "        bio_map_test, _, _ = validation_score(test_query_results)\n",
    "\n",
    "        if bio_map_test >= max_bio_map_test:\n",
    "            max_bio_map_test = bio_map_test\n",
    "\n",
    "            deepRank_model.save_weights(\"deep_rank_v6_weights.h5\")\n",
    "                #deepRank_model.save(\"deep_rank_model.h5\")\n",
    "                \n",
    "    loss.append(loss_per_epoach)\n",
    "    print(\"\",end=\"\\r\")#clear the line\n",
    "    print(\"Epoach:\",epoach,\"| avg loss:\",np.mean(loss[-1]),\"| max loss:\",np.max(loss[-1]),\"| min loss:\",np.min(loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = \"/backup/results/deep_rank\"\n",
    "path_save = os.path.join(path_save, \"deep_rank_v2_17_1_test_data.p\")\n",
    "\n",
    "with open(path_save, \"wb\") as f:\n",
    "    pickle.dump(test_query_results,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE SUBMISSION\n",
    "\n",
    "\n",
    "test_bioASQ_results_results = list(map(lambda k:{\"id\":k[0],\"documents\":list(map(lambda x:\"http://www.ncbi.nlm.nih.gov/pubmed/\"+str(x[0]), k[1][\"result\"]))[:10]}, test_bioASQ_results.items()))\n",
    "_temp = []\n",
    "\n",
    "for query in bioASQ_data:\n",
    "    _jump = False\n",
    "    for r_query in test_bioASQ_results_results:\n",
    "        if query[\"id\"] == r_query[\"id\"]:\n",
    "            _jump = True\n",
    "    \n",
    "    #no match so add\n",
    "    if not _jump:\n",
    "        _temp.append({\"id\":query[\"id\"],\"documents\":[]})\n",
    "\n",
    "test_bioASQ_results_results.extend(_temp)\n",
    "\n",
    "print(len(test_bioASQ_results_results))\n",
    "assert len(test_bioASQ_results_results) == 100\n",
    "a = {\"questions\": test_bioASQ_results_results}\n",
    "with open(\"5b_phaseA_01.json\",\"w\") as f:\n",
    "    json.dump(a,f)\n",
    "    \n",
    "\n",
    "test_bioASQ_results_results[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_to_test_index = 0\n",
    "\n",
    "data_generator = TestDataGenerator(test_articles_collection, tk)\n",
    "data_generator = iter(data_generator)\n",
    "for _ in range(query_to_test_index+1):\n",
    "    X = next(data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_ranking = document_score_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_ranking = map(lambda x:x[0],re_ranking.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_results = test_articles_collection[\"bioasq_data\"][query_to_test_index][\"documents\"]\n",
    "positive_docs = test_articles_collection[\"bioasq_data\"][query_to_test_index][\"positive_pmid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_ranking_pmid = list(zip(bm25_results,re_ranking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_ranking_pmid.sort(key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('24794627', 5.3322014808654785),\n",
       " ('30251567', 5.313037872314453),\n",
       " ('28796422', 5.227417945861816),\n",
       " ('30114722', 5.093368053436279),\n",
       " ('29947303', 5.0901007652282715),\n",
       " ('30697454', 5.0804266929626465),\n",
       " ('30569414', 4.911670207977295),\n",
       " ('28901190', 4.814671039581299),\n",
       " ('24577791', 4.803395748138428),\n",
       " ('26907255', 4.67585563659668)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_ranking_pmid[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13502,    43,   478,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_articles_collection[\"bioasq_data\"][query_to_test_index][\"query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['24554704',\n",
       " '24784583',\n",
       " '24577791',\n",
       " '23197849',\n",
       " '24035588',\n",
       " '21060967',\n",
       " '25479728',\n",
       " '21755313',\n",
       " '24469711',\n",
       " '22512788',\n",
       " '24911883',\n",
       " '24794627',\n",
       " '21464439',\n",
       " '25059784']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '24794627', 5.3322014808654785), (8, '24577791', 4.803395748138428), (11, '24554704', 4.6346845626831055), (12, '23197849', 4.612618446350098), (16, '21060967', 4.446898460388184), (18, '24469711', 4.439567565917969), (19, '21755313', 4.431821823120117), (25, '22512788', 4.27816104888916), (34, '21464439', 4.063064098358154), (35, '25059784', 4.056085586547852), (38, '24784583', 4.003556728363037), (43, '25479728', 3.937878131866455), (50, '24035588', 3.7958528995513916), (343, '24911883', 2.838761806488037)]\n",
      "[(3, '23197849'), (4, '21755313'), (6, '25479728'), (7, '24784583'), (8, '24577791'), (9, '24035588'), (15, '22512788'), (17, '24911883'), (19, '21464439'), (22, '24794627'), (126, '25059784'), (134, '24554704'), (466, '21060967'), (2793, '24469711')]\n"
     ]
    }
   ],
   "source": [
    "positive_docs_ranked = []\n",
    "for i,result in enumerate(re_ranking_pmid):\n",
    "    if result[0] in set(positive_docs):\n",
    "        positive_docs_ranked.append((i,result[0],result[1]))\n",
    "        \n",
    "true_ranked = []\n",
    "for i,pmid in enumerate(bm25_results):\n",
    "    if pmid in set(positive_docs):\n",
    "        true_ranked.append((i,pmid))\n",
    "\n",
    "print(positive_docs_ranked)\n",
    "print(true_ranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with train set, check overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokens = np.array([X[0][0]])\n",
    "snippet_list = np.array([X[1][0]])\n",
    "\n",
    "query_tokens = X[0][:2]\n",
    "snippet_list = X[1][:2]\n",
    "\n",
    "model_input = [query_tokens, snippet_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 15)\n",
      "(2, 15, 3, 15)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(query_tokens.shape)\n",
    "print(snippet_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 15, 3, 15, 15, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = input_model.predict(model_input)\n",
    "np.array(matrix).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[0][4][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(matrix[0][7][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  165, 13502,    26,    61,     8,     1,    43,     2,   478],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  363,     5,  2386,    97,  7598,   774,    32, 13502,    18,\n",
       "          15,     7, 11695,   117,    17,     0], dtype=int32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_list[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05703647,  0.19594026,  0.03365219,  0.15514491,  0.00540348,\n",
       "       -0.02335026, -0.06095085,  0.0226689 , -0.05668721,  0.01571985,\n",
       "       -0.09896637,  0.13836679,  0.02710932,  0.06420047, -0.03692323,\n",
       "        0.03899341,  0.00553868, -0.08639584, -0.05358738, -0.02609682,\n",
       "        0.06495432, -0.00129713, -0.01882407, -0.10850747, -0.02421302,\n",
       "        0.05556208,  0.00291283, -0.04882976,  0.01770345,  0.0035051 ,\n",
       "        0.07192209, -0.00432884, -0.15161929, -0.07024549, -0.04793473,\n",
       "        0.01823143,  0.10337584, -0.04076301,  0.01026187,  0.12004871,\n",
       "        0.03939956, -0.03548966, -0.10689223, -0.16337523,  0.10883316,\n",
       "        0.01135785,  0.03041399,  0.06011688, -0.09919181,  0.01741308,\n",
       "       -0.04328503, -0.00256405, -0.11370766,  0.0522779 ,  0.0702537 ,\n",
       "        0.01021139,  0.06773005,  0.01114117, -0.05878652,  0.0720681 ,\n",
       "        0.05551391,  0.08731035,  0.07339004,  0.0031227 ,  0.10792159,\n",
       "        0.12050318, -0.05851915, -0.08350374, -0.03341928,  0.12355518,\n",
       "        0.11631501,  0.09690028, -0.02127477,  0.05462002, -0.03942551,\n",
       "        0.0049153 , -0.12599072,  0.04783105, -0.00842254,  0.06550868,\n",
       "       -0.06309314, -0.04500391, -0.08562729,  0.0129618 ,  0.05385976,\n",
       "        0.004466  ,  0.0030678 ,  0.02940105, -0.01520511,  0.04611286,\n",
       "        0.03223389,  0.01480899,  0.06665637, -0.00282597, -0.14461097,\n",
       "       -0.16931795,  0.01149627,  0.09099851, -0.06041984, -0.04964583,\n",
       "        0.04263903, -0.02741712,  0.04248934, -0.04117119,  0.1337102 ,\n",
       "       -0.01596204,  0.05762576,  0.00730769,  0.06105822,  0.02038225,\n",
       "        0.06695575,  0.03489211,  0.01739459, -0.00652035, -0.05772134,\n",
       "        0.07827891,  0.03879946,  0.13483463,  0.00304335, -0.07165361,\n",
       "        0.06038161, -0.05664064, -0.10519326, -0.00272405, -0.07754581,\n",
       "        0.06773265,  0.04561247,  0.07964371, -0.07700854,  0.08914506,\n",
       "        0.17518474,  0.01715964,  0.02536894, -0.02523981, -0.10770504,\n",
       "       -0.15260579, -0.00970635,  0.02915863,  0.06111136,  0.00900377,\n",
       "        0.04059591,  0.07735578, -0.04844462,  0.14468352,  0.03376452,\n",
       "       -0.06587124,  0.08225945,  0.02598596,  0.03487838,  0.04218515,\n",
       "        0.07036549, -0.01025326, -0.02166397, -0.06253622,  0.06407471,\n",
       "       -0.123892  , -0.01470529,  0.00293271,  0.05741141, -0.0694873 ,\n",
       "       -0.11187772, -0.07452565,  0.04860114, -0.06068123, -0.03620556,\n",
       "       -0.00046228, -0.05690295, -0.04587905,  0.04772501, -0.04207376,\n",
       "       -0.04783105,  0.05724974, -0.2321706 ,  0.08884832,  0.00071572,\n",
       "        0.05809747, -0.10940351,  0.01475556,  0.01292174, -0.07808141,\n",
       "        0.02383626,  0.06215472,  0.01264787,  0.04288376,  0.06052897,\n",
       "       -0.02552988,  0.05216181,  0.0164598 , -0.03271155, -0.05829453,\n",
       "       -0.09264817,  0.16744114, -0.01112105,  0.07499924,  0.05138046,\n",
       "       -0.00072611, -0.03267605, -0.00081905, -0.02593878, -0.10709276],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dict[13502]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\r"
     ]
    }
   ],
   "source": [
    "data_generator = TrainDataGenerator(train_articles_collection, tk, 256)\n",
    "data_generator = iter(data_generator)\n",
    "for i in range(9):\n",
    "    print(i,end=\"\\r\")\n",
    "    X,Y = next(data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: len(list(filter(lambda y:y!=0,x))),X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 988,  988,  279, 1208,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  1075,      2,    986,      4,      6,    248,    988,   2250,\n",
       "             44,    713,   6300,   6300,      0],\n",
       "        [  3974,      2,    986,      4,      6,    248,    988,   2250,\n",
       "             18,    713,   6300,   6300,      0],\n",
       "        [    17,     17,  46298,  59753,   9143,   2106,    988,   2250,\n",
       "             16,      6,    200,    174,      0]],\n",
       "\n",
       "       [[  1075,      2,    986,      4,      6,    248,    988,   2250,\n",
       "             44,    713,   6300,   6300,      0],\n",
       "        [  3974,      2,    986,      4,      6,    248,    988,   2250,\n",
       "             18,    713,   6300,   6300,      0],\n",
       "        [    17,     17,  46298,  59753,   9143,   2106,    988,   2250,\n",
       "             16,      6,    200,    174,      0]],\n",
       "\n",
       "       [[     2,    248,    986,      4,      1,  10169,    279,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[  1063,   2870,   1361,      7,      6,   6336,   1208,    544,\n",
       "          22931,    137,     27,    587,      0],\n",
       "        [     1,    102,      2,      6,    134,   6336,   1208,    544,\n",
       "          22931, 551603,    137,      8,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]]], dtype=int32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3][26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos,neg = deepRank_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'bioasq_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-10cc45d13ac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery_to_test_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_articles_collection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_to_test_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-219-bffc59abe895>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, article_collection, tokenizer, article_map)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_collection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bioasq_data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_collection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"collection\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'bioasq_data'"
     ]
    }
   ],
   "source": [
    "query_to_test_index = 1\n",
    "\n",
    "data_generator = TestDataGenerator(train_articles_collection, tk)\n",
    "data_generator = iter(data_generator)\n",
    "for _ in range(query_to_test_index+1):\n",
    "    X = next(data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('26671317', 7.931817054748535),\n",
       " ('20975159', 7.902041435241699),\n",
       " ('20650709', 7.8478264808654785),\n",
       " ('19805301', 7.842199802398682),\n",
       " ('21731768', 7.818233013153076),\n",
       " ('24681619', 7.759010314941406),\n",
       " ('26631348', 7.714381217956543),\n",
       " ('22196114', 7.696432590484619),\n",
       " ('23817568', 7.648404598236084),\n",
       " ('26410599', 7.3919854164123535)]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_ranking = document_score_model.predict(X)\n",
    "\n",
    "bm25_results = train_articles_collection[\"test_data\"][query_to_test_index][\"documents\"]\n",
    "positive_docs = train_articles_collection[\"test_data\"][query_to_test_index][\"positive_pmid\"]\n",
    "\n",
    "re_ranking_pmid = list(zip(bm25_results,re_ranking.tolist()))\n",
    "\n",
    "re_ranking_pmid.sort(key=lambda x:-x[1])\n",
    "\n",
    "re_ranking_pmid[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kind enzyme encoded proto oncogene abl1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['21435002',\n",
       " '20841568',\n",
       " '9500553',\n",
       " '24012954',\n",
       " '18796434',\n",
       " '23842646',\n",
       " '18528425']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tk.sequences_to_texts([train_articles_collection[\"test_data\"][query_to_test_index][\"query\"]]))\n",
    "positive_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, '24012954', 5.5485920906066895), (29, '9500553', 4.8889594078063965), (87, '21435002', 4.592401504516602), (112, '23842646', 4.525805473327637), (155, '18796434', 4.400295257568359), (342, '18528425', 4.139955043792725), (2244, '20841568', 3.1609909534454346)]\n",
      "[(1, '9500553'), (29, '21435002'), (187, '24012954'), (309, '18528425'), (333, '23842646'), (610, '20841568'), (2354, '18796434')]\n"
     ]
    }
   ],
   "source": [
    "positive_docs_ranked = []\n",
    "for i,result in enumerate(re_ranking_pmid):\n",
    "    if result[0] in set(positive_docs):\n",
    "        positive_docs_ranked.append((i,result[0],result[1]))\n",
    "        \n",
    "true_ranked = []\n",
    "for i,pmid in enumerate(bm25_results):\n",
    "    if pmid in set(positive_docs):\n",
    "        true_ranked.append((i,pmid))\n",
    "\n",
    "print(positive_docs_ranked)\n",
    "print(true_ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
