{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "import os\n",
    "os.chdir(\"/home/tiagoalmeida/bioASQ-taskb/\")\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from bisect import bisect\n",
    "\n",
    "\n",
    "##add keras to the modules\n",
    "module_path = os.path.abspath(os.path.join('pubmed_data'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from pubmed_data import pubmed_helper as ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepRank\n",
    "Reference PAPER :https://arxiv.org/pdf/1710.05649.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network structure\n",
    " - [General Network Configuration](#var_def)\n",
    " - [Input Network](#input_net)\n",
    " - [Measure Network](#measure_net)\n",
    " - [Aggregation Network](#aggreation_net)\n",
    " - [Final Network](#final_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load regex_full_tokens_tokenizer.p\n",
      "Load regex_full_tokens_word_embedding.p\n"
     ]
    }
   ],
   "source": [
    "#Load tokenizer and the embedding matrix\n",
    "\n",
    "MODE = \"regex_full_tokens\"\n",
    "tk = ph.load_tokenizer(mode=MODE)\n",
    "emb_dict = ph.load_embeddings(mode=MODE)\n",
    "\n",
    "assert len(tk.word_counts) == len(emb_dict)\n",
    "\n",
    "#Number of different words\n",
    "VOCAB_SIZE = len(tk.word_counts)+1\n",
    "\n",
    "#Dimension of embeddings\n",
    "EMB_DIM = emb_dict[1].shape[0]\n",
    "\n",
    "emb_matrix = np.zeros((VOCAB_SIZE, EMB_DIM))\n",
    "\n",
    "for i,vector in emb_dict.items():\n",
    "    emb_matrix[i] = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenized stopwords\n",
    "\n",
    "biomedical_stop_words = [\"a\", \"about\", \"again\", \"all\", \"almost\", \"also\", \"although\", \"always\", \"among\", \"an\", \"and\", \"another\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"between\", \"both\", \"but\", \"by\", \"can\", \"could\", \"did\", \"do\", \"does\", \"done\", \"due\", \"during\", \"each\", \"either\", \"enough\", \"especially\", \"etc\", \"for\", \"found\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"here\", \"how\", \"however\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"just\", \"kg\", \"km\", \"made\", \"mainly\", \"make\", \"may\", \"mg\", \"might\", \"ml\", \"mm\", \"most\", \"mostly\", \"must\", \"nearly\", \"neither\", \"no\", \"nor\", \"obtained\", \"of\", \"often\", \"on\", \"our\", \"overall\", \"perhaps\", \"pmid\", \"quite\", \"rather\", \"really\", \"regarding\", \"seem\", \"seen\", \"several\", \"should\", \"show\", \"showed\", \"shown\", \"shows\", \"significantly\", \"since\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"then\", \"there\", \"therefore\", \"these\", \"they\", \"this\", \"those\", \"through\", \"thus\", \"to\", \"upon\", \"use\", \"used\", \"using\", \"various\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"which\", \"while\", \"with\", \"within\", \"without\", \"would\"]\n",
    "biomedical_stop_words_tokens = set(tk.texts_to_sequences([biomedical_stop_words])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='var_def'></a>\n",
    "## General Network Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import unstack, stack\n",
    "##Test \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, activations\n",
    "from tensorflow.keras.initializers import Zeros, Ones, Constant\n",
    "from tensorflow.keras.layers import Dense, Lambda, Bidirectional, Dot,Masking,Reshape, Concatenate, Layer, Embedding, Input, Conv2D, GlobalMaxPooling2D, Flatten, TimeDistributed, GRU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.activations import tanh, sigmoid\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "#Number max of term per query\n",
    "MAX_Q_TERM = 13\n",
    "\n",
    "#Number max of the snippet terms\n",
    "QUERY_CENTRIC_CONTEX = 15\n",
    "\n",
    "#Number max of passages per query term\n",
    "MAX_PASSAGES_PER_QUERY = 5\n",
    "\n",
    "#Snippet position padding value\n",
    "SNIPPET_POSITION_PADDING_VALUE = -1\n",
    "\n",
    "#Mode for the creation of the S matrix\n",
    "S_MATRIX_MODE = 0\n",
    "#S_MATRIX_DIMENSION = EMB_DIM*2+1\n",
    "\n",
    "#Train embedding weights\n",
    "EMB_TRAINABLE = False\n",
    "\n",
    "#Number of filters in CNN\n",
    "CNN_FILTERS = 100\n",
    "CNN_KERNELS = (3,3)\n",
    "\n",
    "#RNN DIM\n",
    "USE_BIDIRECTIONAL = False\n",
    "GRU_REPRESENTATION_DIM = 56\n",
    "\n",
    "ACTIVATION_FUNCTION = \"selu\"\n",
    "\n",
    "REGULARIZATION = regularizers.l2(0.0001)\n",
    "\n",
    "#Term gating network mode\n",
    "TERM_GATING_MODE =  3#2- weigt fixed per position, 1 - DRMM like term gating\n",
    "\n",
    "assert S_MATRIX_MODE in [0,1]\n",
    "assert TERM_GATING_MODE in [0,1,2,3]\n",
    "\n",
    "#MACRO STYLE\n",
    "def S_MATRIX_3D_DIMENSION():\n",
    "    if S_MATRIX_MODE==0:\n",
    "        return 1\n",
    "    elif S_MATRIX_MODE==1:\n",
    "        return EMB_DIM*2+1\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='input_net'></a>\n",
    "## Input Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "snippet_emb_model summary\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "snippet_token (InputLayer)   (None, 13, 5, 15)         0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  (None, 13, 5, 15, 200)    858359000 \n",
      "_________________________________________________________________\n",
      "snippet_transpose (Lambda)   (None, 13, 5, 200, 15)    0         \n",
      "=================================================================\n",
      "Total params: 858,359,000\n",
      "Trainable params: 0\n",
      "Non-trainable params: 858,359,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "  ---- Custom Layers ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "class SimilarityMatrix(Layer):\n",
    "    \n",
    "    def __init__(self, query_max_term, snippet_max_term, interaction_mode=0, **kwargs):\n",
    "        \"\"\"\n",
    "        interaction mode 0: only use similarity matrix\n",
    "                    mode 1: similarity matrix + query and snippet embeddings\n",
    "        \"\"\"\n",
    "        assert interaction_mode in [0,1] #only valid modes\n",
    "        \n",
    "        self.query_max_term = query_max_term\n",
    "        self.snippet_max_term = snippet_max_term\n",
    "        self.interaction_mode = interaction_mode\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self,x):\n",
    "        if self.interaction_mode==0:\n",
    "            #sim => dot product (None, MAX_Q_TERM, EMB_DIM) x (None, MAX_Q_TERM, MAX_PASSAGE_PER_Q, EMB_DIM, QUERY_CENTRIC_CONTEX)\n",
    "            query = K.expand_dims(x[0], axis=1) #(None, 1, MAX_Q_TERM, EMB_DIM)\n",
    "            query = K.expand_dims(query, axis=1) #(None, 1, 1, MAX_Q_TERM, EMB_DIM)\n",
    "            query = K.repeat_elements(query,x[1].shape[1],axis=1) #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, EMB_DIM)\n",
    "            query = K.repeat_elements(query,x[1].shape[2],axis=2)\n",
    "            s_matrix = K.batch_dot(query,x[1]) #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, EMB_DIM)\n",
    "            \n",
    "            s_matrix = K.expand_dims(s_matrix)\n",
    "            \n",
    "            return s_matrix #Add one more dimension #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, #(None, MAX_PASSAGE_PER_Q, MAX_Q_TERM, EMB_DIM, 1)\n",
    "        elif self.interaction_mode==1:\n",
    "            raise NotImplementedError(\"interaction mode of layer SimilarityMatrix is not implemented\")\n",
    "    \"\"\"                  \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.interaction_mode==0:\n",
    "            return (input_shape[0][0], input_shape[0][1], self.query_max_term, self.snippet_max_term, 1)\n",
    "        elif self.interaction_mode==1:\n",
    "            return (input_shape[0][0], input_shape[0][1], self.query_max_term, self.snippet_max_term, input_shape[0][2]*input_shape[1][2]+1) \n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "     ---- Layers ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "#Embedding Layer\n",
    "embedding = Embedding(VOCAB_SIZE,EMB_DIM, name=\"embedding_layer\",weights=[emb_matrix], trainable=EMB_TRAINABLE)\n",
    "\n",
    "#S matrix ref in the paper\n",
    "similarity_matrix = SimilarityMatrix(MAX_Q_TERM, QUERY_CENTRIC_CONTEX, interaction_mode=S_MATRIX_MODE, name=\"query_snippet_similarity\")\n",
    "\n",
    "#transpose (None, QUERY_CENTRIC_CONTEX, EMB_DIM) => (None, EMB_DIM, QUERY_CENTRIC_CONTEX) \n",
    "transpose_layer = Lambda(lambda x:K.permute_dimensions(x,[0,1,2,4,3]), name=\"snippet_transpose\") \n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    " ---- Auxiliar Models ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "#Snippet single embedding transformation\n",
    "snippet_token_input = Input(shape = (MAX_Q_TERM, MAX_PASSAGES_PER_QUERY, QUERY_CENTRIC_CONTEX,), name = \"snippet_token\")\n",
    "snippet_emb = embedding(snippet_token_input)\n",
    "snippet_emb_transpose = transpose_layer(snippet_emb)\n",
    "snippet_emb_model = Model(inputs = [snippet_token_input], outputs=[snippet_emb_transpose], name = \"snippet_emb_model\")\n",
    "print(\"\\n\\nsnippet_emb_model summary\")\n",
    "snippet_emb_model.summary()\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "  ---- Input Network ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "if DEBUG:\n",
    "    query_token_input = Input(shape=(MAX_Q_TERM,), name=\"query_tokens\")\n",
    "\n",
    "\n",
    "    snippets_tokens_input = Input(shape = (MAX_Q_TERM, MAX_PASSAGES_PER_QUERY, QUERY_CENTRIC_CONTEX), name = \"snippet_tokens_ipmodel\") \n",
    "    \n",
    "    query_emb = embedding(query_token_input)\n",
    "\n",
    "    snippet_emb = embedding(snippets_tokens_input)\n",
    "    snippet_emb_transpose = transpose_layer(snippet_emb)\n",
    "    \n",
    "    sim_matrix_layer = similarity_matrix([query_emb,snippet_emb_transpose])\n",
    "    \n",
    "    \n",
    "    \n",
    "    input_model = Model(inputs = [query_token_input,snippets_tokens_input], outputs=[sim_matrix_layer], name=\"input_model\")\n",
    "    print(\"\\n\\ninput_model summary\")\n",
    "    input_model.summary()\n",
    "    \n",
    "    \n",
    "    print(\"\\nOutput tensor\",sim_matrix_layer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='measure_net'></a>\n",
    "## Measure Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "\n",
      "\n",
      "cnn_extraction_model summary\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masked_conv2d (MaskedConv2D) (None, 56)                19056     \n",
      "=================================================================\n",
      "Total params: 19,056\n",
      "Trainable params: 19,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Tensor(\"time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri (None, 5, 56)             19056     \n",
      "=================================================================\n",
      "Total params: 19,056\n",
      "Trainable params: 19,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class FastMaskedConv2D(Layer):\n",
    "    \n",
    "    def __init__(self, filters, kernel_size, activation, initializer='glorot_normal', regularizer=None, **kargs):\n",
    "        super(FastMaskedConv2D, self).__init__(**kargs)\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        \n",
    "        if regularizer is None or isinstance(regularizer,str):\n",
    "            self.regularizer = regularizers.get(regularizer)\n",
    "        else:\n",
    "            self.regularizer = regularizer\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        #self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_filter = int(input_shape[-1])\n",
    "        \n",
    "        self.kernel_3_3 = self.add_variable(name = \"conv_kernel_3_3\",\n",
    "                                   shape = (3,3,input_filter,CNN_FILTERS),\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.kernel_5_1 = self.add_variable(name = \"conv_kernel_3_1\",\n",
    "                                   shape = (5,1,input_filter,CNN_FILTERS),\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.kernel_1_5 = self.add_variable(name = \"conv_kernel_1_3\",\n",
    "                                   shape = (1,5,input_filter,CNN_FILTERS),\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.kernel_3_3_bias = self.add_variable(name = \"conv_kernel_3_3_bias\",\n",
    "                                   shape = (CNN_FILTERS,),\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.kernel_5_1_bias = self.add_variable(name = \"conv_kernel_3_1_bias\",\n",
    "                                   shape = (CNN_FILTERS,),\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.kernel_1_5_bias = self.add_variable(name = \"conv_kernel_1_3_bias\",\n",
    "                                   shape = (CNN_FILTERS,),\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        #end dimensions = 7, 9, 100\n",
    "        \n",
    "        \n",
    "        self.dense = self.add_variable(name = \"dense_projection\",\n",
    "                                   shape = (CNN_FILTERS*3, GRU_REPRESENTATION_DIM),\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.dense_bias = self.add_variable(name = \"dense_projection_bias\",\n",
    "                                   shape = (GRU_REPRESENTATION_DIM,),\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "       \n",
    "        \n",
    "        super(FastMaskedConv2D, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        #condition = K.all(x) #if all the values are the same\n",
    "        #zero_out = K.repeat_elements(x[:,:self.convOutShape[1],:self.convOutShape[2],:],self.convOutShape[3],axis=-1)\n",
    "        \n",
    "        #conv\n",
    "        kernel_3_3 = K.conv2d(x, self.kernel_3_3)\n",
    "        kernel_3_3 = K.bias_add(kernel_3_3, self.kernel_3_3_bias)\n",
    "        kernel_3_3 = self.activation(kernel_3_3)\n",
    "        kernel_3_3_pool = K.pool2d(kernel_3_3,(11,13))\n",
    "        \n",
    "        kernel_5_1 = K.conv2d(x, self.kernel_5_1)\n",
    "        kernel_5_1 = K.bias_add(kernel_5_1, self.kernel_5_1_bias)\n",
    "        kernel_5_1 = self.activation(kernel_5_1)\n",
    "        print(kernel_5_1)\n",
    "        #kernel_5_1_pool = K.pool2d(kernel_5_1,(11,13))\n",
    "        \n",
    "        kernel_1_5 = K.conv2d(x, self.kernel_1_5)\n",
    "        kernel_1_5 = K.bias_add(kernel_1_5, self.kernel_1_5_bias)\n",
    "        kernel_1_5 = self.activation(kernel_1_5)\n",
    "        print(kernel_1_5)\n",
    "        #kernel_1_5_pool = K.pool2d(kernel_1_3,(11,13))\n",
    "        print(kernel_3_3_pool)\n",
    "        print(kernel_3_1_pool)\n",
    "        print(kernel_1_3_pool)\n",
    "        \n",
    "        kernel_3_3_flat = K.reshape(kernel_3_3_pool,(-1,CNN_FILTERS))\n",
    "        kernel_3_1_flat = K.reshape(kernel_3_1_pool,(-1,CNN_FILTERS))\n",
    "        kernel_1_3_flat = K.reshape(kernel_1_3_pool,(-1,CNN_FILTERS))\n",
    "        print(kernel_3_3_flat)\n",
    "        concat =  K.concatenate([kernel_3_3_flat,kernel_3_1_flat,kernel_1_3_flat])\n",
    "        print(concat)\n",
    "        \n",
    "        proj = K.dot(concat, self.dense)\n",
    "        proj = K.bias_add(proj,self.dense_bias)\n",
    "        proj = self.activation(proj)\n",
    "        return proj\n",
    "\n",
    "        #masked_conv = K.switch(condition, zero_out , self.conv2layer(x) )\n",
    "        \n",
    "        #return masked_conv\n",
    "    \n",
    "class MaskedConv2D(Layer):\n",
    "    \n",
    "    def __init__(self, filters, kernel_size, activation, initializer='glorot_normal', regularizer=None, **kargs):\n",
    "        super(MaskedConv2D, self).__init__(**kargs)\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        \n",
    "        if regularizer is None or isinstance(regularizer,str):\n",
    "            self.regularizer = regularizers.get(regularizer)\n",
    "        else:\n",
    "            self.regularizer = regularizer\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        input_filter = int(input_shape[-1])\n",
    "        \n",
    "        self.kernel_3_3 = self.add_variable(name = \"conv_kernel_3_3\",\n",
    "                                   shape = (3,3,input_filter,CNN_FILTERS),\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.kernel_5_1 = self.add_variable(name = \"conv_kernel_5_1\",\n",
    "                                   shape = (5,1,input_filter,CNN_FILTERS),\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.kernel_1_5 = self.add_variable(name = \"conv_kernel_1_5\",\n",
    "                                   shape = (1,5,input_filter,CNN_FILTERS),\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.kernel_3_3_bias = self.add_variable(name = \"conv_kernel_3_3_bias\",\n",
    "                                   shape = (self.filters,),)\n",
    "        \n",
    "        self.kernel_5_1_bias = self.add_variable(name = \"conv_kernel_5_1_bias\",\n",
    "                                   shape = (self.filters,),)\n",
    "        \n",
    "        self.kernel_1_5_bias = self.add_variable(name = \"conv_kernel_1_5_bias\",\n",
    "                                   shape = (self.filters,),)\n",
    "        \n",
    "        #end dimensions = 7, 9, 100\n",
    "        \n",
    "        \n",
    "        self.dense = self.add_variable(name = \"dense_projection\",\n",
    "                                   shape = (self.filters*3, GRU_REPRESENTATION_DIM),\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.dense_bias = self.add_variable(name = \"dense_projection_bias\",\n",
    "                                   shape = (GRU_REPRESENTATION_DIM,),)\n",
    "        \n",
    "        \n",
    "        super(MaskedConv2D, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        condition = K.all(x) #if all the values are the same\n",
    "        inv_condition = (1-K.cast(condition, K.floatx()))\n",
    "        \n",
    "        kernel_3_3 = K.conv2d(x, self.kernel_3_3)\n",
    "        kernel_3_3 = K.bias_add(kernel_3_3, self.kernel_3_3_bias)\n",
    "        kernel_3_3 = self.activation(kernel_3_3)\n",
    "        kernel_3_3_pool = K.pool2d(kernel_3_3,(11,13))\n",
    "        \n",
    "        kernel_5_1 = K.conv2d(x, self.kernel_5_1)\n",
    "        kernel_5_1 = K.bias_add(kernel_5_1, self.kernel_5_1_bias)\n",
    "        kernel_5_1 = self.activation(kernel_5_1)\n",
    "        kernel_5_1_pool = K.pool2d(kernel_5_1,(9,15))\n",
    "        \n",
    "        kernel_1_5 = K.conv2d(x, self.kernel_1_5)\n",
    "        kernel_1_5 = K.bias_add(kernel_1_5, self.kernel_1_5_bias)\n",
    "        kernel_1_5 = self.activation(kernel_1_5)\n",
    "        kernel_1_5_pool = K.pool2d(kernel_1_5,(13,11))\n",
    "        \n",
    "        print(kernel_3_3_pool)\n",
    "        print(kernel_5_1_pool)\n",
    "        print(kernel_1_5_pool)\n",
    "        \n",
    "        kernel_3_3_flat = K.reshape(kernel_3_3_pool,(-1,self.filters))\n",
    "        kernel_5_1_flat = K.reshape(kernel_5_1_pool,(-1,self.filters))\n",
    "        kernel_1_5_flat = K.reshape(kernel_1_5_pool,(-1,self.filters))\n",
    "        print(kernel_3_3_flat)\n",
    "        print(kernel_5_1_flat)\n",
    "        print(kernel_1_5_flat)\n",
    "        \n",
    "        concat =  K.concatenate([kernel_3_3_flat,kernel_5_1_flat,kernel_1_5_flat])\n",
    "        print(concat)\n",
    "        \n",
    "        proj = K.dot(concat, self.dense)\n",
    "        proj = K.bias_add(proj,self.dense_bias)\n",
    "        proj = self.activation(proj)\n",
    "\n",
    "        \n",
    "        return proj * inv_condition\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    " ---- Auxiliar Models ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "#measure_layer = MaskedConv2D(filters = CNN_FILTERS, kernel_size=CNN_KERNELS, activation=ACTIVATION_FUNCTION)\n",
    "\n",
    "cnn_extraction_model = Sequential(name=\"cnn_extraction_model\")\n",
    "cnn_extraction_model.add(MaskedConv2D(input_shape = (MAX_Q_TERM, QUERY_CENTRIC_CONTEX, S_MATRIX_3D_DIMENSION()), filters = CNN_FILTERS, kernel_size=CNN_KERNELS, activation=ACTIVATION_FUNCTION ))\n",
    "print(\"\\n\\ncnn_extraction_model summary\")\n",
    "cnn_extraction_model.summary()\n",
    "\n",
    "td_cnn_extraction_model = Sequential(name=\"TD_cnn_extraction_model\")\n",
    "td_cnn_extraction_model.add(TimeDistributed(cnn_extraction_model, input_shape=(MAX_PASSAGES_PER_QUERY, MAX_Q_TERM, QUERY_CENTRIC_CONTEX, S_MATRIX_3D_DIMENSION())))\n",
    "td_cnn_extraction_model.summary()\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "     ---- Layers ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "#concatenation layer over the last dimension\n",
    "concat_snippet_position = Concatenate( name = \"concat_snippet_position\")\n",
    "\n",
    "#RNN using GRU units\n",
    "if USE_BIDIRECTIONAL:\n",
    "    rnn_instance = GRU(GRU_REPRESENTATION_DIM, kernel_regularizer=REGULARIZATION, activation=ACTIVATION_FUNCTION, name=\"aggregation_snippet_by_q_term\")\n",
    "    gru = Bidirectional(rnn_instance, merge_mode=\"concat\")\n",
    "else:\n",
    "    gru = GRU(GRU_REPRESENTATION_DIM, kernel_regularizer=REGULARIZATION, activation=ACTIVATION_FUNCTION, name=\"aggregation_snippet_by_q_term\")\n",
    "    \n",
    "#add dimension Layer\n",
    "add_passage_dim = Lambda(lambda x:K.expand_dims(x,axis=1), name=\"add_passage_dim\")#Reshape(target_shape=(1,GRU_REPRESENTATION_DIM))\n",
    "\n",
    "#add last dimension Layer\n",
    "add_dim = Lambda(lambda x:K.expand_dims(x), name=\"add_dim\")\n",
    "\n",
    "#reciprocal function\n",
    "reciprocal_f = Lambda(lambda x:1/(x+2), name=\"reciprocal_function\")\n",
    "\n",
    "#concatenation layer over second dimension (passage dimension)\n",
    "concat_representation = Concatenate(axis = 1,name = \"concat_representation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='measure_net'></a>\n",
    "## Aggregation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "  ---- Custom Layers ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "if USE_BIDIRECTIONAL:\n",
    "    snippet_rnn_rep_dim = GRU_REPRESENTATION_DIM*2\n",
    "else:\n",
    "    snippet_rnn_rep_dim = GRU_REPRESENTATION_DIM\n",
    "\n",
    "class TermGating(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size,activation=None, initializer='glorot_normal', regularizer=None):\n",
    "        super(TermGating, self).__init__()\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        \n",
    "        if regularizer is None or isinstance(regularizer,str):\n",
    "            self.regularizer = regularizers.get(regularizer)\n",
    "        else:\n",
    "            self.regularizer = regularizer\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        #term gating W\n",
    "        self.We = self.add_variable(name = \"term_gating_We\",\n",
    "                                   shape = [self.vocab_size,1],\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        #self.ones = K.constant(np.ones((aggreation_dimension,1)))\n",
    "       \n",
    "        \"\"\" self.fully_connected_W = self.add_variable(name = \"fully_connect_We\",\n",
    "                                   shape = [self.vocab_size,1],\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \"\"\"\n",
    "        self.dense_score = Dense(1,kernel_regularizer = self.regularizer, activation=self.activation)\n",
    "        self.dense_score.build(input_shape[1])\n",
    "        self._trainable_weights += self.dense_score.trainable_weights\n",
    "        \n",
    "        super(TermGating, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        #broadcast = self.We * self.ones\n",
    "        \n",
    "        query_token = K.cast(x[0],'int32') #(None, MAX_Q_TERM)\n",
    "        snippet_representation_per_query = x[1] #(None, MAX_Q_TERM, BI_GRU_DIM)\n",
    "        \n",
    "        query_token_onehot = K.one_hot(query_token, self.vocab_size)\n",
    "        print(query_token_onehot)\n",
    "        \n",
    "        query_vars = K.squeeze(K.dot(query_token_onehot,self.We),axis=-1)\n",
    "        print(query_vars) #(None, MAX_Q_TERM)\n",
    "        \n",
    "        query_term_prob = K.softmax(query_vars) \n",
    "        print(query_term_prob) #(None, MAX_Q_TERM)\n",
    "        \n",
    "        snippet_representation = K.expand_dims(query_term_prob) * snippet_representation_per_query\n",
    "        snippet_representation_sum = K.sum(snippet_representation, axis= 1)\n",
    "        #self.dense_score(snippet_representation_sum)\n",
    "        \n",
    "        return self.dense_score(snippet_representation_sum)\n",
    "    \n",
    "\n",
    "\n",
    "class TermGatingDRMM(Layer):\n",
    "    \n",
    "    def __init__(self, embedding_dim = EMB_DIM, activation=None, initializer='glorot_normal', regularizer=None):\n",
    "        super(TermGatingDRMM, self).__init__()\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        \n",
    "        if regularizer is None or isinstance(regularizer,str):\n",
    "            self.regularizer = regularizers.get(regularizer)\n",
    "        else:\n",
    "            self.regularizer = regularizer\n",
    "        \n",
    "        self.emb_dim = embedding_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        #term gating W\n",
    "        self.We = self.add_variable(name = \"term_gating_We\",\n",
    "                                   shape = [self.emb_dim,1],\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        #self.ones = K.constant(np.ones((aggreation_dimension,1)))\n",
    "        \n",
    "        super(TermGatingDRMM, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        query_embeddings = x[0] #(None, MAX_Q_TERM, EMB_SIZE)\n",
    "        snippet_representation_per_query = x[1] #(None, MAX_Q_TERM, BI_GRU_DIM)\n",
    "        \n",
    "        #compute gated weights\n",
    "        gated_logits = K.dot(query_embeddings, self.We) \n",
    "        gated_distribution = K.softmax(gated_logits)\n",
    "        \n",
    "        #apply the gated weights\n",
    "        #print(snippet_representation_per_query)\n",
    "        #print(gated_distribution)\n",
    "        \n",
    "        #normalization?\n",
    "        \n",
    "        return K.sum(K.sum(snippet_representation_per_query * gated_distribution, axis = -1),  axis = -1) # Replace with K.sum of all elements?\n",
    "\n",
    "\n",
    "\n",
    "class TermGatingDRMM_Projection(Layer):\n",
    "    \n",
    "    def __init__(self, embedding_dim = EMB_DIM, rnn_dim = snippet_rnn_rep_dim ,activation=None, initializer='glorot_normal', regularizer=None):\n",
    "        super(TermGatingDRMM_Projection, self).__init__()\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        \n",
    "        if regularizer is None or isinstance(regularizer,str):\n",
    "            self.regularizer = regularizers.get(regularizer)\n",
    "        else:\n",
    "            self.regularizer = regularizer\n",
    "        \n",
    "        self.emb_dim = embedding_dim\n",
    "        self.rnn_dim = rnn_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        #term gating W\n",
    "        self.W_query = self.add_variable(name = \"term_gating_We\",\n",
    "                                   shape = [self.emb_dim,1],\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.W_snippets_projection_score = self.add_variable(name = \"snippets_projection_score\",\n",
    "                                   shape = [self.rnn_dim,1],\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "    \n",
    "        #self.ones = K.constant(np.ones((aggreation_dimension,1)))\n",
    "        \n",
    "        super(TermGatingDRMM_Projection, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        query_embeddings = x[0] #(None, MAX_Q_TERM, EMB_SIZE)\n",
    "        snippet_representation_per_query = x[1] #(None, MAX_Q_TERM, BI_GRU_DIM)\n",
    "        \n",
    "        #compute gated weights\n",
    "        gated_logits = K.squeeze(K.dot(query_embeddings, self.W_query), axis = -1 )\n",
    "        #print(gated_logits)\n",
    "        gated_distribution = K.softmax(gated_logits)\n",
    "        #print(gated_distribution)\n",
    "        #snippet projection\n",
    "        snippets_score = K.squeeze(K.dot(snippet_representation_per_query, self.W_snippets_projection_score), axis = -1)\n",
    "        #snippets_score_activation = tanh(snippets_score)\n",
    "        \n",
    "        weighted_score = snippets_score * gated_distribution\n",
    "        \n",
    "        return K.sum(weighted_score,  axis = 1) # Replace with K.sum of all elements?\n",
    "\n",
    "    \n",
    "class TermGatingDRMM_FFN(Layer):\n",
    "    \n",
    "    def __init__(self, embedding_dim = EMB_DIM, rnn_dim = snippet_rnn_rep_dim ,activation=None, initializer='glorot_normal', regularizer=None):\n",
    "        super(TermGatingDRMM_FFN, self).__init__()\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        \n",
    "        if regularizer is None or isinstance(regularizer,str):\n",
    "            self.regularizer = regularizers.get(regularizer)\n",
    "        else:\n",
    "            self.regularizer = regularizer\n",
    "        \n",
    "        self.emb_dim = embedding_dim\n",
    "        self.rnn_dim = rnn_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        #term gating W\n",
    "        self.W_query = self.add_variable(name = \"term_gating_We\",\n",
    "                                   shape = [self.emb_dim,1],\n",
    "                                   initializer = self.initializer,\n",
    "                                   regularizer = self.regularizer,)\n",
    "        \n",
    "        self.dense_score = Dense(1,kernel_regularizer = self.regularizer, activation=self.activation)\n",
    "        \n",
    "        dense_shape = input_shape[1]\n",
    "        print(dense_shape)\n",
    "        \n",
    "        self.dense_score.build((dense_shape[0],dense_shape[2]))\n",
    "        self._trainable_weights += self.dense_score.trainable_weights\n",
    "        #self.ones = K.constant(np.ones((aggreation_dimension,1)))\n",
    "        \n",
    "        super(TermGatingDRMM_FFN, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        query_embeddings = x[0] #(None, MAX_Q_TERM, EMB_SIZE)\n",
    "        snippet_representation_per_query = x[1] #(None, MAX_Q_TERM, BI_GRU_DIM)\n",
    "        \n",
    "        #compute gated weights\n",
    "        gated_logits = K.squeeze(K.dot(query_embeddings, self.W_query), axis = -1 )\n",
    "        #print(gated_logits)\n",
    "        gated_distribution = K.expand_dims(K.softmax(gated_logits))\n",
    "        #print(gated_distribution)\n",
    "        #snippet projection\n",
    "        \n",
    "        weighted_score = K.sum(snippet_representation_per_query * gated_distribution,  axis = 1)\n",
    "        print(weighted_score)\n",
    "        \n",
    "        return self.dense_score(weighted_score) # Replace with K.sum of all elements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='final_net'></a>\n",
    "## Final Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"TD_cnn_extraction_model/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_1/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_1/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_1/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_1/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_1/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_1/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_1/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_2/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_2/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_2/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_2/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_2/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_2/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_2/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_3/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_3/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_3/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_3/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_3/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_3/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_3/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_4/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_4/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_4/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_4/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_4/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_4/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_4/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_5/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_5/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_5/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_5/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_5/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_5/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_5/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_6/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_6/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_6/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_6/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_6/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_6/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_6/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_7/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_7/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_7/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_7/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_7/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_7/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_7/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_8/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_8/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_8/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_8/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_8/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_8/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_8/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_9/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_9/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_9/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_9/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_9/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_9/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_9/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_10/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_10/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_10/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_10/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_10/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_10/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_10/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"TD_cnn_extraction_model_11/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_11/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_11/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_11/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_11/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_11/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_11/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_12/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_12/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_12/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_12/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_12/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_12/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"TD_cnn_extraction_model_12/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "(?, 13, 56)\n",
      "Tensor(\"term_gating_drmm_ffn/Sum:0\", shape=(?, 56), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ds_query_tokens (InputLayer)    (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ds_snippet_tokens (InputLayer)  (None, 13, 5, 15)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     multiple             858359000   ds_query_tokens[0][0]            \n",
      "                                                                 ds_snippet_tokens[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "snippet_transpose (Lambda)      (None, 13, 5, 200, 1 0           embedding_layer[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "query_snippet_similarity (Simil (None, 13, 5, 13, 15 0           embedding_layer[1][0]            \n",
      "                                                                 snippet_transpose[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ds_snippet_position_tokens (Inp (None, 13, 5)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "unstack_query_term (Lambda)     multiple             0           query_snippet_similarity[0][0]   \n",
      "                                                                 ds_snippet_position_tokens[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reciprocal_function (Lambda)    (None, 5)            0           unstack_query_term[1][0]         \n",
      "                                                                 unstack_query_term[1][1]         \n",
      "                                                                 unstack_query_term[1][2]         \n",
      "                                                                 unstack_query_term[1][3]         \n",
      "                                                                 unstack_query_term[1][4]         \n",
      "                                                                 unstack_query_term[1][5]         \n",
      "                                                                 unstack_query_term[1][6]         \n",
      "                                                                 unstack_query_term[1][7]         \n",
      "                                                                 unstack_query_term[1][8]         \n",
      "                                                                 unstack_query_term[1][9]         \n",
      "                                                                 unstack_query_term[1][10]        \n",
      "                                                                 unstack_query_term[1][11]        \n",
      "                                                                 unstack_query_term[1][12]        \n",
      "__________________________________________________________________________________________________\n",
      "TD_cnn_extraction_model (Sequen (None, 5, 56)        19056       unstack_query_term[0][0]         \n",
      "                                                                 unstack_query_term[0][1]         \n",
      "                                                                 unstack_query_term[0][2]         \n",
      "                                                                 unstack_query_term[0][3]         \n",
      "                                                                 unstack_query_term[0][4]         \n",
      "                                                                 unstack_query_term[0][5]         \n",
      "                                                                 unstack_query_term[0][6]         \n",
      "                                                                 unstack_query_term[0][7]         \n",
      "                                                                 unstack_query_term[0][8]         \n",
      "                                                                 unstack_query_term[0][9]         \n",
      "                                                                 unstack_query_term[0][10]        \n",
      "                                                                 unstack_query_term[0][11]        \n",
      "                                                                 unstack_query_term[0][12]        \n",
      "__________________________________________________________________________________________________\n",
      "add_dim (Lambda)                (None, 5, 1)         0           reciprocal_function[0][0]        \n",
      "                                                                 reciprocal_function[1][0]        \n",
      "                                                                 reciprocal_function[2][0]        \n",
      "                                                                 reciprocal_function[3][0]        \n",
      "                                                                 reciprocal_function[4][0]        \n",
      "                                                                 reciprocal_function[5][0]        \n",
      "                                                                 reciprocal_function[6][0]        \n",
      "                                                                 reciprocal_function[7][0]        \n",
      "                                                                 reciprocal_function[8][0]        \n",
      "                                                                 reciprocal_function[9][0]        \n",
      "                                                                 reciprocal_function[10][0]       \n",
      "                                                                 reciprocal_function[11][0]       \n",
      "                                                                 reciprocal_function[12][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concat_snippet_position (Concat (None, 5, 57)        0           TD_cnn_extraction_model[1][0]    \n",
      "                                                                 add_dim[0][0]                    \n",
      "                                                                 TD_cnn_extraction_model[2][0]    \n",
      "                                                                 add_dim[1][0]                    \n",
      "                                                                 TD_cnn_extraction_model[3][0]    \n",
      "                                                                 add_dim[2][0]                    \n",
      "                                                                 TD_cnn_extraction_model[4][0]    \n",
      "                                                                 add_dim[3][0]                    \n",
      "                                                                 TD_cnn_extraction_model[5][0]    \n",
      "                                                                 add_dim[4][0]                    \n",
      "                                                                 TD_cnn_extraction_model[6][0]    \n",
      "                                                                 add_dim[5][0]                    \n",
      "                                                                 TD_cnn_extraction_model[7][0]    \n",
      "                                                                 add_dim[6][0]                    \n",
      "                                                                 TD_cnn_extraction_model[8][0]    \n",
      "                                                                 add_dim[7][0]                    \n",
      "                                                                 TD_cnn_extraction_model[9][0]    \n",
      "                                                                 add_dim[8][0]                    \n",
      "                                                                 TD_cnn_extraction_model[10][0]   \n",
      "                                                                 add_dim[9][0]                    \n",
      "                                                                 TD_cnn_extraction_model[11][0]   \n",
      "                                                                 add_dim[10][0]                   \n",
      "                                                                 TD_cnn_extraction_model[12][0]   \n",
      "                                                                 add_dim[11][0]                   \n",
      "                                                                 TD_cnn_extraction_model[13][0]   \n",
      "                                                                 add_dim[12][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "aggregation_snippet_by_q_term ( (None, 56)           19152       concat_snippet_position[0][0]    \n",
      "                                                                 concat_snippet_position[1][0]    \n",
      "                                                                 concat_snippet_position[2][0]    \n",
      "                                                                 concat_snippet_position[3][0]    \n",
      "                                                                 concat_snippet_position[4][0]    \n",
      "                                                                 concat_snippet_position[5][0]    \n",
      "                                                                 concat_snippet_position[6][0]    \n",
      "                                                                 concat_snippet_position[7][0]    \n",
      "                                                                 concat_snippet_position[8][0]    \n",
      "                                                                 concat_snippet_position[9][0]    \n",
      "                                                                 concat_snippet_position[10][0]   \n",
      "                                                                 concat_snippet_position[11][0]   \n",
      "                                                                 concat_snippet_position[12][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_passage_dim (Lambda)        (None, 1, 56)        0           aggregation_snippet_by_q_term[0][\n",
      "                                                                 aggregation_snippet_by_q_term[1][\n",
      "                                                                 aggregation_snippet_by_q_term[2][\n",
      "                                                                 aggregation_snippet_by_q_term[3][\n",
      "                                                                 aggregation_snippet_by_q_term[4][\n",
      "                                                                 aggregation_snippet_by_q_term[5][\n",
      "                                                                 aggregation_snippet_by_q_term[6][\n",
      "                                                                 aggregation_snippet_by_q_term[7][\n",
      "                                                                 aggregation_snippet_by_q_term[8][\n",
      "                                                                 aggregation_snippet_by_q_term[9][\n",
      "                                                                 aggregation_snippet_by_q_term[10]\n",
      "                                                                 aggregation_snippet_by_q_term[11]\n",
      "                                                                 aggregation_snippet_by_q_term[12]\n",
      "__________________________________________________________________________________________________\n",
      "concat_representation (Concaten (None, 13, 56)       0           add_passage_dim[0][0]            \n",
      "                                                                 add_passage_dim[1][0]            \n",
      "                                                                 add_passage_dim[2][0]            \n",
      "                                                                 add_passage_dim[3][0]            \n",
      "                                                                 add_passage_dim[4][0]            \n",
      "                                                                 add_passage_dim[5][0]            \n",
      "                                                                 add_passage_dim[6][0]            \n",
      "                                                                 add_passage_dim[7][0]            \n",
      "                                                                 add_passage_dim[8][0]            \n",
      "                                                                 add_passage_dim[9][0]            \n",
      "                                                                 add_passage_dim[10][0]           \n",
      "                                                                 add_passage_dim[11][0]           \n",
      "                                                                 add_passage_dim[12][0]           \n",
      "__________________________________________________________________________________________________\n",
      "term_gating_drmm_ffn (TermGatin (None, 1)            257         embedding_layer[1][0]            \n",
      "                                                                 concat_representation[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 858,397,465\n",
      "Trainable params: 38,465\n",
      "Non-trainable params: 858,359,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "  ---- Final Network ----\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "query_token_input = Input(shape=(MAX_Q_TERM,), name=\"ds_query_tokens\")\n",
    "doc_score_snippet_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), name = \"ds_snippet_tokens\")\n",
    "doc_score_snippet_position_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY), name = \"ds_snippet_position_tokens\")\n",
    "\n",
    "\n",
    "unstack_by_q_term = Lambda(lambda x:unstack(x,axis=1), name=\"unstack_query_term\")\n",
    "\n",
    "#doc_score_snippet_by_q_term = unstack_by_q_term(doc_score_snippet_input)\n",
    "#doc_score_snippet_position_by_q_term = unstack_by_q_term(doc_score_snippet_position_input)\n",
    "\n",
    "#INPUT in token format\n",
    "#query_token_input = Input(shape=(MAX_Q_TERM,), name=\"query_tokens\")\n",
    "#snippets_tokens_input = [Input(shape = (MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), name = \"snippet_tokens_\"+str(q)) for q in range(MAX_Q_TERM)]\n",
    "#inputs_contex_position = [Input(shape = (MAX_PASSAGES_PER_QUERY,), name = \"q_context_position_\"+str(q)) for q in range(MAX_Q_TERM)]\n",
    "\n",
    "query_emb = embedding(query_token_input)\n",
    "\n",
    "doc_score_snippet_emb = embedding(doc_score_snippet_input)\n",
    "doc_score_snippet_emb_transpose = transpose_layer(doc_score_snippet_emb)\n",
    "\n",
    "query_snippets_s_matrix = similarity_matrix([query_emb,doc_score_snippet_emb_transpose])\n",
    "\n",
    "list_of_s_matrix_by_q_term = unstack_by_q_term(query_snippets_s_matrix)\n",
    "list_of_snippet_postion_by_q_term = unstack_by_q_term(doc_score_snippet_position_input)\n",
    "\n",
    "relevance_representation = []\n",
    "for i in range(MAX_Q_TERM):\n",
    "    \n",
    "    snippet_relative_position = reciprocal_f(list_of_snippet_postion_by_q_term[i])\n",
    "    \n",
    "    local_relevance = td_cnn_extraction_model(list_of_s_matrix_by_q_term[i])\n",
    "    \n",
    "    local_relevance_position = concat_snippet_position([local_relevance,add_dim(snippet_relative_position)])\n",
    "    \n",
    "    relevance_representation.append(add_passage_dim(gru(local_relevance_position)))\n",
    "\n",
    "concat_relevance = concat_representation(relevance_representation)\n",
    "\n",
    "if TERM_GATING_MODE==0:\n",
    "    term_gating = TermGating(vocab_size=VOCAB_SIZE, activation=ACTIVATION_FUNCTION)\n",
    "    document_score = term_gating([query_token_input,concat_relevance])\n",
    "    \n",
    "elif TERM_GATING_MODE==1:\n",
    "    term_gating = TermGatingDRMM()\n",
    "    document_score = term_gating([query_emb,concat_relevance])\n",
    "\n",
    "elif TERM_GATING_MODE==2:\n",
    "    term_gating = TermGatingDRMM_Projection()\n",
    "    document_score = term_gating([query_emb,concat_relevance])\n",
    "\n",
    "elif TERM_GATING_MODE==3:\n",
    "    term_gating = TermGatingDRMM_FFN(activation=ACTIVATION_FUNCTION, regularizer=REGULARIZATION)\n",
    "    document_score = term_gating([query_emb,concat_relevance])\n",
    "\n",
    "document_score_model = Model(inputs = [query_token_input, doc_score_snippet_input, doc_score_snippet_position_input], outputs = [document_score], name=\"query_document_score\")\n",
    "document_score_model.summary()      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL Trainable arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"query_document_score/TD_cnn_extraction_model/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"query_document_score/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score/term_gating_drmm_ffn/Sum:0\", shape=(?, 56), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_1/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_2/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_3/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_4/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_5/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_6/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_7/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_8/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_9/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_10/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_11/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/MaxPool:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/MaxPool_1:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/MaxPool_2:0\", shape=(?, 1, 1, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/Reshape_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"query_document_score_1/TD_cnn_extraction_model_12/time_distributed/masked_conv2d/concat:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"query_document_score_1/term_gating_drmm_ffn/Sum:0\", shape=(?, 56), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dr_query_tokens (InputLayer)    (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_snippet_tokens (InputL (None, 13, 5, 15)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_snippet_position_token (None, 13, 5)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_snippet_tokens (InputL (None, 13, 5, 15)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_snippet_position_token (None, 13, 5)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "query_document_score (Model)    (None, 1)            858397465   dr_query_tokens[0][0]            \n",
      "                                                                 positive_snippet_tokens[0][0]    \n",
      "                                                                 positive_snippet_position_tokens[\n",
      "                                                                 dr_query_tokens[0][0]            \n",
      "                                                                 negative_snippet_tokens[0][0]    \n",
      "                                                                 negative_snippet_position_tokens[\n",
      "==================================================================================================\n",
      "Total params: 858,397,465\n",
      "Trainable params: 38,465\n",
      "Non-trainable params: 858,359,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "query_token_input = Input(shape=(MAX_Q_TERM,), name=\"dr_query_tokens\")\n",
    "positive_snippet_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), name = \"positive_snippet_tokens\")\n",
    "positive_snippet_position_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY), name = \"positive_snippet_position_tokens\")\n",
    "negative_snippet_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), name = \"negative_snippet_tokens\")\n",
    "negative_snippet_position_input = Input(shape = (MAX_Q_TERM,MAX_PASSAGES_PER_QUERY), name = \"negative_snippet_position_tokens\")\n",
    "\n",
    "positive_documents_score = document_score_model([query_token_input, positive_snippet_input, positive_snippet_position_input])\n",
    "negative_documents_score = document_score_model([query_token_input, negative_snippet_input, negative_snippet_position_input])\n",
    "\n",
    "#stack_socres = stack_scores_layer([positive_documents_score,negative_documents_score])\n",
    "\n",
    "\n",
    "\n",
    "#pairwise_loss_layer = Lambda(pairwise_hinge_loss, name=\"pairwise_hinge\")\n",
    "#pairwise_loss = pairwise_loss_layer([positive_documents_score,negative_documents_score])\n",
    "\n",
    "\n",
    "\n",
    "inputs = [query_token_input, positive_snippet_input, positive_snippet_position_input, negative_snippet_input, negative_snippet_position_input]\n",
    "\n",
    "deepRank_model = Model(inputs = inputs, outputs = [positive_documents_score, negative_documents_score], name=\"deep_rank\")\n",
    "\n",
    "\n",
    "p_loss = K.mean(K.maximum(0.0, 1.0 - positive_documents_score + negative_documents_score))\n",
    "\n",
    "deepRank_model.add_loss(p_loss)\n",
    "\n",
    "deepRank_model.summary() \n",
    "#m.predict([Q, Q_t1_passage, Q_t2_passage, Q_t3_passage, Q_t1_passage_pos, Q_t2_passage_pos, Q_t3_passage_pos])\n",
    "\n",
    "#deepRank_model.compile(loss=pairwise_hinge_loss, optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITE_GRAPH = False\n",
    "if WRITE_GRAPH:\n",
    "    from tensorflow.summary import FileWriter\n",
    "\n",
    "    graph = K.get_session().graph\n",
    "     # Your model implementation\n",
    "    #with graph.as_default():\n",
    "      # compile method actually creates the model in the graph.\n",
    "      #deepRank_model.compile(loss=identity_loss, optimizer='adam', metrics=['accuracy'])\n",
    "    writer = FileWriter(logdir='tensorboard/deepRank', graph=graph)\n",
    "    writer.flush()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_score.predict([Q, Q_t1_passage, Q_t2_passage, Q_t3_passage, Q_t1_passage_pos, Q_t2_passage_pos, Q_t3_passage_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open /backup/results/fast_method_relevant_results/train_data_deep_models_v2.tar.gz\n"
     ]
    }
   ],
   "source": [
    "path_dl_train = \"/backup/results/fast_method_relevant_results/train_data_deep_models_v2.tar.gz\"\n",
    "\n",
    "\n",
    "\n",
    "tar = tarfile.open(path_dl_train)\n",
    "#open\n",
    "print(\"Open\",path_dl_train)\n",
    "m = tar.getmembers()[0]\n",
    "f = tar.extractfile(m)\n",
    "train_articles_collection = pickle.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTILLY_POSITIVE_SAMPLES = 3\n",
    "NEGATIVE_SAMPLES = 1\n",
    "\n",
    "class TrainDataGenerator(object):\n",
    "    def __init__(self, article_collection, tokenizer, batch_queries_size):\n",
    "        \n",
    "        self.batch_size = batch_queries_size\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.train_data = article_collection[\"bioasq_data\"]\n",
    "        self.articles = article_collection[\"collection\"]\n",
    "        self.irrelevant_pmid = article_collection[\"irrelevant_pmid\"]\n",
    "        \n",
    "        self.num_steps = len(self.train_data)//self.batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        \n",
    "        query = []\n",
    "        query_positive_doc = []\n",
    "        query_positive_doc_position = []\n",
    "        query_negative_doc = []\n",
    "        query_negative_doc_position = []\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            #stop condition\n",
    "            if len(query)>=self.batch_size:\n",
    "                #missing fill the gap for the missing query_terms\n",
    "                query = np.array(query)\n",
    "                p=np.random.permutation(query.shape[0])\n",
    "                query = query[p]\n",
    "                query_positive_doc = np.array(query_positive_doc)[p]\n",
    "                query_positive_doc_position = np.array(query_positive_doc_position)[p]\n",
    "                query_negative_doc = np.array(query_negative_doc)[p]\n",
    "                query_negative_doc_position =  np.array(query_negative_doc_position)[p]\n",
    "                \n",
    "                X = [query, query_positive_doc, query_positive_doc_position, query_negative_doc, query_negative_doc_position]\n",
    "                #Y = [np.zeros((len(query))),np.zeros((len(query)))]\n",
    "                yield X\n",
    "\n",
    "\n",
    "                #reset\n",
    "                query = []\n",
    "                query_positive_doc = []\n",
    "                query_positive_doc_position = []\n",
    "                query_negative_doc = []\n",
    "                query_negative_doc_position = []\n",
    "            \n",
    "            #select a random question\n",
    "            random_query_index = random.randint(0, len(self.train_data)-1) \n",
    "            query_data = self.train_data[random_query_index]\n",
    "            \n",
    "            #list of partilly relevant documents\n",
    "            partilly_positive_pmid_docs = query_data[\"partilly_positive_pmid\"]\n",
    "\n",
    "            tokenized_query = query_data[\"query\"][:MAX_Q_TERM]\n",
    "            \n",
    "            for j in range(PARTILLY_POSITIVE_SAMPLES+NEGATIVE_SAMPLES):\n",
    "                #select a random positive\n",
    "                random_doc_index = random.randint(0, len(query_data[\"positive_pmid\"])-1) \n",
    "                doc_pmid = query_data[\"positive_pmid\"][random_doc_index]\n",
    "\n",
    "                tokenized_positive_doc = self.articles[doc_pmid]\n",
    "                positive_snippets, positive_snippets_position = self.__snippet_interaction(tokenized_query, tokenized_positive_doc)\n",
    "                \n",
    "                if j<PARTILLY_POSITIVE_SAMPLES:\n",
    "                    #select the partilly posivite doc\n",
    "                    random_ind = bisect(query_data[\"partially_positive_cumulative_prob\"],random.random())\n",
    "                    random_negative_doc_pmid = query_data[\"partilly_positive_pmid\"][random_ind]\n",
    "                    #print(self.__get_article(random_negative_doc_pmid))\n",
    "                    tokenized_negative_doc = self.articles[random_negative_doc_pmid]\n",
    "                    negative_snippets, negative_snippets_position = self.__snippet_interaction(tokenized_query, tokenized_negative_doc)\n",
    "                else:\n",
    "                    #select a random negative\n",
    "                    random_doc_index = random.randint(0, len(self.irrelevant_pmid)-1) \n",
    "                    doc_pmid = self.irrelevant_pmid[random_doc_index]\n",
    "                    \n",
    "                    tokenized_negative_doc = self.articles[doc_pmid]\n",
    "                    negative_snippets, negative_snippets_position = self.__snippet_interaction(tokenized_query, tokenized_negative_doc)\n",
    "                \n",
    "                \n",
    "                ### add ###\n",
    "\n",
    "                #not efficient\n",
    "                query.append(tokenized_query)\n",
    "\n",
    "                #positive doc\n",
    "                query_positive_doc.append(positive_snippets)\n",
    "                query_positive_doc_position.append(positive_snippets_position)\n",
    "\n",
    "                #negative doc\n",
    "                query_negative_doc.append(negative_snippets)\n",
    "                query_negative_doc_position.append(negative_snippets_position)\n",
    "            \n",
    "\n",
    "            \n",
    "    def __snippet_interaction(self, tokenized_query, tokenized_doc, snippet_length=QUERY_CENTRIC_CONTEX):\n",
    "        \n",
    "        snippets = []\n",
    "        snippets_position = [] \n",
    "\n",
    "        half_size = snippet_length//2\n",
    "        \n",
    "        #O(n^2) complexity, probably can do better with better data struct TODO see if is worthit\n",
    "        for query_token in tokenized_query:\n",
    "            \n",
    "            snippets_per_token = []\n",
    "            snippets_per_token_position = []\n",
    "            \n",
    "            if query_token != 0: #jump padded token\n",
    "            \n",
    "                for i,doc_token in enumerate(tokenized_doc):\n",
    "\n",
    "                    if doc_token==query_token:\n",
    "\n",
    "                        lower_index = i-half_size\n",
    "                        lower_index = max(0,lower_index)\n",
    "\n",
    "                        higher_index = i+half_size\n",
    "                        higher_index = min(len(tokenized_doc),higher_index)\n",
    "\n",
    "                        snippets_per_token.append(tokenized_doc[lower_index:higher_index])\n",
    "                        snippets_per_token_position.append(i)\n",
    "            \n",
    "            if len(snippets_per_token)==0:\n",
    "                snippets.append(np.zeros((MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), dtype=np.int32))\n",
    "                snippets_position.append(np.zeros((MAX_PASSAGES_PER_QUERY), dtype=np.int32)+SNIPPET_POSITION_PADDING_VALUE)\n",
    "                continue\n",
    "                \n",
    "            max_snippets_len = min(MAX_PASSAGES_PER_QUERY, len(snippets_per_token))\n",
    "            \n",
    "            ### snippets in matrix format\n",
    "            #pad\n",
    "            snippets_per_token = pad_sequences(snippets_per_token, maxlen = QUERY_CENTRIC_CONTEX, padding=\"post\")\n",
    "            #fill the gaps\n",
    "            _temp = np.zeros((MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), dtype=np.int32)\n",
    "            _temp[:max_snippets_len] = snippets_per_token[:max_snippets_len]\n",
    "            snippets.append(_temp)\n",
    "            \n",
    "            ### snippets_position in matrix format\n",
    "            #pad\n",
    "            snippets_per_token_position = pad_sequences([snippets_per_token_position], maxlen = MAX_PASSAGES_PER_QUERY, padding=\"post\", value=SNIPPET_POSITION_PADDING_VALUE)[0]\n",
    "            snippets_position.append(snippets_per_token_position)\n",
    "            \n",
    "        return snippets, snippets_position\n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,Y = next(data_generator)\n",
    "\n",
    "TESTE = False\n",
    "if TESTE:\n",
    "    data_generator = TrainDataGenerator(train_articles_collection, tk, 1)\n",
    "    data_generator = iter(data_generator)\n",
    "    X,Y = next(data_generator)\n",
    "    \n",
    "    print(X[0].shape)\n",
    "    print(X[1].shape)\n",
    "    print(X[2].shape)\n",
    "    print(X[3].shape)\n",
    "    print(X[4].shape)\n",
    "    print(Y[0].shape)\n",
    "    print(Y[1].shape)\n",
    "    deepRank_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"query_document_score\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"query_document_score\" during training.\n",
      "WARNING:tensorflow:Output \"query_document_score\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"query_document_score\" during training.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam,  Adadelta\n",
    "\n",
    "#sgd = SGD(lr=0.001)\n",
    "#adam = Adam(lr=0.001)\n",
    "adadelta = Adadelta(lr=2)\n",
    "\n",
    "deepRank_model.compile( optimizer=adadelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoach: 0 | avg loss: 0.7913 | max loss: 1.0618795 | min loss: 0.46426705\n",
      "Epoach: 1 | avg loss: 0.28404543 | max loss: 0.41293472 | min loss: 0.15731314\n",
      "Epoach: 2 | avg loss: 0.1978395 | max loss: 0.26958653 | min loss: 0.12772673\n",
      "Epoach: 3 | avg loss: 0.17477201 | max loss: 0.22347149 | min loss: 0.13149385\n",
      "Epoach: 4 | avg loss: 0.17481062 | max loss: 0.21199152 | min loss: 0.15189683\n",
      "Epoach: 5 | avg loss: 0.1647676 | max loss: 0.2269466 | min loss: 0.13477898\n",
      "Epoach: 6 | avg loss: 0.1689105 | max loss: 0.23404966 | min loss: 0.1150029\n",
      "Epoach: 7 | avg loss: 0.14810923 | max loss: 0.18586242 | min loss: 0.12309752\n",
      "Epoach: 8 | avg loss: 0.13462949 | max loss: 0.17335048 | min loss: 0.077673875\n",
      "Epoach: 9 | avg loss: 0.12295155 | max loss: 0.16165823 | min loss: 0.087628536\n",
      "Epoach: 10 | avg loss: 0.13260557 | max loss: 0.16866186 | min loss: 0.09964288\n",
      "Epoach: 11 | avg loss: 0.14011426 | max loss: 0.1801532 | min loss: 0.09014022\n",
      "Epoach: 12 | avg loss: 0.11646027 | max loss: 0.15067527 | min loss: 0.05838028\n",
      "Epoach: 13 | avg loss: 0.12596129 | max loss: 0.16557948 | min loss: 0.08515809\n",
      "Epoach: 14 | avg loss: 0.13141373 | max loss: 0.15750787 | min loss: 0.083021514\n",
      "Epoach: 15 | avg loss: 0.13047595 | max loss: 0.16579753 | min loss: 0.086023845\n",
      "Epoach: 16 | avg loss: 0.14593278 | max loss: 0.18704325 | min loss: 0.12499703\n",
      "Epoach: 17 | avg loss: 0.12739152 | max loss: 0.15108636 | min loss: 0.096091874\n",
      "Epoach: 18 | avg loss: 0.1179543 | max loss: 0.16291215 | min loss: 0.06810553\n",
      "Epoach: 19 | avg loss: 0.12429599 | max loss: 0.27108377 | min loss: 0.0759926\n",
      "Epoach: 20 | avg loss: 0.13926244 | max loss: 0.1789112 | min loss: 0.09119717\n",
      "Epoach: 21 | avg loss: 0.13788617 | max loss: 0.17901434 | min loss: 0.07835434\n",
      "Epoach: 22 | avg loss: 0.12969619 | max loss: 0.19277957 | min loss: 0.099311896\n",
      "Epoach: 23 | avg loss: 0.11899766 | max loss: 0.14887658 | min loss: 0.08977747\n",
      "Epoach: 24 | avg loss: 0.121603414 | max loss: 0.14294554 | min loss: 0.09802099\n",
      "Epoach: 25 | avg loss: 0.12062402 | max loss: 0.17781141 | min loss: 0.09250293\n",
      "Epoach: 26 | avg loss: 0.11475769 | max loss: 0.14624047 | min loss: 0.09415564\n",
      "Epoach: 27 | avg loss: 0.12985554 | max loss: 0.14151382 | min loss: 0.11938328\n",
      "Epoach: 28 | avg loss: 0.1124015 | max loss: 0.14284305 | min loss: 0.084773906\n",
      "Epoach: 29 | avg loss: 0.13849054 | max loss: 0.17483906 | min loss: 0.10754509\n",
      "Epoach: 30 | avg loss: 0.11084038 | max loss: 0.15636994 | min loss: 0.07996927\n",
      "Epoach: 31 | avg loss: 0.117730685 | max loss: 0.15289901 | min loss: 0.073151864\n",
      "Epoach: 32 | avg loss: 0.11150893 | max loss: 0.17578901 | min loss: 0.07496312\n",
      "Epoach: 33 | avg loss: 0.11423007 | max loss: 0.16071947 | min loss: 0.078760624\n",
      "Epoach: 34 | avg loss: 0.11972954 | max loss: 0.19714452 | min loss: 0.09675728\n",
      "Epoach: 35 | avg loss: 0.097426295 | max loss: 0.12082767 | min loss: 0.06328648\n",
      "Epoach: 36 | avg loss: 0.13947389 | max loss: 0.19475818 | min loss: 0.08402937\n",
      "Epoach: 37 | avg loss: 0.10599454 | max loss: 0.1555747 | min loss: 0.075975634\n",
      "Epoach: 38 | avg loss: 0.10722926 | max loss: 0.1508336 | min loss: 0.06725452\n",
      "Epoach: 39 | avg loss: 0.10424751 | max loss: 0.12187329 | min loss: 0.06386935\n",
      "Epoach: 40 | avg loss: 0.11917004 | max loss: 0.14775407 | min loss: 0.083357915\n",
      "Epoach: 41 | avg loss: 0.10085824 | max loss: 0.14052686 | min loss: 0.0650363\n",
      "Epoach: 42 | avg loss: 0.116143495 | max loss: 0.17071506 | min loss: 0.085502476\n",
      "Epoach: 43 | avg loss: 0.12733865 | max loss: 0.15290272 | min loss: 0.07892705\n",
      "Epoach: 44 | avg loss: 0.111755796 | max loss: 0.13933055 | min loss: 0.086340666\n",
      "Epoach: 45 | avg loss: 0.116692916 | max loss: 0.15911792 | min loss: 0.07588838\n",
      "Epoach: 46 | avg loss: 0.11483912 | max loss: 0.15482141 | min loss: 0.065362476\n",
      "Epoach: 47 | avg loss: 0.12650701 | max loss: 0.14522901 | min loss: 0.09347246\n",
      "Epoach: 48 | avg loss: 0.10875323 | max loss: 0.13698451 | min loss: 0.055478387\n",
      "Epoach: 49 | avg loss: 0.13607788 | max loss: 0.20968163 | min loss: 0.09681437\n",
      "Epoach: 50 | avg loss: 0.116480485 | max loss: 0.13223824 | min loss: 0.08614442\n",
      "Epoach: 51 | avg loss: 0.13365093 | max loss: 0.1776949 | min loss: 0.090657525\n",
      "Epoach: 52 | avg loss: 0.11753736 | max loss: 0.17549644 | min loss: 0.04856431\n",
      "Epoach: 53 | avg loss: 0.13344832 | max loss: 0.20890476 | min loss: 0.081052266\n",
      "Epoach: 54 | avg loss: 0.11278716 | max loss: 0.14817552 | min loss: 0.05748473\n",
      "Epoach: 55 | avg loss: 0.13265096 | max loss: 0.14777918 | min loss: 0.119007796\n",
      "Epoach: 56 | avg loss: 0.12007712 | max loss: 0.19838749 | min loss: 0.08426441\n",
      "Epoach: 57 | avg loss: 0.11721572 | max loss: 0.13986857 | min loss: 0.076402664\n",
      "Epoach: 58 | avg loss: 0.12330821 | max loss: 0.17576915 | min loss: 0.086135484\n",
      "Epoach: 59 | avg loss: 0.13334861 | max loss: 0.18815741 | min loss: 0.072440475\n",
      "Epoach: 60 | avg loss: 0.11326547 | max loss: 0.1462403 | min loss: 0.07761183\n",
      "Epoach: 61 | avg loss: 0.12014651 | max loss: 0.15994038 | min loss: 0.043410357\n",
      "Epoach: 62 | avg loss: 0.10601982 | max loss: 0.13334203 | min loss: 0.08300901\n",
      "Epoach: 63 | avg loss: 0.13408887 | max loss: 0.24156587 | min loss: 0.09827203\n",
      "Epoach: 64 | avg loss: 0.11766198 | max loss: 0.16153066 | min loss: 0.082095236\n",
      "Epoach: 65 | avg loss: 0.11630936 | max loss: 0.14841156 | min loss: 0.09010305\n",
      "Epoach: 66 | avg loss: 0.10824981 | max loss: 0.15853973 | min loss: 0.05946823\n",
      "Epoach: 67 | avg loss: 0.10158459 | max loss: 0.13600849 | min loss: 0.067369625\n",
      "Epoach: 68 | avg loss: 0.105071686 | max loss: 0.15153307 | min loss: 0.07394057\n",
      "Epoach: 69 | avg loss: 0.11168639 | max loss: 0.13593616 | min loss: 0.08692096\n",
      "Epoach: 70 | avg loss: 0.10817595 | max loss: 0.16776076 | min loss: 0.06912857\n",
      "Epoach: 71 | avg loss: 0.097785264 | max loss: 0.128236 | min loss: 0.06794786\n",
      "Epoach: 72 | avg loss: 0.11720523 | max loss: 0.14664792 | min loss: 0.08617152\n",
      "Epoach: 73 | avg loss: 0.09728803 | max loss: 0.13718382 | min loss: 0.059547797\n",
      "Epoach: 74 | avg loss: 0.11348902 | max loss: 0.212527 | min loss: 0.06461703\n",
      "Epoach: 75 | avg loss: 0.12537849 | max loss: 0.18493257 | min loss: 0.092715025\n",
      "Epoach: 76 | avg loss: 0.11060819 | max loss: 0.13955122 | min loss: 0.076900825\n",
      "Epoach: 77 | avg loss: 0.09270489 | max loss: 0.14962801 | min loss: 0.05550894\n",
      "Epoach: 78 | avg loss: 0.114269696 | max loss: 0.1729523 | min loss: 0.06680051\n",
      "Epoach: 79 | avg loss: 0.09810392 | max loss: 0.120402314 | min loss: 0.0673907\n",
      "Epoach: 80 | avg loss: 0.11328231 | max loss: 0.1557617 | min loss: 0.03719992\n",
      "Epoach: 81 | avg loss: 0.0985623 | max loss: 0.12576711 | min loss: 0.08458119\n",
      "Epoach: 82 | avg loss: 0.108291715 | max loss: 0.1610197 | min loss: 0.07069578\n",
      "Epoach: 83 | avg loss: 0.108208776 | max loss: 0.13533655 | min loss: 0.07238289\n",
      "Epoach: 84 | avg loss: 0.10308546 | max loss: 0.14475822 | min loss: 0.06314745\n",
      "Epoach: 85 | avg loss: 0.10850935 | max loss: 0.13273902 | min loss: 0.0854526\n",
      "Epoach: 86 | avg loss: 0.11004184 | max loss: 0.1453655 | min loss: 0.06737094\n",
      "Epoach: 87 | avg loss: 0.11486107 | max loss: 0.12653354 | min loss: 0.100399315\n",
      "Epoach: 88 | avg loss: 0.10279809 | max loss: 0.15412775 | min loss: 0.07756917\n",
      "Epoach: 89 | avg loss: 0.13155544 | max loss: 0.21126251 | min loss: 0.05624419\n",
      "Epoach: 90 | avg loss: 0.10922875 | max loss: 0.14038292 | min loss: 0.08104789\n",
      "Epoach: 91 | avg loss: 0.10923787 | max loss: 0.15001234 | min loss: 0.07098788\n",
      "Epoach: 92 | avg loss: 0.0915112 | max loss: 0.12731515 | min loss: 0.07281609\n",
      "Epoach: 93 | avg loss: 0.11327694 | max loss: 0.15482062 | min loss: 0.07012023\n",
      "Epoach: 94 | avg loss: 0.10389386 | max loss: 0.16543268 | min loss: 0.054602105\n",
      "Epoach: 95 | avg loss: 0.11143899 | max loss: 0.1483069 | min loss: 0.08595426\n",
      "Epoach: 96 | avg loss: 0.09632932 | max loss: 0.12171943 | min loss: 0.06870121\n",
      "Epoach: 97 | avg loss: 0.101160675 | max loss: 0.16164918 | min loss: 0.07085505\n",
      "Epoach: 98 | avg loss: 0.10965392 | max loss: 0.13924176 | min loss: 0.073260725\n",
      "Epoach: 99 | avg loss: 0.10410707 | max loss: 0.14402242 | min loss: 0.07199932\n",
      "Epoach: 100 | avg loss: 0.112890214 | max loss: 0.14395496 | min loss: 0.09091102\n",
      "Epoach: 101 | avg loss: 0.08151793 | max loss: 0.108919986 | min loss: 0.052792113\n",
      "Epoach: 102 | avg loss: 0.116253816 | max loss: 0.149783 | min loss: 0.076108634\n",
      "Epoach: 103 | avg loss: 0.11023883 | max loss: 0.1495764 | min loss: 0.040344097\n",
      "Epoach: 104 | avg loss: 0.11505641 | max loss: 0.17823075 | min loss: 0.044534564\n",
      "Epoach: 105 | avg loss: 0.103226334 | max loss: 0.13674396 | min loss: 0.053838324\n",
      "Epoach: 106 | avg loss: 0.11102614 | max loss: 0.18019408 | min loss: 0.07996505\n",
      "Epoach: 107 | avg loss: 0.09050908 | max loss: 0.17754248 | min loss: 0.05062079\n",
      "Epoach: 108 | avg loss: 0.09436315 | max loss: 0.12310808 | min loss: 0.047721297\n",
      "Epoach: 109 | avg loss: 0.10809183 | max loss: 0.12863562 | min loss: 0.08790006\n",
      "Epoach: 110 | avg loss: 0.09272333 | max loss: 0.12134538 | min loss: 0.055214904\n",
      "Epoach: 111 | avg loss: 0.10118795 | max loss: 0.18421397 | min loss: 0.07336506\n",
      "Epoach: 112 | avg loss: 0.10454561 | max loss: 0.1657717 | min loss: 0.068203025\n",
      "Epoach: 113 | avg loss: 0.10131329 | max loss: 0.1434192 | min loss: 0.067824416\n",
      "Epoach: 114 | avg loss: 0.10195008 | max loss: 0.13436775 | min loss: 0.048328392\n",
      "Epoach: 115 | avg loss: 0.11011289 | max loss: 0.14102578 | min loss: 0.08928312\n",
      "Epoach: 116 | avg loss: 0.10248957 | max loss: 0.1233241 | min loss: 0.07807307\n",
      "Epoach: 117 | avg loss: 0.10403554 | max loss: 0.1458693 | min loss: 0.053614188\n",
      "Epoach: 118 | avg loss: 0.10585389 | max loss: 0.1451609 | min loss: 0.069659926\n",
      "Epoach: 119 | avg loss: 0.10954674 | max loss: 0.14247821 | min loss: 0.07642556\n",
      "Epoach: 120 | avg loss: 0.118698835 | max loss: 0.1911204 | min loss: 0.06525303\n",
      "Epoach: 121 | avg loss: 0.117975146 | max loss: 0.14915325 | min loss: 0.080923736\n",
      "Epoach: 122 | avg loss: 0.11846657 | max loss: 0.13882229 | min loss: 0.09956311\n",
      "Epoach: 123 | avg loss: 0.09207174 | max loss: 0.13557072 | min loss: 0.046435013\n",
      "Epoach: 124 | avg loss: 0.093737096 | max loss: 0.12811929 | min loss: 0.07425325\n",
      "Epoach: 125 | avg loss: 0.09892942 | max loss: 0.16271168 | min loss: 0.07109538\n",
      "Epoach: 126 | avg loss: 0.09313631 | max loss: 0.10716723 | min loss: 0.07862532\n",
      "Epoach: 127 | avg loss: 0.09468996 | max loss: 0.11137549 | min loss: 0.082638144\n",
      "Epoach: 128 | avg loss: 0.1046213 | max loss: 0.13767529 | min loss: 0.057003725\n",
      "Epoach: 129 | avg loss: 0.10265262 | max loss: 0.11761041 | min loss: 0.086031854\n",
      "Epoach: 130 | avg loss: 0.08848466 | max loss: 0.107778296 | min loss: 0.062838435\n",
      "Epoach: 131 | avg loss: 0.094562344 | max loss: 0.13624735 | min loss: 0.072295226\n",
      "Epoach: 132 | avg loss: 0.1050097 | max loss: 0.18282332 | min loss: 0.061752845\n",
      "Epoach: 133 | avg loss: 0.11067559 | max loss: 0.13708456 | min loss: 0.08614606\n",
      "Epoach: 134 | avg loss: 0.10331831 | max loss: 0.14007446 | min loss: 0.07223327\n",
      "Epoach: 135 | avg loss: 0.11270507 | max loss: 0.20224686 | min loss: 0.0597809\n",
      "Epoach: 136 | avg loss: 0.09767308 | max loss: 0.15377274 | min loss: 0.06399126\n",
      "Epoach: 137 | avg loss: 0.09798211 | max loss: 0.10842432 | min loss: 0.062254276\n",
      "Epoach: 138 | avg loss: 0.09973551 | max loss: 0.14290075 | min loss: 0.06654842\n",
      "Epoach: 139 | avg loss: 0.10351008 | max loss: 0.12631269 | min loss: 0.083375044\n",
      "Epoach: 140 | avg loss: 0.109465174 | max loss: 0.1531916 | min loss: 0.06109685\n",
      "Epoach: 141 | avg loss: 0.102900125 | max loss: 0.17686051 | min loss: 0.072311014\n",
      "Epoach: 142 | avg loss: 0.10896961 | max loss: 0.14597167 | min loss: 0.06587903\n",
      "Epoach: 143 | avg loss: 0.09672167 | max loss: 0.12844615 | min loss: 0.07328561\n",
      "Epoach: 144 | avg loss: 0.11684843 | max loss: 0.18855992 | min loss: 0.07444431\n",
      "Epoach: 145 | avg loss: 0.09295238 | max loss: 0.12938423 | min loss: 0.059391186\n",
      "Epoach: 146 | avg loss: 0.10752241 | max loss: 0.15036488 | min loss: 0.06845089\n",
      "Epoach: 147 | avg loss: 0.096142456 | max loss: 0.11460549 | min loss: 0.056305\n",
      "Epoach: 148 | avg loss: 0.11001752 | max loss: 0.16283645 | min loss: 0.086647846\n",
      "Epoach: 149 | avg loss: 0.10790098 | max loss: 0.16259769 | min loss: 0.08364259\n",
      "Epoach: 150 | avg loss: 0.087419406 | max loss: 0.13068868 | min loss: 0.04287772\n",
      "Epoach: 151 | avg loss: 0.10257162 | max loss: 0.16701795 | min loss: 0.059701197\n",
      "Epoach: 152 | avg loss: 0.093494356 | max loss: 0.13411956 | min loss: 0.0575029\n",
      "Epoach: 153 | avg loss: 0.09286963 | max loss: 0.116296254 | min loss: 0.067527965\n",
      "Epoach: 154 | avg loss: 0.13265407 | max loss: 0.18505077 | min loss: 0.10313914\n",
      "Epoach: 155 | avg loss: 0.09988881 | max loss: 0.12119356 | min loss: 0.080540724\n",
      "Epoach: 156 | avg loss: 0.09848158 | max loss: 0.12539415 | min loss: 0.0430329\n",
      "Epoach: 157 | avg loss: 0.1194842 | max loss: 0.16175762 | min loss: 0.08171929\n",
      "Epoach: 158 | avg loss: 0.10735426 | max loss: 0.14939997 | min loss: 0.08981554\n",
      "Epoach: 159 | avg loss: 0.09410735 | max loss: 0.13515182 | min loss: 0.062443834\n",
      "Epoach: 160 | avg loss: 0.10518884 | max loss: 0.14404428 | min loss: 0.071505316\n",
      "Epoach: 161 | avg loss: 0.11136063 | max loss: 0.1374845 | min loss: 0.079174\n",
      "Epoach: 162 | avg loss: 0.10092647 | max loss: 0.14107777 | min loss: 0.061350256\n",
      "Epoach: 163 | avg loss: 0.09901797 | max loss: 0.13218644 | min loss: 0.065764144\n",
      "Epoach: 164 | avg loss: 0.10533391 | max loss: 0.15510207 | min loss: 0.073963694\n",
      "Epoach: 165 | avg loss: 0.10273074 | max loss: 0.1256215 | min loss: 0.07989165\n",
      "Epoach: 166 | avg loss: 0.10054114 | max loss: 0.1357202 | min loss: 0.060999446\n",
      "Epoach: 167 | avg loss: 0.09071709 | max loss: 0.11855787 | min loss: 0.077335805\n",
      "Epoach: 168 | avg loss: 0.10096879 | max loss: 0.15328641 | min loss: 0.06782779\n",
      "Epoach: 169 | avg loss: 0.11109594 | max loss: 0.18448196 | min loss: 0.071635\n",
      "Epoach: 170 | avg loss: 0.09774071 | max loss: 0.122312 | min loss: 0.064621665\n",
      "Epoach: 171 | avg loss: 0.107100606 | max loss: 0.13845697 | min loss: 0.06262695\n",
      "Epoach: 172 | avg loss: 0.093554296 | max loss: 0.11347792 | min loss: 0.07340658\n",
      "Epoach: 173 | avg loss: 0.11230466 | max loss: 0.17365119 | min loss: 0.0592336\n",
      "Epoach: 174 | avg loss: 0.11184552 | max loss: 0.17145039 | min loss: 0.065905124\n",
      "Epoach: 175 | avg loss: 0.10087291 | max loss: 0.13813372 | min loss: 0.07985956\n",
      "Epoach: 176 | avg loss: 0.10696697 | max loss: 0.17668882 | min loss: 0.072525874\n",
      "Epoach: 177 | avg loss: 0.103541434 | max loss: 0.15408133 | min loss: 0.07577943\n",
      "Epoach: 178 | avg loss: 0.10487984 | max loss: 0.13395508 | min loss: 0.07840207\n",
      "Epoach: 179 | avg loss: 0.10156215 | max loss: 0.11427263 | min loss: 0.08799457\n",
      "Epoach: 180 | avg loss: 0.101573445 | max loss: 0.15091993 | min loss: 0.052664407\n",
      "Epoach: 181 | avg loss: 0.12167763 | max loss: 0.18922062 | min loss: 0.08031639\n",
      "Epoach: 182 | avg loss: 0.10650312 | max loss: 0.14623383 | min loss: 0.04546663\n",
      "Epoach: 183 | avg loss: 0.1049338 | max loss: 0.1458633 | min loss: 0.07302699\n",
      "Epoach: 184 | avg loss: 0.089882806 | max loss: 0.13232642 | min loss: 0.03861609\n",
      "Epoach: 185 | avg loss: 0.10657928 | max loss: 0.13916816 | min loss: 0.07095079\n",
      "Epoach: 186 | avg loss: 0.10175833 | max loss: 0.11779977 | min loss: 0.07640566\n",
      "Epoach: 187 | avg loss: 0.105438806 | max loss: 0.13969527 | min loss: 0.062993065\n",
      "Epoach: 188 | avg loss: 0.090829805 | max loss: 0.12452654 | min loss: 0.065058134\n",
      "Epoach: 189 | avg loss: 0.09508456 | max loss: 0.12245668 | min loss: 0.07497601\n",
      "Epoach: 190 | avg loss: 0.10266368 | max loss: 0.13938327 | min loss: 0.056319322\n",
      "Epoach: 191 | avg loss: 0.098421715 | max loss: 0.16616522 | min loss: 0.04625917\n",
      "Epoach: 192 | avg loss: 0.11633588 | max loss: 0.16640204 | min loss: 0.07370431\n",
      "Epoach: 193 | avg loss: 0.10134196 | max loss: 0.14759767 | min loss: 0.07389061\n",
      "Epoach: 194 | avg loss: 0.0914502 | max loss: 0.1214828 | min loss: 0.056257013\n",
      "Epoach: 195 | avg loss: 0.0949249 | max loss: 0.14403458 | min loss: 0.049247283\n",
      "Epoach: 196 | avg loss: 0.10618679 | max loss: 0.16726944 | min loss: 0.049284395\n",
      "Epoach: 197 | avg loss: 0.109718576 | max loss: 0.13533147 | min loss: 0.072187826\n",
      "Epoach: 198 | avg loss: 0.10850906 | max loss: 0.1306461 | min loss: 0.08860599\n",
      "Epoach: 199 | avg loss: 0.1119276 | max loss: 0.1653376 | min loss: 0.068253666\n",
      "Epoach: 200 | avg loss: 0.10805361 | max loss: 0.13660672 | min loss: 0.096748956\n",
      "Epoach: 201 | avg loss: 0.108067125 | max loss: 0.18237656 | min loss: 0.058695756\n",
      "Epoach: 202 | avg loss: 0.09134601 | max loss: 0.123734415 | min loss: 0.060500726\n",
      "Epoach: 203 | avg loss: 0.10337788 | max loss: 0.15146276 | min loss: 0.06680953\n",
      "Epoach: 204 | avg loss: 0.09088999 | max loss: 0.13488932 | min loss: 0.04438645\n",
      "Epoach: 205 | avg loss: 0.0911894 | max loss: 0.12657681 | min loss: 0.07145997\n",
      "Epoach: 206 | avg loss: 0.10450883 | max loss: 0.14714415 | min loss: 0.071404815\n",
      "Epoach: 207 | avg loss: 0.12141373 | max loss: 0.16393319 | min loss: 0.09287025\n",
      "Epoach: 208 | avg loss: 0.10165231 | max loss: 0.13187416 | min loss: 0.06684606\n",
      "Epoach: 209 | avg loss: 0.09412184 | max loss: 0.12966609 | min loss: 0.064183325\n",
      "Epoach: 210 | avg loss: 0.09840199 | max loss: 0.1236307 | min loss: 0.077719636\n",
      "Epoach: 211 | avg loss: 0.101199426 | max loss: 0.14415741 | min loss: 0.06877424\n",
      "Epoach: 212 | avg loss: 0.09747147 | max loss: 0.13487731 | min loss: 0.07618614\n",
      "Epoach: 213 | avg loss: 0.08669886 | max loss: 0.116664596 | min loss: 0.05828826\n",
      "Epoach: 214 | avg loss: 0.09177564 | max loss: 0.15729496 | min loss: 0.029696083\n",
      "Epoach: 215 | avg loss: 0.10186516 | max loss: 0.15863463 | min loss: 0.05413175\n",
      "Epoach: 216 | avg loss: 0.10760173 | max loss: 0.13675892 | min loss: 0.0726009\n",
      "Epoach: 217 | avg loss: 0.113231145 | max loss: 0.14628217 | min loss: 0.0851154\n",
      "Epoach: 218 | avg loss: 0.08303565 | max loss: 0.11335366 | min loss: 0.044338513\n",
      "Epoach: 219 | avg loss: 0.10596731 | max loss: 0.12318842 | min loss: 0.08393176\n",
      "Epoach: 220 | avg loss: 0.092974335 | max loss: 0.14933035 | min loss: 0.07083565\n",
      "Epoach: 221 | avg loss: 0.0793442 | max loss: 0.10358328 | min loss: 0.056015108\n",
      "Epoach: 222 | avg loss: 0.10112713 | max loss: 0.15722637 | min loss: 0.05920155\n",
      "Epoach: 223 | avg loss: 0.09686212 | max loss: 0.14727408 | min loss: 0.0666085\n",
      "Epoach: 224 | avg loss: 0.10917578 | max loss: 0.13299759 | min loss: 0.07995947\n",
      "Epoach: 225 | avg loss: 0.10173546 | max loss: 0.1274477 | min loss: 0.07579104\n",
      "Epoach: 226 | avg loss: 0.09269179 | max loss: 0.122178234 | min loss: 0.06766646\n",
      "Epoach: 227 | avg loss: 0.105859675 | max loss: 0.1912986 | min loss: 0.07020916\n",
      "Epoach: 228 | avg loss: 0.0943495 | max loss: 0.12735964 | min loss: 0.0537556\n",
      "Epoach: 229 | avg loss: 0.097286865 | max loss: 0.11580349 | min loss: 0.08082592\n",
      "Epoach: 230 | avg loss: 0.09494276 | max loss: 0.14633943 | min loss: 0.06209961\n",
      "Epoach: 231 | avg loss: 0.09702287 | max loss: 0.13259591 | min loss: 0.0716077\n",
      "Epoach: 232 | avg loss: 0.09924874 | max loss: 0.15197904 | min loss: 0.051797565\n",
      "Epoach: 233 | avg loss: 0.09696025 | max loss: 0.13676493 | min loss: 0.05756273\n",
      "Epoach: 234 | avg loss: 0.10956974 | max loss: 0.15774687 | min loss: 0.08009295\n",
      "Epoach: 235 | avg loss: 0.0997298 | max loss: 0.14019956 | min loss: 0.044473004\n",
      "Epoach: 236 | avg loss: 0.090777695 | max loss: 0.14612362 | min loss: 0.049336065\n",
      "Epoach: 237 | avg loss: 0.09891192 | max loss: 0.16247296 | min loss: 0.06378823\n",
      "Epoach: 238 | avg loss: 0.07741852 | max loss: 0.10915451 | min loss: 0.04417769\n",
      "Epoach: 239 | avg loss: 0.11231985 | max loss: 0.14349169 | min loss: 0.075562954\n",
      "Epoach: 240 | avg loss: 0.09060335 | max loss: 0.108024485 | min loss: 0.050989214\n",
      "Epoach: 241 | avg loss: 0.099112265 | max loss: 0.13195185 | min loss: 0.08218765\n",
      "Epoach: 242 | avg loss: 0.117952675 | max loss: 0.14619592 | min loss: 0.09022345\n",
      "Epoach: 243 | avg loss: 0.09369432 | max loss: 0.1285255 | min loss: 0.053368438\n",
      "Epoach: 244 | avg loss: 0.101520345 | max loss: 0.13520119 | min loss: 0.08379963\n",
      "Epoach: 245 | avg loss: 0.0967459 | max loss: 0.12943517 | min loss: 0.067886084\n",
      "Epoach: 246 | avg loss: 0.0915809 | max loss: 0.1401097 | min loss: 0.06932654\n",
      "Epoach: 247 | avg loss: 0.10581402 | max loss: 0.1565482 | min loss: 0.062699035\n",
      "Epoach: 248 | avg loss: 0.09370225 | max loss: 0.12080685 | min loss: 0.076816745\n",
      "Epoach: 249 | avg loss: 0.11260407 | max loss: 0.20239833 | min loss: 0.08683154\n",
      "Epoach: 250 | avg loss: 0.12841228 | max loss: 0.19759518 | min loss: 0.07147565\n",
      "Epoach: 251 | avg loss: 0.08319229 | max loss: 0.17330956 | min loss: 0.057029165\n",
      "Epoach: 252 | avg loss: 0.104011476 | max loss: 0.13876112 | min loss: 0.07639765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoach: 253 | avg loss: 0.12833104 | max loss: 0.24614295 | min loss: 0.0934112\n",
      "Epoach: 254 | avg loss: 0.11724376 | max loss: 0.17380525 | min loss: 0.07709921\n",
      "Epoach: 255 | avg loss: 0.112171225 | max loss: 0.18124862 | min loss: 0.055527654\n",
      "Epoach: 256 | avg loss: 0.11262966 | max loss: 0.19989505 | min loss: 0.065707095\n",
      "Epoach: 257 | avg loss: 0.11024994 | max loss: 0.17043734 | min loss: 0.05789831\n",
      "Epoach: 258 | avg loss: 0.123399734 | max loss: 0.1557032 | min loss: 0.0762832\n",
      "Epoach: 259 | avg loss: 0.101884246 | max loss: 0.1473418 | min loss: 0.06542783\n",
      "Epoach: 260 | avg loss: 0.10480735 | max loss: 0.15775296 | min loss: 0.07111898\n",
      "Epoach: 261 | avg loss: 0.09542991 | max loss: 0.12660466 | min loss: 0.075599164\n",
      "Epoach: 262 | avg loss: 0.1210182 | max loss: 0.17749389 | min loss: 0.06298205\n",
      "Epoach: 263 | avg loss: 0.1021522 | max loss: 0.1728346 | min loss: 0.065358765\n",
      "Epoach: 264 | avg loss: 0.10175727 | max loss: 0.14613143 | min loss: 0.07817303\n",
      "Epoach: 265 | avg loss: 0.09193308 | max loss: 0.12321667 | min loss: 0.063248925\n",
      "Epoach: 266 | avg loss: 0.12662788 | max loss: 0.15974806 | min loss: 0.09322629\n",
      "Epoach: 267 | avg loss: 0.096928775 | max loss: 0.12039516 | min loss: 0.08455774\n",
      "Epoach: 268 | avg loss: 0.11876829 | max loss: 0.16218363 | min loss: 0.076181576\n",
      "Epoach: 269 | avg loss: 0.123890355 | max loss: 0.17235436 | min loss: 0.08051309\n",
      "Epoach: 270 | avg loss: 0.11983079 | max loss: 0.1660204 | min loss: 0.0903559\n",
      "Epoach: 271 | avg loss: 0.1013785 | max loss: 0.12306838 | min loss: 0.06671774\n",
      "Epoach: 272 | avg loss: 0.12478306 | max loss: 0.16844334 | min loss: 0.06835256\n",
      "Epoach: 273 | avg loss: 0.09860836 | max loss: 0.11736826 | min loss: 0.0779014\n",
      "Epoach: 274 | avg loss: 0.10427151 | max loss: 0.20584169 | min loss: 0.044762302\n",
      "Epoach: 275 | avg loss: 0.103100196 | max loss: 0.14072226 | min loss: 0.079066366\n",
      "Epoach: 276 | avg loss: 0.109462574 | max loss: 0.1366608 | min loss: 0.044994857\n",
      "Epoach: 277 | avg loss: 0.117894985 | max loss: 0.17784077 | min loss: 0.06500742\n",
      "Epoach: 278 | avg loss: 0.121291935 | max loss: 0.17192942 | min loss: 0.08475112\n",
      "Epoach: 279 | avg loss: 0.11614558 | max loss: 0.16499414 | min loss: 0.06152623\n",
      "Epoach: 280 | avg loss: 0.11690228 | max loss: 0.17423938 | min loss: 0.08452561\n",
      "Epoach: 281 | avg loss: 0.09338845 | max loss: 0.15698984 | min loss: 0.06737619\n",
      "Epoach: 282 | avg loss: 0.10353719 | max loss: 0.14206684 | min loss: 0.06715773\n",
      "Epoach: 283 | avg loss: 0.089844234 | max loss: 0.14475685 | min loss: 0.04750999\n",
      "Epoach: 284 | avg loss: 0.102650866 | max loss: 0.17267549 | min loss: 0.048702292\n",
      "Epoach: 285 | avg loss: 0.09212196 | max loss: 0.11777888 | min loss: 0.05582382\n",
      "Epoach: 286 | avg loss: 0.10579778 | max loss: 0.14416249 | min loss: 0.048060764\n",
      "Epoach: 287 | avg loss: 0.108318605 | max loss: 0.18301651 | min loss: 0.08110485\n",
      "Epoach: 288 | avg loss: 0.108272046 | max loss: 0.12658557 | min loss: 0.07982288\n",
      "Epoach: 289 | avg loss: 0.07961036 | max loss: 0.10479206 | min loss: 0.04127873\n",
      "Epoach: 290 | avg loss: 0.08508295 | max loss: 0.11429747 | min loss: 0.06576832\n",
      "Epoach: 291 | avg loss: 0.11156942 | max loss: 0.17411634 | min loss: 0.07690608\n",
      "Epoach: 292 | avg loss: 0.0829436 | max loss: 0.123708494 | min loss: 0.043931395\n",
      "Epoach: 293 | avg loss: 0.11947176 | max loss: 0.17998946 | min loss: 0.09098755\n",
      "Epoach: 294 | avg loss: 0.10442075 | max loss: 0.14934386 | min loss: 0.04970112\n",
      "Epoach: 295 | avg loss: 0.09738163 | max loss: 0.14173093 | min loss: 0.062142402\n",
      "Epoach: 296 | avg loss: 0.10829666 | max loss: 0.17437097 | min loss: 0.07017641\n",
      "Epoach: 297 | avg loss: 0.09395446 | max loss: 0.13482699 | min loss: 0.06930309\n",
      "Epoach: 298 | avg loss: 0.12906033 | max loss: 0.2115992 | min loss: 0.07979551\n",
      "Epoach: 299 | avg loss: 0.12340632 | max loss: 0.1778889 | min loss: 0.07636512\n",
      "Epoach: 300 | avg loss: 0.097318776 | max loss: 0.13208516 | min loss: 0.055921026\n",
      "Epoach: 301 | avg loss: 0.12363882 | max loss: 0.17012784 | min loss: 0.085071355\n",
      "Epoach: 302 | avg loss: 0.11121773 | max loss: 0.14193127 | min loss: 0.07249377\n",
      "Epoach: 303 | avg loss: 0.097823925 | max loss: 0.14493689 | min loss: 0.07900192\n",
      "Epoach: 304 | avg loss: 0.11433712 | max loss: 0.14284019 | min loss: 0.07724537\n",
      "Epoach: 305 | avg loss: 0.1162034 | max loss: 0.13923319 | min loss: 0.07751018\n",
      "Epoach: 306 | avg loss: 0.108215444 | max loss: 0.13894157 | min loss: 0.08012954\n",
      "Epoach: 307 | avg loss: 0.11029222 | max loss: 0.17067163 | min loss: 0.07134023\n",
      "Epoach: 308 | avg loss: 0.11872777 | max loss: 0.1706681 | min loss: 0.0861164\n",
      "Epoach: 309 | avg loss: 0.094919994 | max loss: 0.13692045 | min loss: 0.061477363\n",
      "Epoach: 310 | avg loss: 0.10355705 | max loss: 0.15468894 | min loss: 0.04154553\n",
      "Epoach: 311 | avg loss: 0.12522516 | max loss: 0.19716503 | min loss: 0.06721941\n",
      "Epoach: 312 | avg loss: 0.100793645 | max loss: 0.1407467 | min loss: 0.056042686\n",
      "Epoach: 313 | avg loss: 0.10902841 | max loss: 0.16146137 | min loss: 0.051392462\n",
      "Epoach: 314 | avg loss: 0.09961805 | max loss: 0.15791266 | min loss: 0.06664481\n",
      "Epoach: 315 | avg loss: 0.0998264 | max loss: 0.1270162 | min loss: 0.061375678\n",
      "Epoach: 316 | avg loss: 0.103988856 | max loss: 0.1818383 | min loss: 0.07199573\n",
      "Epoach: 317 | avg loss: 0.10037816 | max loss: 0.12068441 | min loss: 0.076579474\n",
      "Epoach: 318 | avg loss: 0.10016051 | max loss: 0.12124941 | min loss: 0.06576042\n",
      "Epoach: 319 | avg loss: 0.10621226 | max loss: 0.21554585 | min loss: 0.06066658\n",
      "Epoach: 320 | avg loss: 0.117797025 | max loss: 0.14832139 | min loss: 0.09700202\n",
      "Epoach: 321 | avg loss: 0.1117415 | max loss: 0.15201879 | min loss: 0.074506626\n",
      "Epoach: 322 | avg loss: 0.10744366 | max loss: 0.16673903 | min loss: 0.06332594\n",
      "Epoach: 323 | avg loss: 0.08149476 | max loss: 0.121671125 | min loss: 0.045861788\n",
      "Epoach: 324 | avg loss: 0.09506313 | max loss: 0.119518206 | min loss: 0.072859675\n",
      "Epoach: 325 | avg loss: 0.090833806 | max loss: 0.14187786 | min loss: 0.039021753\n",
      "Epoach: 326 | avg loss: 0.10812673 | max loss: 0.14980108 | min loss: 0.07638466\n",
      "Epoach: 327 | avg loss: 0.11610901 | max loss: 0.13182595 | min loss: 0.09803629\n",
      "Epoach: 328 | avg loss: 0.11526446 | max loss: 0.15623853 | min loss: 0.078987546\n",
      "Epoach: 329 | avg loss: 0.11082272 | max loss: 0.15344228 | min loss: 0.08282715\n",
      "Epoach: 330 | avg loss: 0.09899141 | max loss: 0.13214834 | min loss: 0.05353259\n",
      "Epoach: 331 | avg loss: 0.11129923 | max loss: 0.17859066 | min loss: 0.041113216\n",
      "Epoach: 332 | avg loss: 0.09559166 | max loss: 0.121000506 | min loss: 0.06372276\n",
      "Epoach: 333 | avg loss: 0.11090155 | max loss: 0.16670433 | min loss: 0.07656429\n",
      "Epoach: 334 | avg loss: 0.12582268 | max loss: 0.20839252 | min loss: 0.06350871\n",
      "Epoach: 335 | avg loss: 0.0923293 | max loss: 0.12351185 | min loss: 0.06453033\n",
      "Epoach: 336 | avg loss: 0.11335676 | max loss: 0.1490889 | min loss: 0.055486836\n",
      "Epoach: 337 | avg loss: 0.107909724 | max loss: 0.14285517 | min loss: 0.085877925\n",
      "Epoach: 338 | avg loss: 0.09691908 | max loss: 0.1383508 | min loss: 0.06715219\n",
      "Epoach: 339 | avg loss: 0.09340436 | max loss: 0.14816792 | min loss: 0.05538634\n",
      "Epoach: 340 | avg loss: 0.10817162 | max loss: 0.17343925 | min loss: 0.046823326\n",
      "Epoach: 341 | avg loss: 0.11115892 | max loss: 0.15058929 | min loss: 0.080696665\n",
      "Epoach: 342 | avg loss: 0.09190059 | max loss: 0.10926985 | min loss: 0.07617847\n",
      "Epoach: 343 | avg loss: 0.10468502 | max loss: 0.12876746 | min loss: 0.07229045\n",
      "Epoach: 344 | avg loss: 0.09720032 | max loss: 0.12777275 | min loss: 0.07518842\n",
      "Epoach: 345 | avg loss: 0.09683347 | max loss: 0.14541544 | min loss: 0.048013262\n",
      "Epoach: 346 | avg loss: 0.10536146 | max loss: 0.1626854 | min loss: 0.057457432\n",
      "Epoach: 347 | avg loss: 0.11285551 | max loss: 0.15029503 | min loss: 0.08087413\n",
      "Epoach: 348 | avg loss: 0.1128153 | max loss: 0.15155102 | min loss: 0.07330294\n",
      "Epoach: 349 | avg loss: 0.092823595 | max loss: 0.1328105 | min loss: 0.055147313\n",
      "Epoach: 350 | avg loss: 0.1061633 | max loss: 0.15377386 | min loss: 0.06562915\n",
      "Epoach: 351 | avg loss: 0.10217047 | max loss: 0.12902525 | min loss: 0.07207771\n",
      "Epoach: 352 | avg loss: 0.08736557 | max loss: 0.12993768 | min loss: 0.043799765\n",
      "Epoach: 353 | avg loss: 0.111470416 | max loss: 0.15564206 | min loss: 0.05214241\n",
      "Epoach: 354 | avg loss: 0.10422311 | max loss: 0.14061902 | min loss: 0.07110079\n",
      "Epoach: 355 | avg loss: 0.105489 | max loss: 0.17587028 | min loss: 0.06955047\n",
      "Epoach: 356 | avg loss: 0.086416215 | max loss: 0.13749464 | min loss: 0.0549401\n",
      "Epoach: 357 | avg loss: 0.0906132 | max loss: 0.15577622 | min loss: 0.06949361\n",
      "Epoach: 358 | avg loss: 0.09834069 | max loss: 0.14738065 | min loss: 0.063154474\n",
      "Epoach: 359 | avg loss: 0.11016463 | max loss: 0.17257498 | min loss: 0.059685975\n",
      "Epoach: 360 | avg loss: 0.116862595 | max loss: 0.19546178 | min loss: 0.08006248\n",
      "Epoach: 361 | avg loss: 0.093835786 | max loss: 0.109368615 | min loss: 0.058478553\n",
      "Epoach: 362 | avg loss: 0.10391523 | max loss: 0.1714407 | min loss: 0.07200498\n",
      "Epoach: 363 | avg loss: 0.11849396 | max loss: 0.15772778 | min loss: 0.083132245\n",
      "Epoach: 364 | avg loss: 0.10749446 | max loss: 0.21889013 | min loss: 0.057292182\n",
      "Epoach: 365 | avg loss: 0.09846696 | max loss: 0.13796355 | min loss: 0.070237756\n",
      "Epoach: 366 | avg loss: 0.10800594 | max loss: 0.14688367 | min loss: 0.065578446\n",
      "Epoach: 367 | avg loss: 0.090975225 | max loss: 0.16673012 | min loss: 0.05106787\n",
      "Epoach: 368 | avg loss: 0.10008061 | max loss: 0.1358778 | min loss: 0.058936283\n",
      "Epoach: 369 | avg loss: 0.11008082 | max loss: 0.17318425 | min loss: 0.05592556\n",
      "Epoach: 370 | avg loss: 0.10070242 | max loss: 0.13177529 | min loss: 0.06590059\n",
      "Epoach: 371 | avg loss: 0.101384684 | max loss: 0.12782331 | min loss: 0.08538759\n",
      "Epoach: 372 | avg loss: 0.10269343 | max loss: 0.16674016 | min loss: 0.06211669\n",
      "Epoach: 373 | avg loss: 0.08975202 | max loss: 0.14172998 | min loss: 0.062548555\n",
      "Epoach: 374 | avg loss: 0.08971307 | max loss: 0.13538261 | min loss: 0.059459265\n",
      "Epoach: 375 | avg loss: 0.11016417 | max loss: 0.15418294 | min loss: 0.053111587\n",
      "Epoach: 376 | avg loss: 0.096084766 | max loss: 0.13593481 | min loss: 0.05395782\n",
      "Epoach: 377 | avg loss: 0.095470205 | max loss: 0.13082592 | min loss: 0.06526968\n",
      "Epoach: 378 | avg loss: 0.1114707 | max loss: 0.16035223 | min loss: 0.06726796\n",
      "Epoach: 379 | avg loss: 0.109200716 | max loss: 0.16045699 | min loss: 0.06684103\n",
      "Epoach: 380 | avg loss: 0.09354499 | max loss: 0.14967091 | min loss: 0.064545296\n",
      "Epoach: 381 | avg loss: 0.11428091 | max loss: 0.14447595 | min loss: 0.099058546\n",
      "Epoach: 382 | avg loss: 0.092246294 | max loss: 0.10946555 | min loss: 0.07765019\n",
      "Epoach: 383 | avg loss: 0.10099957 | max loss: 0.15531012 | min loss: 0.046552435\n",
      "Epoach: 384 | avg loss: 0.09218459 | max loss: 0.16450864 | min loss: 0.04736992\n",
      "Epoach: 385 | avg loss: 0.10339084 | max loss: 0.15537731 | min loss: 0.08076697\n",
      "Epoach: 386 | avg loss: 0.11061065 | max loss: 0.15423696 | min loss: 0.08772038\n",
      "Epoach: 387 | avg loss: 0.10557858 | max loss: 0.13244404 | min loss: 0.0625647\n",
      "Epoach: 388 | avg loss: 0.10720171 | max loss: 0.19494362 | min loss: 0.068670906\n",
      "Epoach: 389 | avg loss: 0.104478434 | max loss: 0.17074184 | min loss: 0.060304258\n",
      "Epoach: 390 | avg loss: 0.106606565 | max loss: 0.13510263 | min loss: 0.08146422\n",
      "Epoach: 391 | avg loss: 0.12632176 | max loss: 0.17663842 | min loss: 0.10126125\n",
      "Epoach: 392 | avg loss: 0.103706345 | max loss: 0.13271643 | min loss: 0.06367671\n",
      "Epoach: 393 | avg loss: 0.103484824 | max loss: 0.13504672 | min loss: 0.06786113\n",
      "Epoach: 394 | avg loss: 0.112421215 | max loss: 0.2085063 | min loss: 0.0690721\n",
      "Epoach: 395 | avg loss: 0.10975595 | max loss: 0.12907074 | min loss: 0.09023307\n",
      "Epoach: 396 | avg loss: 0.08223595 | max loss: 0.10630182 | min loss: 0.059237283\n",
      "Epoach: 397 | avg loss: 0.08650397 | max loss: 0.11175973 | min loss: 0.06908433\n",
      "Epoach: 398 | avg loss: 0.11404097 | max loss: 0.15704513 | min loss: 0.036464963\n",
      "Epoach: 399 | avg loss: 0.108944744 | max loss: 0.15603979 | min loss: 0.07570466\n",
      "Epoach: 400 | avg loss: 0.1199363 | max loss: 0.17766953 | min loss: 0.062814154\n",
      "Epoach: 401 | avg loss: 0.100976765 | max loss: 0.15532789 | min loss: 0.066089235\n",
      "Epoach: 402 | avg loss: 0.108384505 | max loss: 0.16513658 | min loss: 0.058073997\n",
      "Epoach: 403 | avg loss: 0.110074 | max loss: 0.14473017 | min loss: 0.079460934\n",
      "Epoach: 404 | avg loss: 0.12496139 | max loss: 0.15188985 | min loss: 0.10707518\n",
      "Epoach: 405 | avg loss: 0.11616263 | max loss: 0.14332938 | min loss: 0.076868124\n",
      "Epoach: 406 | avg loss: 0.09511712 | max loss: 0.14078712 | min loss: 0.059739\n",
      "Epoach: 407 | avg loss: 0.08971417 | max loss: 0.12035762 | min loss: 0.050472226\n",
      "Epoach: 408 | avg loss: 0.10001196 | max loss: 0.15326887 | min loss: 0.061852396\n",
      "Epoach: 409 | avg loss: 0.098864265 | max loss: 0.12432897 | min loss: 0.07378994\n",
      "Epoach: 410 | avg loss: 0.11800078 | max loss: 0.20581783 | min loss: 0.08003096\n",
      "Epoach: 411 | avg loss: 0.11024167 | max loss: 0.15123048 | min loss: 0.08530954\n",
      "Epoach: 412 | avg loss: 0.12967218 | max loss: 0.1924094 | min loss: 0.0861704\n",
      "Epoach: 413 | avg loss: 0.110688165 | max loss: 0.17427954 | min loss: 0.059799768\n",
      "Epoach: 414 | avg loss: 0.11777164 | max loss: 0.15230538 | min loss: 0.08388391\n",
      "Epoach: 415 | avg loss: 0.0909451 | max loss: 0.15109813 | min loss: 0.053735442\n",
      "Epoach: 416 | avg loss: 0.12464255 | max loss: 0.18734343 | min loss: 0.08815926\n",
      "Epoach: 417 | avg loss: 0.101941615 | max loss: 0.13640742 | min loss: 0.07121658\n",
      "Epoach: 418 | avg loss: 0.10117501 | max loss: 0.14927861 | min loss: 0.060960658\n",
      "Epoach: 419 | avg loss: 0.10250238 | max loss: 0.18072255 | min loss: 0.06523177\n",
      "Epoach: 420 | avg loss: 0.116418496 | max loss: 0.14348483 | min loss: 0.084441714\n",
      "Epoach: 421 | avg loss: 0.099751845 | max loss: 0.13789493 | min loss: 0.06809156\n",
      "Epoach: 422 | avg loss: 0.09812973 | max loss: 0.1411438 | min loss: 0.067747526\n",
      "Epoach: 423 | avg loss: 0.094124585 | max loss: 0.1480903 | min loss: 0.07004557\n",
      "Epoach: 424 | avg loss: 0.11774875 | max loss: 0.17080669 | min loss: 0.07487704\n",
      "Epoach: 425 | avg loss: 0.1050459 | max loss: 0.15359533 | min loss: 0.061183598\n",
      "Epoach: 426 | avg loss: 0.113974884 | max loss: 0.17901787 | min loss: 0.07703273\n",
      "Epoach: 427 | avg loss: 0.08070792 | max loss: 0.11077277 | min loss: 0.042509098\n",
      "Epoach: 428 | avg loss: 0.12084276 | max loss: 0.15995595 | min loss: 0.08051547\n",
      "Epoach: 429 | avg loss: 0.12220128 | max loss: 0.18992357 | min loss: 0.08897345\n",
      "Epoach: 430 | avg loss: 0.110339254 | max loss: 0.13621558 | min loss: 0.07921787\n",
      "Epoach: 431 | avg loss: 0.123192504 | max loss: 0.16434415 | min loss: 0.08598245\n",
      "Epoach: 432 | avg loss: 0.098945245 | max loss: 0.12399402 | min loss: 0.087721236\n",
      "Epoach: 433 | avg loss: 0.11240323 | max loss: 0.19312437 | min loss: 0.06684283\n",
      "Epoach: 434 | avg loss: 0.102808 | max loss: 0.1478045 | min loss: 0.07321678\n",
      "Epoach: 435 | avg loss: 0.08770861 | max loss: 0.13456817 | min loss: 0.050176494\n",
      "Epoach: 436 | avg loss: 0.10374239 | max loss: 0.13092856 | min loss: 0.06878349\n",
      "Epoach: 437 | avg loss: 0.11083535 | max loss: 0.15066761 | min loss: 0.06474236\n",
      "Epoach: 438 | avg loss: 0.10498603 | max loss: 0.14403272 | min loss: 0.07894804\n",
      "Epoach: 439 | avg loss: 0.0859219 | max loss: 0.11276218 | min loss: 0.049418136\n",
      "Epoach: 440 | avg loss: 0.10752842 | max loss: 0.14207585 | min loss: 0.067395486\n",
      "Epoach: 441 | avg loss: 0.09891732 | max loss: 0.12280718 | min loss: 0.059169557\n",
      "Epoach: 442 | avg loss: 0.10813889 | max loss: 0.14512593 | min loss: 0.07542148\n",
      "Epoach: 443 | avg loss: 0.10513307 | max loss: 0.16954431 | min loss: 0.045799907\n",
      "Epoach: 444 | avg loss: 0.10841544 | max loss: 0.20578846 | min loss: 0.07043934\n",
      "Epoach: 445 | avg loss: 0.104131125 | max loss: 0.12667948 | min loss: 0.06785643\n",
      "Epoach: 446 | avg loss: 0.08772404 | max loss: 0.12543982 | min loss: 0.05450356\n",
      "Epoach: 447 | avg loss: 0.110757016 | max loss: 0.15353172 | min loss: 0.0707722\n",
      "Epoach: 448 | avg loss: 0.10594415 | max loss: 0.15812443 | min loss: 0.05481056\n",
      "Epoach: 449 | avg loss: 0.11193009 | max loss: 0.16163346 | min loss: 0.06473881\n",
      "Epoach: 450 | avg loss: 0.09748364 | max loss: 0.1420793 | min loss: 0.06500163\n",
      "Epoach: 451 | avg loss: 0.110254064 | max loss: 0.12963316 | min loss: 0.09442234\n",
      "Epoach: 452 | avg loss: 0.09596509 | max loss: 0.12664467 | min loss: 0.0723444\n",
      "Epoach: 453 | avg loss: 0.09509761 | max loss: 0.110092275 | min loss: 0.07777115\n",
      "Epoach: 454 | avg loss: 0.08645494 | max loss: 0.123690896 | min loss: 0.03450962\n",
      "Epoach: 455 | avg loss: 0.09740497 | max loss: 0.12884139 | min loss: 0.06656906\n",
      "Epoach: 456 | avg loss: 0.11522224 | max loss: 0.1492937 | min loss: 0.08502863\n",
      "Epoach: 457 | avg loss: 0.116402894 | max loss: 0.16984251 | min loss: 0.06090502\n",
      "Epoach: 458 | avg loss: 0.106913224 | max loss: 0.14516956 | min loss: 0.05296205\n",
      "Epoach: 459 | avg loss: 0.108083546 | max loss: 0.14046341 | min loss: 0.083585046\n",
      "Epoach: 460 | avg loss: 0.092723556 | max loss: 0.1499234 | min loss: 0.06822901\n",
      "Epoach: 461 | avg loss: 0.09688613 | max loss: 0.12358087 | min loss: 0.06534784\n",
      "Epoach: 462 | avg loss: 0.11614266 | max loss: 0.19560961 | min loss: 0.085354514\n",
      "Epoach: 463 | avg loss: 0.110750586 | max loss: 0.21935992 | min loss: 0.05999062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoach: 464 | avg loss: 0.11019301 | max loss: 0.18256463 | min loss: 0.0714548\n",
      "Epoach: 465 | avg loss: 0.120230906 | max loss: 0.14045869 | min loss: 0.09243393\n",
      "Epoach: 466 | avg loss: 0.11689432 | max loss: 0.15204023 | min loss: 0.08564075\n",
      "Epoach: 467 | avg loss: 0.100148946 | max loss: 0.14812148 | min loss: 0.06808079\n",
      "Epoach: 468 | avg loss: 0.10733463 | max loss: 0.15563832 | min loss: 0.083371356\n",
      "Epoach: 469 | avg loss: 0.10300331 | max loss: 0.13063261 | min loss: 0.07354251\n",
      "Epoach: 470 | avg loss: 0.10525713 | max loss: 0.13429189 | min loss: 0.058120422\n",
      "Epoach: 471 | avg loss: 0.11827902 | max loss: 0.15857972 | min loss: 0.09311345\n",
      "Epoach: 472 | avg loss: 0.11514536 | max loss: 0.14596418 | min loss: 0.05639213\n",
      "Epoach: 473 | avg loss: 0.10698302 | max loss: 0.17552999 | min loss: 0.04539504\n",
      "Epoach: 474 | avg loss: 0.103786275 | max loss: 0.12186554 | min loss: 0.08961175\n",
      "Epoach: 475 | avg loss: 0.10541556 | max loss: 0.13271034 | min loss: 0.074856214\n",
      "Epoach: 476 | avg loss: 0.117807634 | max loss: 0.13892187 | min loss: 0.08999219\n",
      "Epoach: 477 | avg loss: 0.10052994 | max loss: 0.13715705 | min loss: 0.079200916\n",
      "Epoach: 478 | avg loss: 0.0832157 | max loss: 0.122611664 | min loss: 0.052761763\n",
      "Epoach: 479 | avg loss: 0.104094535 | max loss: 0.14497297 | min loss: 0.073132545\n",
      "Epoach: 480 | avg loss: 0.082942486 | max loss: 0.11456529 | min loss: 0.054528456\n",
      "Epoach: 481 | avg loss: 0.11477398 | max loss: 0.1868238 | min loss: 0.066421926\n",
      "Epoach: 482 | avg loss: 0.11598678 | max loss: 0.15362841 | min loss: 0.081177145\n",
      "Epoach: 483 | avg loss: 0.10706243 | max loss: 0.21559055 | min loss: 0.061650265\n",
      "Epoach: 484 | avg loss: 0.10525679 | max loss: 0.13240391 | min loss: 0.055048015\n",
      "Epoach: 485 | avg loss: 0.10595534 | max loss: 0.15691322 | min loss: 0.0586397\n",
      "Epoach: 486 | avg loss: 0.103346325 | max loss: 0.12943967 | min loss: 0.088970296\n",
      "Epoach: 487 | avg loss: 0.10681388 | max loss: 0.15552604 | min loss: 0.07558304\n",
      "Epoach: 488 | avg loss: 0.10098855 | max loss: 0.123167306 | min loss: 0.0703517\n",
      "Epoach: 489 | avg loss: 0.087577745 | max loss: 0.108397946 | min loss: 0.0709351\n",
      "Epoach: 490 | avg loss: 0.101662815 | max loss: 0.13726646 | min loss: 0.06604943\n",
      "Epoach: 491 | avg loss: 0.10884875 | max loss: 0.17355807 | min loss: 0.056135036\n",
      "Epoach: 492 | avg loss: 0.08663415 | max loss: 0.14266446 | min loss: 0.039794493\n",
      "Epoach: 493 | avg loss: 0.10452918 | max loss: 0.16791521 | min loss: 0.05744909\n",
      "Epoach: 494 | avg loss: 0.10598685 | max loss: 0.13411349 | min loss: 0.08371237\n",
      "Epoach: 495 | avg loss: 0.09923719 | max loss: 0.12878853 | min loss: 0.07797831\n",
      "Epoach: 496 | avg loss: 0.07603977 | max loss: 0.117383346 | min loss: 0.055905387\n",
      "Epoach: 497 | avg loss: 0.118103325 | max loss: 0.17110302 | min loss: 0.081628636\n",
      "Epoach: 498 | avg loss: 0.12703404 | max loss: 0.22964458 | min loss: 0.0743806\n",
      "Epoach: 499 | avg loss: 0.11317023 | max loss: 0.16419841 | min loss: 0.07979868\n",
      "Epoach: 500 | avg loss: 0.079247594 | max loss: 0.10263063 | min loss: 0.044289876\n",
      "Epoach: 501 | avg loss: 0.091917366 | max loss: 0.14866388 | min loss: 0.06221117\n",
      "Epoach: 502 | avg loss: 0.109547794 | max loss: 0.18029135 | min loss: 0.059892703\n",
      "Epoach: 503 | avg loss: 0.09732813 | max loss: 0.13706723 | min loss: 0.0700654\n",
      "Epoach: 504 | avg loss: 0.10682398 | max loss: 0.16219522 | min loss: 0.0767611\n",
      "Epoach: 505 | avg loss: 0.101607926 | max loss: 0.16576329 | min loss: 0.07338047\n",
      "Epoach: 506 | avg loss: 0.1119539 | max loss: 0.15409179 | min loss: 0.06264088\n",
      "Epoach: 507 | avg loss: 0.1119651 | max loss: 0.15344319 | min loss: 0.02826218\n",
      "Epoach: 508 | avg loss: 0.08365874 | max loss: 0.106173895 | min loss: 0.035096634\n",
      "Epoach: 509 | avg loss: 0.07720337 | max loss: 0.09197743 | min loss: 0.053037625\n",
      "Epoach: 510 | avg loss: 0.09659525 | max loss: 0.14617951 | min loss: 0.040468097\n",
      "Epoach: 511 | avg loss: 0.09043363 | max loss: 0.15013857 | min loss: 0.05660859\n",
      "Epoach: 512 | avg loss: 0.116964385 | max loss: 0.17745842 | min loss: 0.050129127\n",
      "Epoach: 513 | avg loss: 0.116682366 | max loss: 0.1955635 | min loss: 0.081328884\n",
      "Epoach: 514 | avg loss: 0.09292643 | max loss: 0.149352 | min loss: 0.05582592\n",
      "Epoach: 515 | avg loss: 0.088417664 | max loss: 0.14851904 | min loss: 0.049387187\n",
      "Epoach: 516 | avg loss: 0.11638035 | max loss: 0.21103112 | min loss: 0.074351266\n",
      "Epoach: 517 | avg loss: 0.09877902 | max loss: 0.1263559 | min loss: 0.06054539\n",
      "Epoach: 518 | avg loss: 0.12305852 | max loss: 0.1843068 | min loss: 0.07598192\n",
      "Epoach: 519 | avg loss: 0.0867938 | max loss: 0.10597549 | min loss: 0.048510723\n",
      "Epoach: 520 | avg loss: 0.112046644 | max loss: 0.15575173 | min loss: 0.03373412\n",
      "Epoach: 521 | avg loss: 0.09343766 | max loss: 0.13044395 | min loss: 0.0723351\n",
      "Epoach: 522 | avg loss: 0.11954844 | max loss: 0.18232067 | min loss: 0.07473884\n",
      "Epoach: 523 | avg loss: 0.093950905 | max loss: 0.14696975 | min loss: 0.051636174\n",
      "Epoach: 524 | avg loss: 0.10315708 | max loss: 0.15256307 | min loss: 0.062040396\n",
      "Epoach: 525 | avg loss: 0.10554107 | max loss: 0.18045056 | min loss: 0.06277265\n",
      "Epoach: 526 | avg loss: 0.110781625 | max loss: 0.14953038 | min loss: 0.072751574\n",
      "Epoach: 527 | avg loss: 0.1071872 | max loss: 0.14969155 | min loss: 0.078909636\n",
      "Epoach: 528 | avg loss: 0.10783343 | max loss: 0.16448435 | min loss: 0.055652644\n",
      "Epoach: 529 | avg loss: 0.118749335 | max loss: 0.15766534 | min loss: 0.081361175\n",
      "Epoach: 530 | avg loss: 0.12080039 | max loss: 0.18937719 | min loss: 0.07129044\n",
      "Epoach: 531 | avg loss: 0.101212054 | max loss: 0.12569742 | min loss: 0.06305767\n",
      "Epoach: 532 | avg loss: 0.10364905 | max loss: 0.14909951 | min loss: 0.07246139\n",
      "Epoach: 533 | avg loss: 0.107285336 | max loss: 0.15181625 | min loss: 0.067610204\n",
      "Epoach: 534 | avg loss: 0.11040266 | max loss: 0.15476994 | min loss: 0.077082105\n",
      "Epoach: 535 | avg loss: 0.098143324 | max loss: 0.1473384 | min loss: 0.063861564\n",
      "Epoach: 536 | avg loss: 0.1049833 | max loss: 0.13388206 | min loss: 0.07902762\n",
      "Epoach: 537 | avg loss: 0.098732084 | max loss: 0.12887093 | min loss: 0.059487466\n",
      "Epoach: 538 | avg loss: 0.089153044 | max loss: 0.14400993 | min loss: 0.047322623\n",
      "Epoach: 539 | avg loss: 0.11639609 | max loss: 0.18844339 | min loss: 0.081058025\n",
      "Epoach: 540 | avg loss: 0.108665116 | max loss: 0.12268725 | min loss: 0.08023071\n",
      "Epoach: 541 | avg loss: 0.099115536 | max loss: 0.15945561 | min loss: 0.048639644\n",
      "Epoach: 542 | avg loss: 0.09227081 | max loss: 0.17229025 | min loss: 0.030404037\n",
      "Epoach: 543 | avg loss: 0.08501221 | max loss: 0.10754377 | min loss: 0.033788968\n",
      "Epoach: 544 | avg loss: 0.10577573 | max loss: 0.14037873 | min loss: 0.07341849\n",
      "Epoach: 545 | avg loss: 0.109680384 | max loss: 0.14195119 | min loss: 0.0750491\n",
      "Epoach: 546 | avg loss: 0.092886336 | max loss: 0.12369095 | min loss: 0.0531196\n",
      "Epoach: 547 | avg loss: 0.10364004 | max loss: 0.127379 | min loss: 0.08893132\n",
      "Epoach: 548 | avg loss: 0.10163774 | max loss: 0.14657596 | min loss: 0.0688657\n",
      "Epoach: 549 | avg loss: 0.10836832 | max loss: 0.17392325 | min loss: 0.06641904\n",
      "Epoach: 550 | avg loss: 0.10306023 | max loss: 0.14647993 | min loss: 0.06814952\n",
      "Epoach: 551 | avg loss: 0.1262232 | max loss: 0.17678429 | min loss: 0.03773277\n",
      "Epoach: 552 | avg loss: 0.07694301 | max loss: 0.10640078 | min loss: 0.03971998\n",
      "Epoach: 553 | avg loss: 0.11553517 | max loss: 0.18031052 | min loss: 0.050966714\n",
      "Epoach: 554 | avg loss: 0.103068456 | max loss: 0.13127328 | min loss: 0.075840004\n",
      "Epoach: 555 | avg loss: 0.109031305 | max loss: 0.13640709 | min loss: 0.08505468\n",
      "Epoach: 556 | avg loss: 0.10054439 | max loss: 0.13948314 | min loss: 0.058378503\n",
      "Epoach: 557 | avg loss: 0.11090155 | max loss: 0.16830094 | min loss: 0.06515334\n",
      "Epoach: 558 | avg loss: 0.09931197 | max loss: 0.13863161 | min loss: 0.077462755\n",
      "Epoach: 559 | avg loss: 0.09223287 | max loss: 0.12804465 | min loss: 0.03471351\n",
      "Epoach: 560 | avg loss: 0.10075122 | max loss: 0.1364942 | min loss: 0.056639552\n",
      "Epoach: 561 | avg loss: 0.10217932 | max loss: 0.14118314 | min loss: 0.07734419\n",
      "Epoach: 562 | avg loss: 0.08922955 | max loss: 0.120729625 | min loss: 0.053381126\n",
      "Epoach: 563 | avg loss: 0.102388084 | max loss: 0.14116806 | min loss: 0.062484957\n",
      "Epoach: 564 | avg loss: 0.08864738 | max loss: 0.12133203 | min loss: 0.06152311\n",
      "Epoach: 565 | avg loss: 0.11417967 | max loss: 0.16512054 | min loss: 0.08258247\n",
      "Epoach: 566 | avg loss: 0.1047992 | max loss: 0.16966486 | min loss: 0.05463752\n",
      "Epoach: 567 | avg loss: 0.090949506 | max loss: 0.12969042 | min loss: 0.031290866\n",
      "Epoach: 568 | avg loss: 0.102204815 | max loss: 0.14501393 | min loss: 0.08174062\n",
      "Epoach: 569 | avg loss: 0.116930515 | max loss: 0.16315664 | min loss: 0.07965861\n",
      "Epoach: 570 | avg loss: 0.09244699 | max loss: 0.14255829 | min loss: 0.043346114\n",
      "Epoach: 571 | avg loss: 0.09794199 | max loss: 0.13193558 | min loss: 0.07849006\n",
      "Epoach: 572 | avg loss: 0.120025225 | max loss: 0.18838042 | min loss: 0.08065048\n",
      "Epoach: 573 | avg loss: 0.10537094 | max loss: 0.13390994 | min loss: 0.063084885\n",
      "Epoach: 574 | avg loss: 0.10034364 | max loss: 0.12113679 | min loss: 0.056921516\n",
      "Epoach: 575 | avg loss: 0.10393052 | max loss: 0.17807758 | min loss: 0.056284692\n",
      "Epoach: 576 | avg loss: 0.09908469 | max loss: 0.132945 | min loss: 0.08454326\n",
      "Epoach: 577 | avg loss: 0.101906635 | max loss: 0.13753535 | min loss: 0.056430303\n",
      "Epoach: 578 | avg loss: 0.12058448 | max loss: 0.15685284 | min loss: 0.08536139\n",
      "Epoach: 579 | avg loss: 0.10299887 | max loss: 0.16473323 | min loss: 0.05385788\n",
      "Epoach: 580 | avg loss: 0.09536976 | max loss: 0.12941469 | min loss: 0.06353727\n",
      "Epoach: 581 | avg loss: 0.08897005 | max loss: 0.1268924 | min loss: 0.059768107\n",
      "Epoach: 582 | avg loss: 0.09810931 | max loss: 0.18096042 | min loss: 0.06979275\n",
      "Epoach: 583 | avg loss: 0.09805824 | max loss: 0.13572907 | min loss: 0.070334956\n",
      "Epoach: 584 | avg loss: 0.11032237 | max loss: 0.15118358 | min loss: 0.077445984\n",
      "Epoach: 585 | avg loss: 0.113882035 | max loss: 0.16818759 | min loss: 0.07486788\n",
      "Epoach: 586 | avg loss: 0.112776585 | max loss: 0.15769792 | min loss: 0.057614036\n",
      "Epoach: 587 | avg loss: 0.09346056 | max loss: 0.14527048 | min loss: 0.052626528\n",
      "Epoach: 588 | avg loss: 0.10798964 | max loss: 0.16327727 | min loss: 0.07184934\n",
      "Epoach: 589 | avg loss: 0.10796327 | max loss: 0.14747737 | min loss: 0.08731636\n",
      "Epoach: 590 | avg loss: 0.120885074 | max loss: 0.15871823 | min loss: 0.086846165\n",
      "Epoach: 591 | avg loss: 0.11017352 | max loss: 0.16530655 | min loss: 0.068247326\n",
      "Epoach: 592 | avg loss: 0.09227946 | max loss: 0.1355169 | min loss: 0.03830665\n",
      "Epoach: 593 | avg loss: 0.08200994 | max loss: 0.10725355 | min loss: 0.058903962\n",
      "Epoach: 594 | avg loss: 0.106587216 | max loss: 0.14820606 | min loss: 0.06949854\n",
      "Epoach: 595 | avg loss: 0.10182014 | max loss: 0.17121921 | min loss: 0.053315807\n",
      "Epoach: 596 | avg loss: 0.09134276 | max loss: 0.13181332 | min loss: 0.040748823\n",
      "Epoach: 597 | avg loss: 0.10206052 | max loss: 0.1311804 | min loss: 0.04889997\n",
      "Epoach: 598 | avg loss: 0.109436035 | max loss: 0.14194572 | min loss: 0.061794292\n",
      "Epoach: 599 | avg loss: 0.10997144 | max loss: 0.13118918 | min loss: 0.0855472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoach: 600 | avg loss: 0.18017107 | max loss: 0.21906209 | min loss: 0.127669722766972 | time: 14.4718029499053961\n",
      "Epoach: 601 | avg loss: 0.2051513 | max loss: 0.2984446 | min loss: 0.16318712.16318712 | time: 14.4547514915466333\n",
      "Epoach: 602 | avg loss: 0.19760701 | max loss: 0.2505924 | min loss: 0.1334693.1334693 | time: 14.53653073310852213\n",
      "Epoach: 603 | avg loss: 0.19505942 | max loss: 0.24717616 | min loss: 0.1134258511342585 | time: 14.437435150146484\n",
      "Epoach: 604 | avg loss: 0.18049185 | max loss: 0.23993 | min loss: 0.1339868813398688 | time: 14.395848274230957\n",
      "Epoach: 605 | avg loss: 0.20090729 | max loss: 0.24189638 | min loss: 0.1143363.1143363 | time: 14.4685728549957284\n",
      "Epoach: 606 | avg loss: 0.20795703 | max loss: 0.2542423 | min loss: 0.1577203915772039 | time: 14.3963932991027836\n",
      "Epoach: 607 | avg loss: 0.1835829 | max loss: 0.21797037 | min loss: 0.13431786.13431786 | time: 14.445376873016357\n",
      "Epoach: 608 | avg loss: 0.2178486 | max loss: 0.30314606 | min loss: 0.15091620.1509162 | time: 14.4913017749786384\n",
      "Epoach: 609 | avg loss: 0.19419783 | max loss: 0.30137616 | min loss: 0.155944275594427 | time: 14.4485144615173348\n",
      "Epoach: 610 | avg loss: 0.20633382 | max loss: 0.259036 | min loss: 0.1621498316214983 | time: 14.43767142295837421\n",
      "Epoach: 611 | avg loss: 0.1961447 | max loss: 0.26221338 | min loss: 0.15885426.15885426 | time: 14.455075979232788\n",
      "Epoach: 612 | avg loss: 0.19713157 | max loss: 0.22014673 | min loss: 0.1716551.1716551 | time: 14.451829195022583\n",
      "Epoach: 613 | avg loss: 0.20549074 | max loss: 0.33537605 | min loss: 0.1329813313298133 | time: 14.435911655426025\n",
      "Epoach: 614 | avg loss: 0.21153754 | max loss: 0.32695958 | min loss: 0.1566761315667613 | time: 14.418686389923096\n",
      "Epoach: 615 | avg loss: 0.2227216 | max loss: 0.2676818 | min loss: 0.18192494.18192494 | time: 14.4291427135467534\n",
      "Epoach: 616 | avg loss: 0.18951856 | max loss: 0.21236232 | min loss: 0.1634949916349499 | time: 14.501180648803711\n",
      "Epoach: 617 | avg loss: 0.1793603 | max loss: 0.23499164 | min loss: 0.15008930.1500893 | time: 14.4979400634765622\n",
      "Epoach: 618 | avg loss: 0.19081815 | max loss: 0.29420987 | min loss: 0.1299158212991582 | time: 14.427405357360846\n",
      "Epoach: 619 | avg loss: 0.20843852 | max loss: 0.2785868 | min loss: 0.14288321428832 | time: 14.515450954437256378\n",
      "Epoach: 620 | avg loss: 0.21086165 | max loss: 0.2589574 | min loss: 0.1547577215475772 | time: 14.4523479938507086\n",
      "Epoach: 621 | avg loss: 0.21091723 | max loss: 0.27121103 | min loss: 0.1654908816549088 | time: 14.553373336791992\n",
      "Epoach: 622 | avg loss: 0.21219312 | max loss: 0.2649533 | min loss: 0.106698260669826 | time: 14.49972629547119169\n",
      "Epoach: 623 | avg loss: 0.21902585 | max loss: 0.32806027 | min loss: 0.1639835416398354 | time: 14.519165277481079\n",
      "Epoach: 624 | avg loss: 0.19626197 | max loss: 0.27190247 | min loss: 0.0947390109473901 | time: 14.468833446502686\n",
      "Epoach: 625 | avg loss: 0.17676845 | max loss: 0.19910355 | min loss: 0.1529789915297899 | time: 14.455148696899414\n",
      "Epoach: 626 | avg loss: 0.19968955 | max loss: 0.22746272 | min loss: 0.1827330118273301 | time: 14.501983165740967\n",
      "Epoach: 627 | avg loss: 0.21290883 | max loss: 0.37340018 | min loss: 0.1438358314383583 | time: 14.396203041076667\n",
      "Epoach: 628 | avg loss: 0.17909153 | max loss: 0.21227404 | min loss: 0.1253078712530787 | time: 14.479704618453987\n",
      "Epoach: 629 | avg loss: 0.19633685 | max loss: 0.24751887 | min loss: 0.1367977413679774 | time: 14.556265830993652\n",
      "Epoach: 630 | avg loss: 0.19247477 | max loss: 0.23120962 | min loss: 0.1672124716721247 | time: 14.435037374496467\n",
      "Epoach: 631 | avg loss: 0.20193751 | max loss: 0.29579157 | min loss: 0.1532914615329146 | time: 14.489209413528442\n",
      "Epoach: 632 | avg loss: 0.20461646 | max loss: 0.27837858 | min loss: 0.1359554713595547 | time: 14.440325021743774\n",
      "Epoach: 633 | avg loss: 0.21292919 | max loss: 0.30839315 | min loss: 0.152100955210095 | time: 14.5361843109130866\n",
      "Epoach: 634 | avg loss: 0.20452051 | max loss: 0.27153862 | min loss: 0.1712920417129204 | time: 14.496951580047607\n",
      "Epoach: 635 | avg loss: 0.2178782 | max loss: 0.25285202 | min loss: 0.16149355.16149355 | time: 14.496932744979858\n",
      "Epoach: 636 | avg loss: 0.23163652 | max loss: 0.27945238 | min loss: 0.1584366715843667 | time: 14.392737150192268\n",
      "Epoach: 637 | avg loss: 0.19225672 | max loss: 0.22349712 | min loss: 0.1580356.1580356 | time: 14.4351618289947515\n",
      "Epoach: 638 | avg loss: 0.21093206 | max loss: 0.30567583 | min loss: 0.1551006915510069 | time: 14.477481842041016\n",
      "Epoach: 639 | avg loss: 0.18813804 | max loss: 0.24548541 | min loss: 0.1476379814763798 | time: 14.496488809585571\n",
      "Epoach: 640 | avg loss: 0.17760888 | max loss: 0.22552449 | min loss: 0.1620716616207166 | time: 14.519109725952148\n",
      "Epoach: 641 | avg loss: 0.20153482 | max loss: 0.2662194 | min loss: 0.1583878115838781 | time: 14.540724277496338\n",
      "Epoach: 642 | avg loss: 0.21225783 | max loss: 0.31259266 | min loss: 0.1505815415058154 | time: 14.437582254409793\n",
      "Epoach: 643 | avg loss: 0.19722918 | max loss: 0.26327258 | min loss: 0.11175530411755304 | time: 14.4081211090087894\n",
      "Epoach: 644 | avg loss: 0.20910487 | max loss: 0.2537732 | min loss: 0.16866271686627 | time: 14.43438339233398486\n",
      "Epoach: 645 | avg loss: 0.19467467 | max loss: 0.284016 | min loss: 0.1564260915642609 | time: 14.50296497344970716\n",
      "Epoach: 646 | avg loss: 0.18886133 | max loss: 0.23602894 | min loss: 0.1473427714734277 | time: 14.449113845825195\n",
      "Epoach: 647 | avg loss: 0.19166969 | max loss: 0.27244934 | min loss: 0.1383426213834262 | time: 14.457039117813111\n",
      "Epoach: 648 | avg loss: 0.20960465 | max loss: 0.26366347 | min loss: 0.12465995624659956 | time: 14.4279048442840584\n",
      "Epoach: 649 | avg loss: 0.21221334 | max loss: 0.28252435 | min loss: 0.1598329515983295 | time: 14.438128709793093\n",
      "Epoach: 650 | avg loss: 0.16542694 | max loss: 0.27750358 | min loss: 0.0983678.0983678 | time: 14.4391388893127443\n",
      "Epoach: 651 | avg loss: 0.19519296 | max loss: 0.2632087 | min loss: 0.1037415610374156 | time: 14.4863212108612067\n",
      "Epoach: 652 | avg loss: 0.20666993 | max loss: 0.26664352 | min loss: 0.1613771.1613771 | time: 14.530704259872437\n",
      "Epoach: 653 | avg loss: 0.20676649 | max loss: 0.23055476 | min loss: 0.157750065775006 | time: 14.5074751377105714\n",
      "Epoach: 654 | avg loss: 0.17431673 | max loss: 0.2663493 | min loss: 0.118637491863749 | time: 14.45848608016967873\n",
      "Epoach: 655 | avg loss: 0.21637218 | max loss: 0.2803134 | min loss: 0.1599417515994175 | time: 14.4253304004669197\n",
      "Epoach: 656 | avg loss: 0.22557771 | max loss: 0.27873668 | min loss: 0.1635051.1635051 | time: 14.3764457702636723\n",
      "Epoach: 657 | avg loss: 0.18823996 | max loss: 0.24523978 | min loss: 0.1296842812968428 | time: 14.509395360946655\n",
      "Epoach: 658 | avg loss: 0.20570241 | max loss: 0.2548626 | min loss: 0.1300268513002685 | time: 14.457706212997437\n",
      "Epoach: 659 | avg loss: 0.1835126 | max loss: 0.23257434 | min loss: 0.13340734.13340734 | time: 14.508399486541748\n",
      "Epoach: 660 | avg loss: 0.18252389 | max loss: 0.25381398 | min loss: 0.1151683311516833 | time: 14.465298175811768\n",
      "Epoach: 661 | avg loss: 0.18099338 | max loss: 0.22397624 | min loss: 0.1459111914591119 | time: 14.424722909927368\n",
      "Epoach: 662 | avg loss: 0.20250031 | max loss: 0.2449101 | min loss: 0.151195185119518 | time: 14.37375235557556288\n",
      "Epoach: 663 | avg loss: 0.18916011 | max loss: 0.24372421 | min loss: 0.1631707.1631707 | time: 14.4190616607666024\n",
      "Epoach: 664 | avg loss: 0.20477162 | max loss: 0.23465821 | min loss: 0.1416763.1416763 | time: 14.4611532688140878\n",
      "Epoach: 665 | avg loss: 0.19382253 | max loss: 0.25980124 | min loss: 0.14203441420344 | time: 14.44022083282470772\n",
      "Epoach: 666 | avg loss: 0.21055464 | max loss: 0.2675096 | min loss: 0.1524289.1524289 | time: 14.4576263427734384\n",
      "Epoach: 667 | avg loss: 0.19768909 | max loss: 0.23697929 | min loss: 0.1442186814421868 | time: 14.485746383666992\n",
      "Epoach: 668 | avg loss: 0.16785802 | max loss: 0.23328082 | min loss: 0.09818224698182246 | time: 14.37071871757507365\n",
      "Epoach: 669 | avg loss: 0.16058186 | max loss: 0.20561962 | min loss: 0.1020734310207343 | time: 14.5062477588653562\n",
      "Epoach: 670 | avg loss: 0.21488404 | max loss: 0.28801927 | min loss: 0.1757116917571169 | time: 14.466028451919556\n",
      "Epoach: 671 | avg loss: 0.19363683 | max loss: 0.26761118 | min loss: 0.1480420414804204 | time: 14.503957509994507\n",
      "Epoach: 672 | avg loss: 0.18957601 | max loss: 0.24497141 | min loss: 0.1359721613597216 | time: 14.393329858779907\n",
      "Epoach: 673 | avg loss: 0.19065195 | max loss: 0.29712203 | min loss: 0.1371829713718297 | time: 14.427503347396856\n",
      "Epoach: 674 | avg loss: 0.1969297 | max loss: 0.27857265 | min loss: 0.14437121.14437121 | time: 14.454439163208008\n",
      "Epoach: 675 | avg loss: 0.19333425 | max loss: 0.25470403 | min loss: 0.1283007.1283007 | time: 14.4042437076568673\n",
      "Epoach: 676 | avg loss: 0.20466815 | max loss: 0.28821787 | min loss: 0.12788451278845 | time: 14.50659179687575111\n",
      "Epoach: 677 | avg loss: 0.22180058 | max loss: 0.32272813 | min loss: 0.1804481918044819 | time: 14.549308300018315\n",
      "Epoach: 678 | avg loss: 0.17917323 | max loss: 0.21535838 | min loss: 0.1600079216000792 | time: 14.431251049041748\n",
      "Epoach: 679 | avg loss: 0.19027102 | max loss: 0.26440558 | min loss: 0.1453632114536321 | time: 14.450539112091064\n",
      "Epoach: 680 | avg loss: 0.22806093 | max loss: 0.30518702 | min loss: 0.17356686356686 | time: 14.43652391433715816\n",
      "Epoach: 681 | avg loss: 0.1866861 | max loss: 0.21043395 | min loss: 0.13934796.13934796 | time: 14.447440147399902\n",
      "Epoach: 682 | avg loss: 0.20034656 | max loss: 0.2649188 | min loss: 0.1652402916524029 | time: 14.443665027618408\n",
      "Epoach: 683 | avg loss: 0.19346598 | max loss: 0.23972002 | min loss: 0.1411386314113863 | time: 14.414475440979004\n",
      "Epoach: 684 | avg loss: 0.2242706 | max loss: 0.292606 | min loss: 0.17586468.17586468 | time: 14.51657390594482494\n",
      "Epoach: 685 | avg loss: 0.16971493 | max loss: 0.19804744 | min loss: 0.1446083814460838 | time: 14.486734628677368\n",
      "Epoach: 686 | avg loss: 0.19325382 | max loss: 0.2595569 | min loss: 0.1445266214452662 | time: 14.5111420154571539\n",
      "Epoach: 687 | avg loss: 0.18330614 | max loss: 0.23769778 | min loss: 0.1280105612801056 | time: 14.497288227081299\n",
      "Epoach: 688 | avg loss: 0.19798085 | max loss: 0.25943527 | min loss: 0.113187745113187745 | time: 14.388103723526001\n",
      "Epoach: 689 | avg loss: 0.17584395 | max loss: 0.24946111 | min loss: 0.1317780313177803 | time: 14.524213790893555\n",
      "Epoach: 690 | avg loss: 0.20191562 | max loss: 0.2531491 | min loss: 0.1486555314865553 | time: 14.5150537490844733\n",
      "Epoach: 691 | avg loss: 0.20509142 | max loss: 0.2920258 | min loss: 0.1446444414464444 | time: 14.398230791091919\n",
      "Epoach: 692 | avg loss: 0.20434001 | max loss: 0.22797681 | min loss: 0.1564752515647525 | time: 14.391390323638916\n",
      "Epoach: 693 | avg loss: 0.20881324 | max loss: 0.27932528 | min loss: 0.1608128116081281 | time: 14.445058107376099\n",
      "Epoach: 694 | avg loss: 0.18149441 | max loss: 0.25250557 | min loss: 0.1025734.1025734 | time: 14.471512079238892\n",
      "Epoach: 695 | avg loss: 0.17179738 | max loss: 0.20977625 | min loss: 0.1142472711424727 | time: 14.419760704040527\n",
      "Epoach: 696 | avg loss: 0.19600898 | max loss: 0.23586631 | min loss: 0.1526973815269738 | time: 14.470276832580566\n",
      "Epoach: 697 | avg loss: 0.20875451 | max loss: 0.24501288 | min loss: 0.1673779.1673779 | time: 14.350717782974243\n",
      "Epoach: 698 | avg loss: 0.21398908 | max loss: 0.28221613 | min loss: 0.1737583117375831 | time: 14.454452514648438\n",
      "Epoach: 699 | avg loss: 0.19381458 | max loss: 0.24541758 | min loss: 0.1671827716718277 | time: 14.402615547180176\n",
      "Epoach: 700 | avg loss: 0.17638528 | max loss: 0.21359743 | min loss: 0.1448667.1448667 | time: 14.4860198497772227\n",
      "Epoach: 701 | avg loss: 0.22125094 | max loss: 0.3029809 | min loss: 0.160721486072148 | time: 14.48242664337158226\n",
      "Epoach: 702 | avg loss: 0.163604 | max loss: 0.24596158 | min loss: 0.120986864.120986864 | time: 14.4135177135467535\n",
      "Epoach: 703 | avg loss: 0.23044807 | max loss: 0.26812723 | min loss: 0.1715248717152487 | time: 14.490523338317871\n",
      "Epoach: 704 | avg loss: 0.17729522 | max loss: 0.22834776 | min loss: 0.1307234.1307234 | time: 14.4927046298980717\n",
      "Epoach: 705 | avg loss: 0.2045634 | max loss: 0.26257542 | min loss: 0.16663027.16663027 | time: 14.366482257843018\n",
      "Epoach: 706 | avg loss: 0.20228034 | max loss: 0.29085174 | min loss: 0.143865544386554 | time: 14.3981065750122076\n",
      "Epoach: 707 | avg loss: 0.18926172 | max loss: 0.22341733 | min loss: 0.1595768615957686 | time: 14.433007001876831\n",
      "Epoach: 708 | avg loss: 0.20425949 | max loss: 0.23731485 | min loss: 0.1646391916463919 | time: 14.566517591476449\n",
      "Epoach: 709 | avg loss: 0.20808057 | max loss: 0.28433186 | min loss: 0.1676013816760138 | time: 14.446823596954346\n",
      "Epoach: 710 | avg loss: 0.19845003 | max loss: 0.25847927 | min loss: 0.1283370412833704 | time: 14.524262905120853\n",
      "Epoach: 711 | avg loss: 0.20166859 | max loss: 0.24184193 | min loss: 0.1714790617147906 | time: 14.464805364608765\n",
      "Epoach: 712 | avg loss: 0.19144478 | max loss: 0.22122486 | min loss: 0.1524712115247121 | time: 14.426972627639776\n",
      "Epoach: 713 | avg loss: 0.19414125 | max loss: 0.24727522 | min loss: 0.131977993197799 | time: 14.4973673820495683\n",
      "Epoach: 714 | avg loss: 0.19525997 | max loss: 0.27921963 | min loss: 0.1085724810857248 | time: 14.443496704101562\n",
      "Epoach: 715 | avg loss: 0.20125926 | max loss: 0.25218773 | min loss: 0.147597034759703 | time: 14.4625086784362817\n",
      "Epoach: 716 | avg loss: 0.20685294 | max loss: 0.2654243 | min loss: 0.148758834875883 | time: 14.39024448394775466\n",
      "Epoach: 717 | avg loss: 0.17852661 | max loss: 0.20786788 | min loss: 0.1263949912639499 | time: 14.443861484527588\n",
      "Epoach: 718 | avg loss: 0.20076782 | max loss: 0.2426442 | min loss: 0.1677642216776422 | time: 14.385553598403936\n",
      "Epoach: 719 | avg loss: 0.17875263 | max loss: 0.22531535 | min loss: 0.1491221514912215 | time: 14.350368261337285\n",
      "Epoach: 720 | avg loss: 0.19362935 | max loss: 0.23027544 | min loss: 0.1547774715477747 | time: 14.351662158966064\n",
      "Epoach: 721 | avg loss: 0.18971045 | max loss: 0.23213379 | min loss: 0.1401881314018813 | time: 14.433502435684204\n",
      "Epoach: 722 | avg loss: 0.19477616 | max loss: 0.23269908 | min loss: 0.1439875714398757 | time: 14.390273094177246\n",
      "Epoach: 723 | avg loss: 0.20353979 | max loss: 0.27332515 | min loss: 0.1612863516128635 | time: 14.337777853012085\n",
      "Epoach: 724 | avg loss: 0.20137425 | max loss: 0.25839296 | min loss: 0.1378354913783549 | time: 14.334950923919678\n",
      "Epoach: 725 | avg loss: 0.17643926 | max loss: 0.22694358 | min loss: 0.1310960413109604 | time: 14.398009777069092\n",
      "Epoach: 726 | avg loss: 0.17752945 | max loss: 0.23944817 | min loss: 0.1243647564364756 | time: 14.34572815895080666\n",
      "Epoach: 727 | avg loss: 0.18975186 | max loss: 0.24756688 | min loss: 0.1352611513526115 | time: 14.441175699234009\n",
      "Epoach: 728 | avg loss: 0.20586377 | max loss: 0.25028884 | min loss: 0.1333601513336015 | time: 14.347856283187866\n",
      "Epoach: 729 | avg loss: 0.16247067 | max loss: 0.21094315 | min loss: 0.1079801110798011 | time: 14.342631816864014\n",
      "Epoach: 730 | avg loss: 0.19968235 | max loss: 0.23476453 | min loss: 0.1704294517042945 | time: 14.267498970031738\n",
      "Epoach: 731 | avg loss: 0.19087052 | max loss: 0.23105735 | min loss: 0.1083641310836413 | time: 14.383307933807373\n",
      "Epoach: 732 | avg loss: 0.20383222 | max loss: 0.24194197 | min loss: 0.1673242616732426 | time: 14.376209259033203\n",
      "Epoach: 733 | avg loss: 0.19545498 | max loss: 0.25362834 | min loss: 0.1486644414866444 | time: 14.336862802505493\n",
      "Epoach: 734 | avg loss: 0.1975683 | max loss: 0.2885574 | min loss: 0.1373484613734846 | time: 14.40122175216674891\n",
      "Epoach: 735 | avg loss: 0.17972386 | max loss: 0.23502094 | min loss: 0.1227762512277625 | time: 14.432024002075195\n",
      "Epoach: 736 | avg loss: 0.17416547 | max loss: 0.20762457 | min loss: 0.149022644902264 | time: 14.3584322929382326\n",
      "Epoach: 737 | avg loss: 0.21471755 | max loss: 0.25695482 | min loss: 0.1929623819296238 | time: 14.328491449356079\n",
      "Epoach: 738 | avg loss: 0.1862383 | max loss: 0.23093249 | min loss: 0.12305418.12305418 | time: 14.272359848022461\n",
      "Epoach: 739 | avg loss: 0.19478236 | max loss: 0.24297822 | min loss: 0.1457914114579141 | time: 14.380554437637329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoach: 740 | avg loss: 0.18280111 | max loss: 0.21016264 | min loss: 0.1386526.1386526 | time: 14.3188416957855222\n",
      "Epoach: 741 | avg loss: 0.18429065 | max loss: 0.22245573 | min loss: 0.153061825306182 | time: 14.3206667900085455\n",
      "Epoach: 742 | avg loss: 0.21404672 | max loss: 0.27120417 | min loss: 0.1029392510293925 | time: 14.286118030548096\n",
      "Epoach: 743 | avg loss: 0.19454741 | max loss: 0.27496868 | min loss: 0.1196507.1196507 | time: 14.301302194595337\n",
      "Epoach: 744 | avg loss: 0.21039118 | max loss: 0.26420984 | min loss: 0.1495484614954846 | time: 14.294727087020874\n",
      "Epoach: 745 | avg loss: 0.21562502 | max loss: 0.27349216 | min loss: 0.12359588623595886 | time: 14.410547733306885\n",
      "Epoach: 746 | avg loss: 0.2038736 | max loss: 0.2622284 | min loss: 0.15408482.15408482 | time: 14.2936491966247568\n",
      "Epoach: 747 | avg loss: 0.19341321 | max loss: 0.22901432 | min loss: 0.158348045834804 | time: 14.2889642715454196\n",
      "Epoach: 748 | avg loss: 0.18538025 | max loss: 0.23477581 | min loss: 0.1566490515664905 | time: 14.275300979614258\n",
      "Epoach: 749 | avg loss: 0.19600785 | max loss: 0.2573698 | min loss: 0.157496145749614 | time: 14.22149467468261727\n",
      "Epoach: 750 | avg loss: 0.20334682 | max loss: 0.24867144 | min loss: 0.136284373628437 | time: 14.4070477485656745\n",
      "Epoach: 751 | avg loss: 0.21009806 | max loss: 0.26665694 | min loss: 0.1297821712978217 | time: 14.332736253738403\n",
      "Epoach: 752 | avg loss: 0.1957475 | max loss: 0.23717755 | min loss: 0.15506636.15506636 | time: 14.339027404785156\n",
      "Epoach: 753 | avg loss: 0.18018782 | max loss: 0.27395365 | min loss: 0.1366008813660088 | time: 14.444791555404663\n",
      "Epoach: 754 | avg loss: 0.18976617 | max loss: 0.22862963 | min loss: 0.1377757313777573 | time: 14.367771387100221\n",
      "Epoach: 755 | avg loss: 0.17077866 | max loss: 0.19834915 | min loss: 0.1313407313134073 | time: 14.377564668655396\n",
      "Epoach: 756 | avg loss: 0.21253547 | max loss: 0.3215854 | min loss: 0.150570025057002 | time: 14.43758463859558116\n",
      "Epoach: 757 | avg loss: 0.20040527 | max loss: 0.25156984 | min loss: 0.1560126215601262 | time: 14.242134571075447\n",
      "Epoach: 758 | avg loss: 0.1903134 | max loss: 0.23047802 | min loss: 0.14900796.14900796 | time: 14.280249595642099\n",
      "Epoach: 759 | avg loss: 0.21094374 | max loss: 0.26056907 | min loss: 0.1561346415613464 | time: 14.337179660797129\n",
      "Epoach: 760 | avg loss: 0.20335476 | max loss: 0.2710623 | min loss: 0.158898445889844 | time: 14.34161448478698763\n",
      "Epoach: 761 | avg loss: 0.19598556 | max loss: 0.23879768 | min loss: 0.1662323216623232 | time: 14.368817806243896\n",
      "Epoach: 762 | avg loss: 0.17493644 | max loss: 0.23973437 | min loss: 0.1255203.1255203 | time: 14.2864551544189456\n",
      "Epoach: 763 | avg loss: 0.19252142 | max loss: 0.25443116 | min loss: 0.1353148413531484 | time: 14.418837785720825\n",
      "Epoach: 764 | avg loss: 0.19982252 | max loss: 0.24921945 | min loss: 0.1452659514526595 | time: 14.368440628051758\n",
      "Epoach: 765 | avg loss: 0.18876381 | max loss: 0.24952236 | min loss: 0.1569401.1569401 | time: 14.3050458431243953\n",
      "Epoach: 766 | avg loss: 0.1851755 | max loss: 0.23969358 | min loss: 0.14795958.14795958 | time: 14.277302503585815\n",
      "Epoach: 767 | avg loss: 0.19791412 | max loss: 0.23381099 | min loss: 0.1588794715887947 | time: 14.341412782669067\n",
      "Epoach: 768 | avg loss: 0.17641976 | max loss: 0.23917988 | min loss: 0.13166721316672 | time: 14.33963680267334117\n",
      "Epoach: 769 | avg loss: 0.16155273 | max loss: 0.23028933 | min loss: 0.0794835.0794835 | time: 14.3796489238739017\n",
      "Epoach: 770 | avg loss: 0.20847118 | max loss: 0.26473665 | min loss: 0.1285630612856306 | time: 14.335984945297241\n",
      "Epoach: 771 | avg loss: 0.19496903 | max loss: 0.25880316 | min loss: 0.136383983638398 | time: 14.3117446899414066\n",
      "Epoach: 772 | avg loss: 0.18517548 | max loss: 0.25325906 | min loss: 0.1342315.1342315 | time: 14.3629124164581389\n",
      "Epoach: 773 | avg loss: 0.19465505 | max loss: 0.25876456 | min loss: 0.1494120814941208 | time: 14.278864622116089\n",
      "Epoach: 774 | avg loss: 0.18931204 | max loss: 0.24450733 | min loss: 0.1359577213595772 | time: 14.268318176269531\n",
      "Epoach: 775 | avg loss: 0.19880661 | max loss: 0.25469762 | min loss: 0.1756077317560773 | time: 14.328479528427124\n",
      "Epoach: 776 | avg loss: 0.18255433 | max loss: 0.24639763 | min loss: 0.1254599412545994 | time: 14.344490051269531\n",
      "Epoach: 777 | avg loss: 0.20676921 | max loss: 0.30609733 | min loss: 0.1419918714199187 | time: 14.318840026855469\n",
      "Epoach: 778 | avg loss: 0.19076884 | max loss: 0.2856764 | min loss: 0.157372395737239 | time: 14.36538147926330678\n",
      "Epoach: 779 | avg loss: 0.19572127 | max loss: 0.2580638 | min loss: 0.1086356810863568 | time: 14.3275902271270754\n",
      "Epoach: 780 | avg loss: 0.17884319 | max loss: 0.22614223 | min loss: 0.11454829614548296 | time: 14.2661180496215821\n",
      "Epoach: 781 | avg loss: 0.1663687 | max loss: 0.20192851 | min loss: 0.12697463.12697463 | time: 14.333304882049565\n",
      "Epoach: 782 | avg loss: 0.197456 | max loss: 0.24956803 | min loss: 0.163713250.16371325 | time: 14.293055534362793\n",
      "Epoach: 783 | avg loss: 0.16986653 | max loss: 0.20224328 | min loss: 0.1263956412639564 | time: 14.317597150802612\n",
      "Epoach: 784 | avg loss: 0.1736606 | max loss: 0.25518513 | min loss: 0.12316462.12316462 | time: 14.318291425704956\n",
      "Epoach: 785 | avg loss: 0.18336931 | max loss: 0.23174739 | min loss: 0.1424737314247373 | time: 14.394404172897339\n",
      "Epoach: 786 | avg loss: 0.19263488 | max loss: 0.27107257 | min loss: 0.1275782612757826 | time: 14.283259153366089\n",
      "Epoach: 787 | avg loss: 0.18511568 | max loss: 0.27497208 | min loss: 0.1356411913564119 | time: 14.278877735137941\n",
      "Epoach: 788 | avg loss: 0.19433922 | max loss: 0.30904323 | min loss: 0.1197635311976353 | time: 14.401183605194092\n",
      "Epoach: 789 | avg loss: 0.17894533 | max loss: 0.28939375 | min loss: 0.1066697710666977 | time: 14.421245574951172\n",
      "Epoach: 790 | avg loss: 0.2017881 | max loss: 0.25431567 | min loss: 0.1425170314251703 | time: 14.3706386089324952\n",
      "Epoach: 791 | avg loss: 0.17476878 | max loss: 0.24642056 | min loss: 0.1107339811073398 | time: 14.380458116531372\n",
      "Epoach: 792 | avg loss: 0.18267837 | max loss: 0.27935174 | min loss: 0.1357622613576226 | time: 14.428255081176758\n",
      "Epoach: 793 | avg loss: 0.20637727 | max loss: 0.2451105 | min loss: 0.1636780216367802 | time: 14.2412915229797363\n",
      "Epoach: 794 | avg loss: 0.17769966 | max loss: 0.24310878 | min loss: 0.1142118711421187 | time: 14.388374805450444\n",
      "Epoach: 795 | avg loss: 0.20593709 | max loss: 0.28134927 | min loss: 0.1459692914596929 | time: 14.383371591567993\n",
      "Epoach: 796 | avg loss: 0.19097741 | max loss: 0.26083064 | min loss: 0.1409791714097917 | time: 14.304097652435303\n",
      "Epoach: 797 | avg loss: 0.1880975 | max loss: 0.2584357 | min loss: 0.14678006.14678006 | time: 14.4430549144744876\n",
      "Epoach: 798 | avg loss: 0.20707473 | max loss: 0.2727801 | min loss: 0.1358761613587616 | time: 14.3266489505767828\n",
      "Epoach: 799 | avg loss: 0.1978989 | max loss: 0.28670138 | min loss: 0.1311826313118263 | time: 14.3200721740722668\n"
     ]
    }
   ],
   "source": [
    "gen = TrainDataGenerator(train_articles_collection, tk, 256)\n",
    "\n",
    "gen_iter = iter(gen)\n",
    "\n",
    "# SE DER ERRO POR CAUSA DA SHAPE A 1º AMOSTRA DAS QUERIES NAO TEM PADDING BURRODE MERDA\n",
    "# MUDA ISSO\n",
    "#loss = []\n",
    "\n",
    "for i,line in enumerate(loss):\n",
    "    \n",
    "    print(\"Epoach:\",i,\"| avg loss:\",np.mean(loss[i]),\"| max loss:\",np.max(loss[i]),\"| min loss:\",np.min(loss[i]))\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "for epoach in range(600,800):\n",
    "    loss_per_epoach = []\n",
    "    for step in range(len(gen)):\n",
    "        X = next(gen_iter)\n",
    "        \n",
    "        start = time.time()\n",
    "        loss_per_epoach.append(deepRank_model.train_on_batch(X))\n",
    "        print(\"Step:\",step,\"| loss:\",loss_per_epoach[-1],\"| current max loss:\",np.max(loss_per_epoach),\"| current min loss:\",np.min(loss_per_epoach),\"| time:\",time.time()-start,end=\"\\r\")\n",
    "    \n",
    "    loss.append(loss_per_epoach)\n",
    "    print(\"\",end=\"\\r\")#clear the line\n",
    "    print(\"Epoach:\",epoach,\"| avg loss:\",np.mean(loss[-1]),\"| max loss:\",np.max(loss[-1]),\"| min loss:\",np.min(loss[-1]))\n",
    "#deepRank_model.fit_generator(gen_iter, steps_per_epoch=len(gen), verbose=1, epochs=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = TrainDataGenerator(train_articles_collection, tk, 20)\n",
    "data_generator = iter(data_generator)\n",
    "X = next(data_generator)\n",
    "\n",
    "#pos, neg = deepRank_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos,neg = deepRank_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test (validation) data generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open /backup/results/fast_method_relevant_results/test_data_deep_models_v2.tar.gz\n"
     ]
    }
   ],
   "source": [
    "path_dl_test = \"/backup/results/fast_method_relevant_results/test_data_deep_models_v2.tar.gz\"\n",
    "\n",
    "\n",
    "\n",
    "tar = tarfile.open(path_dl_test)\n",
    "#open\n",
    "print(\"Open\",path_dl_test)\n",
    "m = tar.getmembers()[0]\n",
    "f = tar.extractfile(m)\n",
    "test_articles_collection = pickle.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TestDataGenerator(object):\n",
    "    def __init__(self, article_collection, tokenizer):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.test_data = article_collection[\"bioasq_data\"] \n",
    "        self.articles = article_collection[\"collection\"]\n",
    "        \n",
    "        self.num_steps = len(self.test_data)\n",
    "        \n",
    "    \n",
    "    def __get_article(self, pmid):\n",
    "        return self.article_map(self.articles[pmid])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        \n",
    "        query = []\n",
    "        query_doc = []\n",
    "        query_doc_position = []\n",
    "        \n",
    "\n",
    "        for query_data in self.test_data:\n",
    "\n",
    "            #tokenized_query = self.tokenizer.texts_to_sequences([query_data[\"query\"]])[0]\n",
    "            tokenized_query = query_data[\"query\"][:MAX_Q_TERM]\n",
    "            #manualy remove the stopwords\n",
    "            #tokenized_query = [ token for token in tokenized_query if token not in biomedical_stop_words_tokens]\n",
    "\n",
    "            #tokenized_query = pad_sequences([tokenized_query], maxlen = MAX_Q_TERM, padding=\"post\")[0]\n",
    "\n",
    "            for doc_pmid in query_data[\"documents\"]:\n",
    "                #positive\n",
    "\n",
    "                tokenized_doc = self.articles[doc_pmid]\n",
    "                doc_snippets, doc_snippets_position = self.__snippet_interaction(tokenized_query, tokenized_doc)\n",
    "\n",
    "                ### add ###\n",
    "\n",
    "                query.append(tokenized_query)\n",
    "\n",
    "                #positive doc\n",
    "                query_doc.append(doc_snippets)\n",
    "                query_doc_position.append(doc_snippets_position)\n",
    "\n",
    "\n",
    "            #missing fill the gap for the missing query_terms\n",
    "\n",
    "            X = [np.array(query), np.array(query_doc), np.array(query_doc_position)]\n",
    "\n",
    "            yield X\n",
    "\n",
    "            #reset\n",
    "            query = []\n",
    "            query_doc = []\n",
    "            query_doc_position = []\n",
    "\n",
    "                \n",
    "    def __snippet_interaction(self, tokenized_query, tokenized_doc, snippet_length=QUERY_CENTRIC_CONTEX):\n",
    "        \n",
    "        snippets = []\n",
    "        snippets_position = [] \n",
    "\n",
    "        half_size = snippet_length//2\n",
    "        \n",
    "        #O(n^2) complexity, probably can do better with better data struct TODO see if is worthit\n",
    "        for query_token in tokenized_query:\n",
    "            \n",
    "            snippets_per_token = []\n",
    "            snippets_per_token_position = []\n",
    "            \n",
    "            if query_token != 0: #jump padded token\n",
    "                \n",
    "                for i,doc_token in enumerate(tokenized_doc):\n",
    "\n",
    "                    if doc_token==query_token:\n",
    "\n",
    "                        lower_index = i-half_size\n",
    "                        lower_index = max(0,lower_index)\n",
    "\n",
    "                        higher_index = i+half_size\n",
    "                        higher_index = min(len(tokenized_doc),higher_index)\n",
    "\n",
    "                        snippets_per_token.append(tokenized_doc[lower_index:higher_index])\n",
    "                        snippets_per_token_position.append(i)\n",
    "\n",
    "            if len(snippets_per_token)==0:\n",
    "                snippets.append(np.zeros((MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), dtype=np.int32))\n",
    "                snippets_position.append(np.zeros((MAX_PASSAGES_PER_QUERY), dtype=np.int32)+SNIPPET_POSITION_PADDING_VALUE)\n",
    "                continue\n",
    "                \n",
    "            max_snippets_len = min(MAX_PASSAGES_PER_QUERY, len(snippets_per_token))\n",
    "            \n",
    "            ### snippets in matrix format\n",
    "            #pad\n",
    "            snippets_per_token = pad_sequences(snippets_per_token, maxlen = QUERY_CENTRIC_CONTEX, padding=\"post\")\n",
    "            #fill the gaps\n",
    "            _temp = np.zeros((MAX_PASSAGES_PER_QUERY,QUERY_CENTRIC_CONTEX), dtype=np.int32)\n",
    "            _temp[:max_snippets_len] = snippets_per_token[:max_snippets_len]\n",
    "            snippets.append(_temp)\n",
    "            \n",
    "            ### snippets_position in matrix format\n",
    "            #pad\n",
    "            snippets_per_token_position = pad_sequences([snippets_per_token_position], maxlen = MAX_PASSAGES_PER_QUERY, padding=\"post\",value=SNIPPET_POSITION_PADDING_VALUE)[0]\n",
    "            snippets_position.append(snippets_per_token_position)\n",
    "            \n",
    "        return snippets, snippets_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict query: 548\r"
     ]
    }
   ],
   "source": [
    "def validate_test_data(data):\n",
    "    data_generator = TestDataGenerator(data, tk)\n",
    "    data_generator = iter(data_generator)\n",
    "\n",
    "    query_results = {}\n",
    "\n",
    "    for i,X in enumerate(data_generator):\n",
    "        print(\"Predict query:\",i,end=\"\\r\")\n",
    "        deep_ranking = document_score_model.predict(X)\n",
    "        deep_ranking = map(lambda x:x[0],deep_ranking.tolist())\n",
    "        bm25_results = data[\"bioasq_data\"][i][\"documents\"]\n",
    "        deep_ranking_pmid = list(zip(bm25_results,deep_ranking))\n",
    "        deep_ranking_pmid.sort(key=lambda x:-x[1])\n",
    "        query_results[data[\"bioasq_data\"][i][\"id\"]] = {\"result\":deep_ranking_pmid,\"goldstandard\":data[\"bioasq_data\"][i][\"positive_pmid\"]}\n",
    "        #print(\"save query results:\",i,end=\"\\r\")\n",
    "        \n",
    "    return query_results\n",
    "\n",
    "test_query_results = validate_test_data(test_articles_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = \"/backup/results/deep_rank\"\n",
    "path_save = os.path.join(path_save, \"deep_rank_v3_test_data.p\")\n",
    "\n",
    "with open(path_save, \"wb\") as f:\n",
    "    pickle.dump(test_query_results,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_to_test_index = 0\n",
    "\n",
    "data_generator = TestDataGenerator(test_articles_collection, tk)\n",
    "data_generator = iter(data_generator)\n",
    "for _ in range(query_to_test_index+1):\n",
    "    X = next(data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_ranking = document_score_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_ranking = map(lambda x:x[0],re_ranking.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_results = test_articles_collection[\"bioasq_data\"][query_to_test_index][\"documents\"]\n",
    "positive_docs = test_articles_collection[\"bioasq_data\"][query_to_test_index][\"positive_pmid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_ranking_pmid = list(zip(bm25_results,re_ranking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_ranking_pmid.sort(key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('24794627', 5.3322014808654785),\n",
       " ('30251567', 5.313037872314453),\n",
       " ('28796422', 5.227417945861816),\n",
       " ('30114722', 5.093368053436279),\n",
       " ('29947303', 5.0901007652282715),\n",
       " ('30697454', 5.0804266929626465),\n",
       " ('30569414', 4.911670207977295),\n",
       " ('28901190', 4.814671039581299),\n",
       " ('24577791', 4.803395748138428),\n",
       " ('26907255', 4.67585563659668)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_ranking_pmid[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13502,    43,   478,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_articles_collection[\"bioasq_data\"][query_to_test_index][\"query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['24554704',\n",
       " '24784583',\n",
       " '24577791',\n",
       " '23197849',\n",
       " '24035588',\n",
       " '21060967',\n",
       " '25479728',\n",
       " '21755313',\n",
       " '24469711',\n",
       " '22512788',\n",
       " '24911883',\n",
       " '24794627',\n",
       " '21464439',\n",
       " '25059784']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '24794627', 5.3322014808654785), (8, '24577791', 4.803395748138428), (11, '24554704', 4.6346845626831055), (12, '23197849', 4.612618446350098), (16, '21060967', 4.446898460388184), (18, '24469711', 4.439567565917969), (19, '21755313', 4.431821823120117), (25, '22512788', 4.27816104888916), (34, '21464439', 4.063064098358154), (35, '25059784', 4.056085586547852), (38, '24784583', 4.003556728363037), (43, '25479728', 3.937878131866455), (50, '24035588', 3.7958528995513916), (343, '24911883', 2.838761806488037)]\n",
      "[(3, '23197849'), (4, '21755313'), (6, '25479728'), (7, '24784583'), (8, '24577791'), (9, '24035588'), (15, '22512788'), (17, '24911883'), (19, '21464439'), (22, '24794627'), (126, '25059784'), (134, '24554704'), (466, '21060967'), (2793, '24469711')]\n"
     ]
    }
   ],
   "source": [
    "positive_docs_ranked = []\n",
    "for i,result in enumerate(re_ranking_pmid):\n",
    "    if result[0] in set(positive_docs):\n",
    "        positive_docs_ranked.append((i,result[0],result[1]))\n",
    "        \n",
    "true_ranked = []\n",
    "for i,pmid in enumerate(bm25_results):\n",
    "    if pmid in set(positive_docs):\n",
    "        true_ranked.append((i,pmid))\n",
    "\n",
    "print(positive_docs_ranked)\n",
    "print(true_ranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with train set, check overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokens = np.array([X[0][0]])\n",
    "snippet_list = np.array([X[1][0]])\n",
    "\n",
    "query_tokens = X[0][:2]\n",
    "snippet_list = X[1][:2]\n",
    "\n",
    "model_input = [query_tokens, snippet_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 15)\n",
      "(2, 15, 3, 15)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(query_tokens.shape)\n",
    "print(snippet_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 15, 3, 15, 15, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = input_model.predict(model_input)\n",
    "np.array(matrix).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[0][4][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(matrix[0][7][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  165, 13502,    26,    61,     8,     1,    43,     2,   478],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  363,     5,  2386,    97,  7598,   774,    32, 13502,    18,\n",
       "          15,     7, 11695,   117,    17,     0], dtype=int32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_list[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05703647,  0.19594026,  0.03365219,  0.15514491,  0.00540348,\n",
       "       -0.02335026, -0.06095085,  0.0226689 , -0.05668721,  0.01571985,\n",
       "       -0.09896637,  0.13836679,  0.02710932,  0.06420047, -0.03692323,\n",
       "        0.03899341,  0.00553868, -0.08639584, -0.05358738, -0.02609682,\n",
       "        0.06495432, -0.00129713, -0.01882407, -0.10850747, -0.02421302,\n",
       "        0.05556208,  0.00291283, -0.04882976,  0.01770345,  0.0035051 ,\n",
       "        0.07192209, -0.00432884, -0.15161929, -0.07024549, -0.04793473,\n",
       "        0.01823143,  0.10337584, -0.04076301,  0.01026187,  0.12004871,\n",
       "        0.03939956, -0.03548966, -0.10689223, -0.16337523,  0.10883316,\n",
       "        0.01135785,  0.03041399,  0.06011688, -0.09919181,  0.01741308,\n",
       "       -0.04328503, -0.00256405, -0.11370766,  0.0522779 ,  0.0702537 ,\n",
       "        0.01021139,  0.06773005,  0.01114117, -0.05878652,  0.0720681 ,\n",
       "        0.05551391,  0.08731035,  0.07339004,  0.0031227 ,  0.10792159,\n",
       "        0.12050318, -0.05851915, -0.08350374, -0.03341928,  0.12355518,\n",
       "        0.11631501,  0.09690028, -0.02127477,  0.05462002, -0.03942551,\n",
       "        0.0049153 , -0.12599072,  0.04783105, -0.00842254,  0.06550868,\n",
       "       -0.06309314, -0.04500391, -0.08562729,  0.0129618 ,  0.05385976,\n",
       "        0.004466  ,  0.0030678 ,  0.02940105, -0.01520511,  0.04611286,\n",
       "        0.03223389,  0.01480899,  0.06665637, -0.00282597, -0.14461097,\n",
       "       -0.16931795,  0.01149627,  0.09099851, -0.06041984, -0.04964583,\n",
       "        0.04263903, -0.02741712,  0.04248934, -0.04117119,  0.1337102 ,\n",
       "       -0.01596204,  0.05762576,  0.00730769,  0.06105822,  0.02038225,\n",
       "        0.06695575,  0.03489211,  0.01739459, -0.00652035, -0.05772134,\n",
       "        0.07827891,  0.03879946,  0.13483463,  0.00304335, -0.07165361,\n",
       "        0.06038161, -0.05664064, -0.10519326, -0.00272405, -0.07754581,\n",
       "        0.06773265,  0.04561247,  0.07964371, -0.07700854,  0.08914506,\n",
       "        0.17518474,  0.01715964,  0.02536894, -0.02523981, -0.10770504,\n",
       "       -0.15260579, -0.00970635,  0.02915863,  0.06111136,  0.00900377,\n",
       "        0.04059591,  0.07735578, -0.04844462,  0.14468352,  0.03376452,\n",
       "       -0.06587124,  0.08225945,  0.02598596,  0.03487838,  0.04218515,\n",
       "        0.07036549, -0.01025326, -0.02166397, -0.06253622,  0.06407471,\n",
       "       -0.123892  , -0.01470529,  0.00293271,  0.05741141, -0.0694873 ,\n",
       "       -0.11187772, -0.07452565,  0.04860114, -0.06068123, -0.03620556,\n",
       "       -0.00046228, -0.05690295, -0.04587905,  0.04772501, -0.04207376,\n",
       "       -0.04783105,  0.05724974, -0.2321706 ,  0.08884832,  0.00071572,\n",
       "        0.05809747, -0.10940351,  0.01475556,  0.01292174, -0.07808141,\n",
       "        0.02383626,  0.06215472,  0.01264787,  0.04288376,  0.06052897,\n",
       "       -0.02552988,  0.05216181,  0.0164598 , -0.03271155, -0.05829453,\n",
       "       -0.09264817,  0.16744114, -0.01112105,  0.07499924,  0.05138046,\n",
       "       -0.00072611, -0.03267605, -0.00081905, -0.02593878, -0.10709276],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dict[13502]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\r"
     ]
    }
   ],
   "source": [
    "data_generator = TrainDataGenerator(train_articles_collection, tk, 256)\n",
    "data_generator = iter(data_generator)\n",
    "for i in range(9):\n",
    "    print(i,end=\"\\r\")\n",
    "    X,Y = next(data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: len(list(filter(lambda y:y!=0,x))),X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 988,  988,  279, 1208,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  1075,      2,    986,      4,      6,    248,    988,   2250,\n",
       "             44,    713,   6300,   6300,      0],\n",
       "        [  3974,      2,    986,      4,      6,    248,    988,   2250,\n",
       "             18,    713,   6300,   6300,      0],\n",
       "        [    17,     17,  46298,  59753,   9143,   2106,    988,   2250,\n",
       "             16,      6,    200,    174,      0]],\n",
       "\n",
       "       [[  1075,      2,    986,      4,      6,    248,    988,   2250,\n",
       "             44,    713,   6300,   6300,      0],\n",
       "        [  3974,      2,    986,      4,      6,    248,    988,   2250,\n",
       "             18,    713,   6300,   6300,      0],\n",
       "        [    17,     17,  46298,  59753,   9143,   2106,    988,   2250,\n",
       "             16,      6,    200,    174,      0]],\n",
       "\n",
       "       [[     2,    248,    986,      4,      1,  10169,    279,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[  1063,   2870,   1361,      7,      6,   6336,   1208,    544,\n",
       "          22931,    137,     27,    587,      0],\n",
       "        [     1,    102,      2,      6,    134,   6336,   1208,    544,\n",
       "          22931, 551603,    137,      8,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]],\n",
       "\n",
       "       [[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0]]], dtype=int32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3][26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos,neg = deepRank_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'bioasq_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-10cc45d13ac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery_to_test_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_articles_collection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_to_test_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-219-bffc59abe895>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, article_collection, tokenizer, article_map)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_collection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bioasq_data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_collection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"collection\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'bioasq_data'"
     ]
    }
   ],
   "source": [
    "query_to_test_index = 1\n",
    "\n",
    "data_generator = TestDataGenerator(train_articles_collection, tk)\n",
    "data_generator = iter(data_generator)\n",
    "for _ in range(query_to_test_index+1):\n",
    "    X = next(data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('26671317', 7.931817054748535),\n",
       " ('20975159', 7.902041435241699),\n",
       " ('20650709', 7.8478264808654785),\n",
       " ('19805301', 7.842199802398682),\n",
       " ('21731768', 7.818233013153076),\n",
       " ('24681619', 7.759010314941406),\n",
       " ('26631348', 7.714381217956543),\n",
       " ('22196114', 7.696432590484619),\n",
       " ('23817568', 7.648404598236084),\n",
       " ('26410599', 7.3919854164123535)]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_ranking = document_score_model.predict(X)\n",
    "\n",
    "bm25_results = train_articles_collection[\"test_data\"][query_to_test_index][\"documents\"]\n",
    "positive_docs = train_articles_collection[\"test_data\"][query_to_test_index][\"positive_pmid\"]\n",
    "\n",
    "re_ranking_pmid = list(zip(bm25_results,re_ranking.tolist()))\n",
    "\n",
    "re_ranking_pmid.sort(key=lambda x:-x[1])\n",
    "\n",
    "re_ranking_pmid[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kind enzyme encoded proto oncogene abl1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['21435002',\n",
       " '20841568',\n",
       " '9500553',\n",
       " '24012954',\n",
       " '18796434',\n",
       " '23842646',\n",
       " '18528425']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tk.sequences_to_texts([train_articles_collection[\"test_data\"][query_to_test_index][\"query\"]]))\n",
    "positive_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, '24012954', 5.5485920906066895), (29, '9500553', 4.8889594078063965), (87, '21435002', 4.592401504516602), (112, '23842646', 4.525805473327637), (155, '18796434', 4.400295257568359), (342, '18528425', 4.139955043792725), (2244, '20841568', 3.1609909534454346)]\n",
      "[(1, '9500553'), (29, '21435002'), (187, '24012954'), (309, '18528425'), (333, '23842646'), (610, '20841568'), (2354, '18796434')]\n"
     ]
    }
   ],
   "source": [
    "positive_docs_ranked = []\n",
    "for i,result in enumerate(re_ranking_pmid):\n",
    "    if result[0] in set(positive_docs):\n",
    "        positive_docs_ranked.append((i,result[0],result[1]))\n",
    "        \n",
    "true_ranked = []\n",
    "for i,pmid in enumerate(bm25_results):\n",
    "    if pmid in set(positive_docs):\n",
    "        true_ranked.append((i,pmid))\n",
    "\n",
    "print(positive_docs_ranked)\n",
    "print(true_ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
