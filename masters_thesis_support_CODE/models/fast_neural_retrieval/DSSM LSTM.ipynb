{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/tiagoalmeida/bioASQ-taskb/\")\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n",
    "import json\n",
    "\n",
    "from pubmed_data import pubmed_helper as ph\n",
    "\n",
    "##add keras to the modules\n",
    "module_path = os.path.abspath(os.path.join('pubmed_data'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Input, LSTM, Dot, Activation, Concatenate\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def dssm_projectiom_model(activation='relu'):\n",
    "    \n",
    "    lstm_1 = LSTM(200, activation=activation, return_sequences=True)\n",
    "        \n",
    "    lstm_2 = LSTM(200, activation=activation, return_sequences=True)\n",
    "\n",
    "    lstm_3 = LSTM(200, activation=activation)\n",
    "\n",
    "    def build_model(inputs, embedding_layer ,model_name):\n",
    "        x = embedding_layer(inputs)\n",
    "        x = lstm_1(x)\n",
    "        x = lstm_2(x)\n",
    "        x = lstm_3(x)\n",
    "\n",
    "        return Model(inputs=[inputs], outputs=[x], name=model_name)\n",
    "    \n",
    "    return build_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"regex_full_tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load regex_full_tokens_tokenizer.p\n",
      "Load regex_full_tokens_word_embedding.p\n"
     ]
    }
   ],
   "source": [
    "#load\n",
    "tk = ph.load_tokenizer(MODE)\n",
    "embedding_dict = ph.load_embeddings(MODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC = len(tk.word_counts)\n",
    "\n",
    "#matrix\n",
    "delta = 0.0001\n",
    "embedding_matrix = np.zeros((VOC, embedding_dict[1].shape[0]))+delta\n",
    "\n",
    "for i in range(1,len(embedding_dict)):\n",
    "    embedding_matrix[i] = embedding_dict[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "q_input (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "doc_input (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "projection_model (Model)        (None, 200)          859321200   q_input[0][0]                    \n",
      "                                                                 doc_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           projection_model[1][0]           \n",
      "                                                                 projection_model[2][0]           \n",
      "==================================================================================================\n",
      "Total params: 859,321,200\n",
      "Trainable params: 962,400\n",
      "Non-trainable params: 858,358,800\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dssm_query_input (InputLayer)   (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dssm_pos_doc_input (InputLayer) (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dssm_neg_doc_input_0 (InputLaye (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dssm_neg_doc_input_1 (InputLaye (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dssm_neg_doc_input_2 (InputLaye (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dssm_neg_doc_input_3 (InputLaye (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "siamese_model (Model)           (None, 1)            859321200   dssm_query_input[0][0]           \n",
      "                                                                 dssm_pos_doc_input[0][0]         \n",
      "                                                                 dssm_query_input[0][0]           \n",
      "                                                                 dssm_neg_doc_input_0[0][0]       \n",
      "                                                                 dssm_query_input[0][0]           \n",
      "                                                                 dssm_neg_doc_input_1[0][0]       \n",
      "                                                                 dssm_query_input[0][0]           \n",
      "                                                                 dssm_neg_doc_input_2[0][0]       \n",
      "                                                                 dssm_query_input[0][0]           \n",
      "                                                                 dssm_neg_doc_input_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 5)            0           siamese_model[1][0]              \n",
      "                                                                 siamese_model[2][0]              \n",
      "                                                                 siamese_model[3][0]              \n",
      "                                                                 siamese_model[4][0]              \n",
      "                                                                 siamese_model[5][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 5)            0           concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 859,321,200\n",
      "Trainable params: 962,400\n",
      "Non-trainable params: 858,358,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_NEG_EXAMPLES = 4\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "embedding_layer = Embedding( input_dim = embedding_matrix.shape[0],\n",
    "           output_dim = embedding_matrix.shape[1], \n",
    "           weights=[embedding_matrix], \n",
    "           trainable=False,\n",
    "           name=\"embedding_layer\")\n",
    "# Follow the paper arch\n",
    "\n",
    "#The INPUT will be the result of the hash trick layer\n",
    "query = Input(shape = (None,), name = \"dssm_query_input\")\n",
    "pos_doc = Input(shape = (None,), name = \"dssm_pos_doc_input\")\n",
    "neg_docs = [Input(shape = (None,), name = (\"dssm_neg_doc_input_\"+str(i))) for i in range(NUM_NEG_EXAMPLES)]\n",
    "\n",
    "\n",
    "\n",
    "#Create a sub model of the network (siamese arch)\n",
    "#2 Inputs query and doc\n",
    "q_input = Input(shape = (None,), name= \"q_input\")\n",
    "doc_input = Input(shape = (None,), name= \"doc_input\")\n",
    "\n",
    "\n",
    "projection_input = Input(shape = (None,), name= \"projection_input\")\n",
    "\n",
    "#same weights\n",
    "dssm_model_builder = dssm_projectiom_model()\n",
    "projection_model = dssm_model_builder(projection_input,embedding_layer,\"projection_model\")\n",
    "\n",
    "\n",
    "\n",
    "#same model for both\n",
    "query_projection_model = projection_model\n",
    "doc_projection_model = projection_model\n",
    "\n",
    "\n",
    "query_projection = query_projection_model(q_input)\n",
    "doc_projection = doc_projection_model(doc_input)\n",
    "#similarity between the query and the docs\n",
    "q_doc_sim = Dot(axes=1,normalize=True)([query_projection,doc_projection])\n",
    "\n",
    "sub_model = Model(inputs=[q_input, doc_input], outputs=[q_doc_sim], name=\"siamese_model\")\n",
    "sub_model.summary()\n",
    "\n",
    "#Making the softmax approximation for 1 pos doc and N neg doc\n",
    "q_doc_pos_output = sub_model([query,pos_doc])\n",
    "q_doc_neg_output = [sub_model([query,neg_doc]) for neg_doc in neg_docs]\n",
    "\n",
    "concat = Concatenate(axis=1)([q_doc_pos_output]+q_doc_neg_output)\n",
    "\n",
    "#missing the smoth factor\n",
    "prob = Activation(\"softmax\")(concat)\n",
    "\n",
    "dssm_model = Model(inputs=[query,pos_doc]+neg_docs,outputs=prob)\n",
    "\n",
    "dssm_model.summary()\n",
    "\n",
    "#try the sgd optimizer\n",
    "dssm_model.compile(optimizer='sgd',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open /backup/pubmed_archive_tokenized/regex_full_tokens_title_abs.tar.gz\n",
      "Creating generator\n",
      "Open the file: regex_full_tokens_file_000_title_abs_pubmed.p\n",
      "Returning: 3690895 articles\n",
      "Force garbage collector 0\n",
      "Open the file: regex_full_tokens_file_001_title_abs_pubmed.p\n",
      "Returning: 3643138 articles\n",
      "Force garbage collector 0\n",
      "Open the file: regex_full_tokens_file_002_title_abs_pubmed.p\n",
      "Returning: 3790281 articles\n",
      "Force garbage collector 0\n",
      "Open the file: regex_full_tokens_file_003_title_abs_pubmed.p\n",
      "Returning: 3838006 articles\n",
      "Force garbage collector 0\n",
      "Open the file: regex_full_tokens_file_004_title_abs_pubmed.p\n",
      "Returning: 3862035 articles\n",
      "Force garbage collector 0\n"
     ]
    }
   ],
   "source": [
    "#load Tokenized articles\n",
    "articles_gen = ph.create_tokenized_pubmed_collection_generator(mode=MODE)\n",
    "\n",
    "tokenized_articles = []\n",
    "for articles in articles_gen():\n",
    "    tokenized_articles.extend(articles)\n",
    "\n",
    "del articles\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load /backup/saved_models/pmid_index_mapping.p\n"
     ]
    }
   ],
   "source": [
    "#load pmid_index_map\n",
    "pmid_index_map = ph.pmid_index_mapping()\n",
    "\n",
    "#Load bioASQ training\n",
    "\n",
    "bioASQ_data_path = \"/backup/BioASQ-training7b/\"\n",
    "bioASQ_data_train = json.load(open(os.path.join(bioASQ_data_path,\"7b_train_split.json\")))\n",
    "bioASQ_data_test = json.load(open(os.path.join(bioASQ_data_path,\"7b_test_split.json\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to the generator expected format\n",
    "convert_format = lambda data:list(map( lambda x:{\"query\":tk.texts_to_sequences([x[\"body\"]])[0],\"documents\":list(map(lambda x:pmid_index_map[x] ,x[\"documents\"]))} ,data))\n",
    "\n",
    "bioASQ_data_train = convert_format(bioASQ_data_train)\n",
    "bioASQ_data_test = convert_format(bioASQ_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training generator\n",
    "#For each query get true positive\n",
    "#For each true positive par with negative samples\n",
    "\n",
    "class DSSM_Train_Generator(object):\n",
    "    def __init__(self, collection, training_data, query_batch_size=16, neg_samples = NUM_NEG_EXAMPLES):\n",
    "        \"\"\"\n",
    "        articles: list of the tokenized articles\n",
    "        training_data: list of dict following {query:\"string\",documents:[index list]} \n",
    "        \"\"\"\n",
    "        \n",
    "        self.collection = collection\n",
    "        self.num_docs = len(collection)\n",
    "        self.training_data = training_data\n",
    "        self.neg_samples = neg_samples\n",
    "        self.query_batch_size = query_batch_size\n",
    "        \n",
    "        training_samples = sum([ len(q[\"documents\"]) for q in training_data])\n",
    "        self.num_steps = training_samples//query_batch_size\n",
    "\n",
    "    def _negative_random_documents(self, exclude):\n",
    "\n",
    "        #bad approach! but the selection exclude is a lot small that the num articles...\n",
    "        neg_random_indexs = np.random.randint(0,self.num_docs,(self.neg_samples,))\n",
    "\n",
    "        while any([i in exclude for i in neg_random_indexs]):\n",
    "            neg_random_indexs = np.random.randint(0,self.num_docs,(self.neg_samples,))\n",
    "\n",
    "        return neg_random_indexs\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \n",
    "        #each list represent a data colum \n",
    "        queries = []\n",
    "        pos_docs = []\n",
    "        neg_docs = [ [] for _ in range(self.neg_samples) ]\n",
    "        \n",
    "        max_len_doc_in_batch = 0\n",
    "        max_article_index = len(self.collection)\n",
    "        \n",
    "        #LOOP the training DATA\n",
    "        while True:\n",
    "\n",
    "            for query_data in self.training_data:\n",
    "\n",
    "                if len(queries)>=self.query_batch_size:\n",
    "                    #apply the padding\n",
    "                    queries = np.array(queries)\n",
    "                    queries = pad_sequences(queries, padding=\"post\", maxlen=max_len_doc_in_batch)\n",
    "                    pos_docs = pad_sequences(pos_docs, padding=\"post\", maxlen=max_len_doc_in_batch)\n",
    "                    neg_docs = [ pad_sequences(neg_doc, padding=\"post\",  maxlen=max_len_doc_in_batch) for neg_doc in neg_docs]\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    X = [queries,pos_docs]+neg_docs\n",
    "\n",
    "                    Y = np.array([[1]+[0]*self.neg_samples]*len(queries))\n",
    "\n",
    "                    yield (X,Y)\n",
    "                    queries = []\n",
    "                    pos_docs = []\n",
    "                    neg_docs = [ [] for _ in range(self.neg_samples) ]\n",
    "                    max_len_doc_in_batch = 0\n",
    "                    \n",
    "                else:\n",
    "                    pos_doc_set = {document_index for document_index in query_data[\"documents\"]}\n",
    "          \n",
    "                    for index_article in pos_doc_set:\n",
    "                        \n",
    "                        batch_doc_len = []\n",
    "                        \n",
    "                        queries.append(query_data[\"query\"])\n",
    "                        \n",
    "                        pos_doc = self.collection[index_article]\n",
    "                        pos_docs.append(pos_doc)\n",
    "                        batch_doc_len.append(len(pos_doc))\n",
    "                        \n",
    "                        neg_random_indexs = self._negative_random_documents(pos_doc_set)\n",
    "                        \n",
    "                        for i in range(self.neg_samples):\n",
    "                            neg_doc = self.collection[neg_random_indexs[i]]\n",
    "                            neg_docs[i].append(neg_doc)\n",
    "                            batch_doc_len.append(len(neg_doc))\n",
    "\n",
    "                        \n",
    "                        #document with higher len in this batch\n",
    "                        max_len_doc_in_batch = max([max_len_doc_in_batch] + batch_doc_len)\n",
    "\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return self.num_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = DSSM_Train_Generator(tokenized_articles, bioASQ_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5140771, 8647685, 8378186, 17319307, 6030864, 16603954, 7654035, 8420211, 7655446, 8651132, 17305694, 8141695}\n",
      "{15793729, 12123889}\n",
      "{14305434}\n",
      "{12906624, 15282756, 17014153, 6122281, 12961678, 473877, 1698102, 10869623, 9052246, 6245657}\n",
      "{16866240, 2519681, 10334051, 16597829, 2290572, 5985871, 11077039, 18183477, 17139414, 8940792, 16874073}\n"
     ]
    }
   ],
   "source": [
    "d = next(iter(gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "   9/1510 [..............................] - ETA: 5:51:38 - loss: 1.6086 - acc: 0.5439"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-ff2181e7bef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdssm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1759\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1760\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1761\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 190\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dssm_model.fit_generator(iter(gen), steps_per_epoch=len(gen), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
