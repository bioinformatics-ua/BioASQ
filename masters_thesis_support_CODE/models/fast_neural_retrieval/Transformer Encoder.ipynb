{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras custom MultiHead-Attention\n",
    "\n",
    "source paper: https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n",
    "\n",
    "Other code repo:\n",
    "\n",
    "https://github.com/Lsdefine/attention-is-all-you-need-keras/blob/master/transformer.py\n",
    "\n",
    "https://github.com/benjamintaiwo/Attention/blob/master/transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, activations\n",
    "from tensorflow.keras.initializers import Zeros, Ones\n",
    "from tensorflow.keras.layers import Layer,Embedding\n",
    "import numpy as np\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelf_Attention(Layer):\n",
    "    def __init__(self, num_heads=6, activation='relu', initializer='glorot_normal', regularizer=None):\n",
    "        super(MultiHeadSelf_Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        ## This implementation assume that Q,K,V have the same input! (ENCODER)\n",
    "\n",
    "        emb_size = input_shape[-1].value\n",
    "\n",
    "        if emb_size % self.num_heads != 0 :\n",
    "            raise TypeError(\"Number of dimension of the embedding must be divided by the number of heads\")\n",
    "        \n",
    "        #feature dimension and dq (query dimension) = dk (key dimension) = dv (value dimension)\n",
    "        dk = emb_size//self.num_heads\n",
    "        self.scaling_factor = np.sqrt(dk)\n",
    "        \n",
    "        #Weight's for the linear projection for all the heads\n",
    "        self.multihead_Wq = []\n",
    "        self.multihead_Wk = []\n",
    "        self.multihead_Wv = []\n",
    "        \n",
    "        for head_i in range(self.num_heads):\n",
    "            #define q,k,v weight's per HEAD\n",
    "            self.multihead_Wq.append(self.add_variable(name = \"Wq_{}\".format(head_i),\n",
    "                                                       shape = [emb_size,dk],\n",
    "                                                       initializer = self.initializer,\n",
    "                                                       regularizer = self.regularizer,)\n",
    "                                    )\n",
    "            self.multihead_Wk.append(self.add_variable(name = \"Wk_{}\".format(head_i),\n",
    "                                                       shape = [emb_size,dk],\n",
    "                                                       initializer = self.initializer,\n",
    "                                                       regularizer = self.regularizer,)\n",
    "                                    )\n",
    "            self.multihead_Wv.append(self.add_variable(name = \"Wv_{}\".format(head_i),\n",
    "                                                       shape = [emb_size,dk],\n",
    "                                                       initializer = self.initializer,\n",
    "                                                       regularizer = self.regularizer,)\n",
    "                                    )\n",
    "        \n",
    "        #Output weight\n",
    "        self.Wo = self.add_variable(name = \"Wo\",\n",
    "                                    shape = [dk*self.num_heads,emb_size], #can be replaced by emb_size,emb_size\n",
    "                                    initializer = self.initializer,\n",
    "                                    regularizer = self.regularizer,)\n",
    "        \n",
    "        \n",
    "        \n",
    "        super(MultiHeadSelf_Attention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        #Q,K,V are the same input!\n",
    "        q = k = v = x\n",
    "        \n",
    "        #head output's\n",
    "        heads = []\n",
    "        \n",
    "        for head_i in range(self.num_heads):\n",
    "            #dim of projection (batch, input_size, dk)\n",
    "            q_projection = K.dot(q, self.multihead_Wq[head_i])\n",
    "            q_projection = self.activation(q_projection)\n",
    "            k_projection = K.dot(k, self.multihead_Wk[head_i])\n",
    "            k_projection = self.activation(k_projection)\n",
    "            v_projection = K.dot(v, self.multihead_Wv[head_i])\n",
    "            v_projection = self.activation(v_projection)\n",
    "            \n",
    "            ### START SCALED DOT PRODUCT\n",
    "            #dim (batch, input_size, input_size)\n",
    "            #transpose 3d matrix\n",
    "            k_transpose = K.permute_dimensions(k_projection, (0,2,1))\n",
    "\n",
    "            q_k_sim = K.batch_dot(q_projection, k_transpose)/self.scaling_factor\n",
    "\n",
    "            #apply the softmax row wise related to the (input,input) matrix\n",
    "            #dim (batch, input_size, input_size)\n",
    "            softmax_weight = K.softmax(q_k_sim, axis =2)\n",
    "            #dim (batch,input_size,dk)\n",
    "            heads.append(K.batch_dot(softmax_weight,v_projection))\n",
    "            ### END SCALED DOT PRODUCT\n",
    "        \n",
    "        #concatenation allong the row dimension related to the matrix (input_size,dk)\n",
    "        \n",
    "        multihead_concat = K.concatenate(heads, axis=2)\n",
    "        return K.dot(multihead_concat,self.Wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, eps=1e-6, **kwargs):\n",
    "        self.eps = eps\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:], initializer=Ones())\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:], initializer=Zeros())\n",
    "        super().build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = K.mean(x, axis=-1, keepdims=True)\n",
    "        std = K.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionFFN(Layer):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 768)         768000    \n",
      "_________________________________________________________________\n",
      "multi_head_self__attention_2 (None, None, 768)         2359296   \n",
      "_________________________________________________________________\n",
      "multi_head_self__attention_3 (None, None, 768)         2359296   \n",
      "_________________________________________________________________\n",
      "multi_head_self__attention_4 (None, None, 768)         2359296   \n",
      "_________________________________________________________________\n",
      "multi_head_self__attention_5 (None, None, 768)         2359296   \n",
      "_________________________________________________________________\n",
      "multi_head_self__attention_6 (None, None, 768)         2359296   \n",
      "_________________________________________________________________\n",
      "multi_head_self__attention_7 (None, None, 768)         2359296   \n",
      "_________________________________________________________________\n",
      "multi_head_self__attention_8 (None, None, 768)         2359296   \n",
      "_________________________________________________________________\n",
      "multi_head_self__attention_9 (None, None, 768)         2359296   \n",
      "_________________________________________________________________\n",
      "multi_head_self__attention_1 (None, None, 768)         2359296   \n",
      "_________________________________________________________________\n",
      "multi_head_self__attention_1 (None, None, 768)         2359296   \n",
      "_________________________________________________________________\n",
      "multi_head_self__attention_1 (None, None, 768)         2359296   \n",
      "_________________________________________________________________\n",
      "multi_head_self__attention_1 (None, None, 768)         2359296   \n",
      "=================================================================\n",
      "Total params: 29,079,552\n",
      "Trainable params: 29,079,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TEST\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 768))\n",
    "model.add(MultiHeadSelf_Attention(num_heads=12))\n",
    "\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
