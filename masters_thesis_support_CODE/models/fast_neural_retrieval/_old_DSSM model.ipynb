{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load hashtrick_full_tokens_tokenizer.p\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import types\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "\n",
    "from scipy import sparse\n",
    "import tarfile\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..','..','pubmed_data'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..',))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from generic_model import ModelAPI\n",
    "import pubmed_helper as ph\n",
    "\n",
    "from keras_new_text import regex_alfanum_tokenizer\n",
    "\n",
    "ht_tokenizer = ph.load_tokenizer(mode=\"hashtrick_full_tokens\")\n",
    "#same memory\n",
    "del ht_tokenizer.index_word\n",
    "del ht_tokenizer.index_docs\n",
    "del ht_tokenizer.word_counts\n",
    "del ht_tokenizer.word_docs\n",
    "\n",
    "TRIGRAM_VOC = len(ht_tokenizer.word_index) + 1 \n",
    "\n",
    "saved_models_path = '/backup/saved_models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSSM model\n",
    " - Missing the hashtrick layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom keras import backend as K\\nfrom keras.layers import Layer\\n\\n#create a letter-trigram-matrix\\nletter_trigram_matrix = None\\n\\nclass TrigramProjection(Layer):\\n\\n    def __init__(self, **kwargs):\\n        super(MyLayer, self).__init__(**kwargs)\\n\\n    def build(self, input_shape):\\n        # Create a trainable weight variable for this layer.\\n        self.letter_trigram_matrix =  K.constant(letter_trigram_matrix, \\n                                                   shape=self.letter_trigram_matrix.shape, \\n                                                   name=\"Letter-trigram-matrix\")\\n\\n        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\\n\\n    def call(self, x):\\n        #self.letter_trigram_matrix has TxV dimension\\n        #x must be Vx1\\n        return K.dot(self.letter_trigram_matrix, x)\\n\\n    def compute_output_shape(self, input_shape):\\n        \\n        return (self.letter_trigram_matrix.shape[0], 1)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a HashTrick Layer\n",
    "#this layer will map bag-of-word vector to a letter-trigram-vector\n",
    "\"\"\"\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "\n",
    "#create a letter-trigram-matrix\n",
    "letter_trigram_matrix = None\n",
    "\n",
    "class TrigramProjection(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.letter_trigram_matrix =  K.constant(letter_trigram_matrix, \n",
    "                                                   shape=self.letter_trigram_matrix.shape, \n",
    "                                                   name=\"Letter-trigram-matrix\")\n",
    "\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        #self.letter_trigram_matrix has TxV dimension\n",
    "        #x must be Vx1\n",
    "        return K.dot(self.letter_trigram_matrix, x)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \n",
    "        return (self.letter_trigram_matrix.shape[0], 1)\n",
    "\"\"\"\n",
    "\n",
    "#The letter_trigram_matrix would have 1.3Mx70k dimension which is a lot\n",
    "#instead of the matrix a on the fly convertion function was created\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dot, Activation, Concatenate\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def bag_of_trigram( texts):\n",
    "\n",
    "    _matrix = np.zeros((len(texts),TRIGRAM_VOC), dtype=np.int8)\n",
    "\n",
    "    for i,text in enumerate(texts):\n",
    "        bag_of_word = regex_alfanum_tokenizer(text)\n",
    "        for j in ht_tokenizer.texts_to_sequences(bag_of_word):\n",
    "            _matrix[i][j] += 1\n",
    "\n",
    "    return _matrix\n",
    "\n",
    "class DSSM(ModelAPI):\n",
    "\n",
    "    def __init__(self, vocabulary_size, num_neg_examples=4, same_q_d_model = True, only_title=False ,show_model_arch=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.TRIGRAM_SIZE = vocabulary_size\n",
    "        self.num_neg_examples = num_neg_examples\n",
    "        self.show_model_arch = show_model_arch\n",
    "        \n",
    "        self.only_title=only_title\n",
    "        if self.only_title:\n",
    "            self.transform_document = lambda x:x[\"title\"]\n",
    "        else:\n",
    "            self.transform_document = lambda x:x[\"title\"]+ \" \" +x[\"abstract\"]\n",
    "        \n",
    "        self.same_q_d_model = same_q_d_model\n",
    "        \n",
    "        #Last line to be exectuted\n",
    "        self.model = self.__build_model()\n",
    "        \n",
    "        \n",
    "\n",
    "    def __build_model(self):\n",
    "        #Build the keras dssm model\n",
    "        K.clear_session()\n",
    "        \n",
    "\n",
    "        # Follow the paper arch\n",
    "        \n",
    "        #The INPUT will be the result of the hash trick layer\n",
    "        query = Input(shape = (self.TRIGRAM_SIZE,), name = \"query_input\")\n",
    "        pos_doc = Input(shape = (self.TRIGRAM_SIZE,), name = \"pos_doc_input\")\n",
    "        neg_docs = [Input(shape = (self.TRIGRAM_SIZE,), name = (\"neg_doc_input_\"+str(i))) for i in range(self.num_neg_examples)]\n",
    "        \n",
    "        #Create a sub model of the network (siamese arch)\n",
    "        #2 Inputs query and doc\n",
    "        q_input = Input(shape = (self.TRIGRAM_SIZE,))\n",
    "        doc_input = Input(shape = (self.TRIGRAM_SIZE,))\n",
    "        \n",
    "        dense_1 = Dense(300, \n",
    "                        activation='tanh',\n",
    "                        kernel_initializer='glorot_uniform', \n",
    "                        bias_initializer='glorot_uniform')\n",
    "        \n",
    "        dense_2 = Dense(300, \n",
    "                        activation='tanh',\n",
    "                        kernel_initializer='glorot_uniform', \n",
    "                        bias_initializer='glorot_uniform')\n",
    "        \n",
    "        dense_3 = Dense(128, \n",
    "                        activation='tanh',\n",
    "                        kernel_initializer='glorot_uniform', \n",
    "                        bias_initializer='glorot_uniform')\n",
    "        \n",
    "        \n",
    "        #connecting input to the neural network\n",
    "        query_sub_model = dense_1(q_input)\n",
    "        query_sub_model = dense_2(query_sub_model)\n",
    "        query_sub_model = dense_3(query_sub_model)\n",
    "        \n",
    "        if self.same_q_d_model is False:\n",
    "            #redefinition of the sub model weight's for the documents\n",
    "            dense_1 = Dense(300, \n",
    "                        activation='tanh',\n",
    "                        kernel_initializer='glorot_uniform', \n",
    "                        bias_initializer='glorot_uniform')\n",
    "\n",
    "            dense_2 = Dense(300, \n",
    "                            activation='tanh',\n",
    "                            kernel_initializer='glorot_uniform', \n",
    "                            bias_initializer='glorot_uniform')\n",
    "\n",
    "            dense_3 = Dense(128, \n",
    "                            activation='tanh',\n",
    "                            kernel_initializer='glorot_uniform', \n",
    "                            bias_initializer='glorot_uniform')\n",
    "\n",
    "        doc_sub_model = dense_1(doc_input)\n",
    "        doc_sub_model = dense_2(doc_sub_model)\n",
    "        doc_sub_model = dense_3(doc_sub_model)\n",
    "        \n",
    "        #model to compute the contiuos representation of the document\n",
    "        self.doc_sub_model = Model(inputs=[doc_input], outputs=[doc_sub_model], name=\"doc_sub_model\")\n",
    "        \n",
    "        if self.same_q_d_model:\n",
    "            self.query_sub_model = self.doc_sub_model\n",
    "        else:\n",
    "            self.query_sub_model = Model(inputs=[q_input], outputs=[query_sub_model], name=\"query_sub_model\")\n",
    "        \n",
    "        #similarity between the query and the docs\n",
    "        q_doc_sim = Dot(axes=1,normalize=True)([query_sub_model,doc_sub_model])\n",
    "        \n",
    "        sub_model = Model(inputs=[q_input,doc_input], outputs=[q_doc_sim])\n",
    "        if self.show_model_arch:\n",
    "            print(\"Sub model arch\")\n",
    "            sub_model.summary()\n",
    "        \n",
    "        #Making the softmax approximation for 1 pos doc and N neg doc\n",
    "        q_doc_pos_output = sub_model([query,pos_doc])\n",
    "        q_doc_neg_output = [sub_model([query,neg_doc]) for neg_doc in neg_docs]\n",
    "        \n",
    "        concat = Concatenate(axis=1)([q_doc_pos_output]+q_doc_neg_output)\n",
    "        \n",
    "        #missing the smoth factor\n",
    "        prob = Activation(\"softmax\")(concat)\n",
    "        \n",
    "        self.dssm_model = Model(inputs=[query,pos_doc]+neg_docs,outputs=prob)\n",
    "        if self.show_model_arch:\n",
    "            self.dssm_model.summary()\n",
    "        \n",
    "        #try the sgd optimizer\n",
    "        self.dssm_model.compile(optimizer='sgd',\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "       \n",
    "    \n",
    "    def create_data_generator(self, data, articles, batch, only_title=False):\n",
    "        \"\"\"\n",
    "        Create a python generator to fed the data in batch to the model\n",
    "        \n",
    "        data: list of queries with the following struct {body:\"question body\",title:\"doc title\",abstract:\"doc abstract\"}\n",
    "        articles: document collection\n",
    "        \"\"\"\n",
    "        \n",
    "        def bag_of_trigram_list_of_list(g_texts):\n",
    "\n",
    "            return [bag_of_trigram(texts) for texts in g_texts]\n",
    "    \n",
    "        def negative_random_index(low, high, selection, exclude):\n",
    "\n",
    "            #bad approach! but the selection exclude is a lot small that the num articles...\n",
    "            neg_random_indexs = np.random.randint(0,len(articles),(selection,))\n",
    "\n",
    "            while any([i in exclude for i in neg_random_indexs]):\n",
    "                neg_random_indexs = np.random.randint(0,len(articles),(selection,))\n",
    "\n",
    "            return neg_random_indexs\n",
    "        \n",
    "        #VER ISTO!\n",
    "        with open(\"/backup/saved_models/pmid_index_mapping.p\",\"rb\") as f:\n",
    "            pmid_document_map = pickle.load(f)\n",
    "\n",
    "        def training_generator(data, batch=batch, neg_examples=self.num_neg_examples, only_title=only_title):\n",
    "\n",
    "            BATCH = batch #approx number of queries to return per batch\n",
    "            \n",
    "            q_pos_neg_doc = []\n",
    "\n",
    "            max_article_index = len(articles)\n",
    "\n",
    "            while True:\n",
    "\n",
    "                for query_data in data:\n",
    "\n",
    "                    if len(q_pos_neg_doc)>=BATCH:\n",
    "                        b_tri = np.array(bag_of_trigram_list_of_list(q_pos_neg_doc))\n",
    "\n",
    "                        q = b_tri[:,0,:]\n",
    "                        pos_doc = b_tri[:,1,:]\n",
    "                        neg_doc = [b_tri[:,i,:] for i in range(2,2+neg_examples)]\n",
    "                        X = [q,pos_doc]+neg_doc\n",
    "\n",
    "                        Y = np.array([[1]+[0]*neg_examples]*len(q_pos_neg_doc))\n",
    "\n",
    "                        yield (X,Y)\n",
    "                        q_pos_neg_doc = []\n",
    "                    else:\n",
    "                        pos_doc_set = {pmid_document_map[document_pmid] for document_pmid in query_data[\"documents\"]}\n",
    "\n",
    "\n",
    "                        for index_article in pos_doc_set:\n",
    "                            row=[]\n",
    "                            row.append(query_data[\"body\"])\n",
    "                            row.append(self.transform_document(articles[index_article]))\n",
    "\n",
    "                            neg_random_indexs = negative_random_index(0, max_article_index, neg_examples, pos_doc_set)\n",
    "                            row.extend([ self.transform_document(articles[neg_index]) for neg_index in neg_random_indexs])\n",
    "                            q_pos_neg_doc.append(row) \n",
    "        \n",
    "        return training_generator(data)\n",
    "        \n",
    "    def _training_process(self, data, **kwargs):\n",
    "        #assume that the data is alredy in the format: (query,pos_doc,[neg_docs])\n",
    "        \n",
    "        if 'training_data' not in kwargs or 'validation_data' not in kwargs:\n",
    "            raise TypeError('training_data and validation_data must be suplied!')\n",
    "        \n",
    "        training_data = kwargs.pop('training_data') \n",
    "        validation_data = kwargs.pop('validation_data') \n",
    "        \n",
    "        if 'batch' in kwargs:\n",
    "            batch = kwargs.pop('batch')\n",
    "        else:\n",
    "            batch = 1024\n",
    "        \n",
    "        if 'epoach' in kwargs:\n",
    "            epoach = kwargs.pop('epoach')\n",
    "        else:\n",
    "            epoach = 20\n",
    "            \n",
    "        if 'only_title' in kwargs:\n",
    "            only_title = kwargs.pop('only_title')\n",
    "        else:\n",
    "            only_title = False\n",
    "        \n",
    "        if 'neg_examples' in kwargs:\n",
    "            self.num_neg_examples = kwargs.pop('neg_examples')\n",
    "        \n",
    "        if kwargs:\n",
    "            raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n",
    "        \n",
    "        \n",
    "        \n",
    "        training_samples = sum([ len(q[\"documents\"]) for q in training_data])\n",
    "        train_steps = training_samples//batch\n",
    "        print(\"Train_steps:\",train_steps)\n",
    "\n",
    "        test_samples = sum([ len(q[\"documents\"]) for q in validation_data])\n",
    "        test_steps = test_samples//batch\n",
    "        print(\"Test_steps:\",test_steps)\n",
    "\n",
    "        #data generators\n",
    "        train_generator = self.create_data_generator(training_data,\n",
    "                                                     data,\n",
    "                                                     batch = batch,\n",
    "                                                     only_title = only_title)\n",
    "        \n",
    "        validation_generator = self.create_data_generator(validation_data,\n",
    "                                                     data,\n",
    "                                                     batch = batch,\n",
    "                                                     only_title = only_title)\n",
    "        \n",
    "        \n",
    "        #callback\n",
    "        save_best_file_name = \"best_checkpoint_dssm_model_\"+(\"title\" if only_title else \"\") + \".h5\"\n",
    "        callback = ModelCheckpoint(os.path.join(self.saved_models_path,save_best_file_name), monitor='val_acc', verbose=0, save_best_only=True)\n",
    "        \n",
    "        print(\"Start dssm training\")\n",
    "        self.dssm_model.fit_generator(train_generator, \n",
    "                                      epochs=20, \n",
    "                                      steps_per_epoch=train_steps,\n",
    "                                      shuffle=True,\n",
    "                                      callbacks = [callback],\n",
    "                                      verbose=1, \n",
    "                                      validation_data=validation_generator,\n",
    "                                      validation_steps=test_steps)\n",
    "\n",
    "    #create generator from collection data\n",
    "    class Bag_of_Trigram_Generator(object):\n",
    "        def __init__(self, dir_name = \"bag_of_trigrams\"):\n",
    "            #TODO: Include batch size option            \n",
    "            path = os.path.join(\"/backup/pubmed_archive_tokenized\",dir_name)\n",
    "            self.files = map(lambda x:os.path.join(path,x), sorted(os.listdir(path)))\n",
    "            \n",
    "        def __iter__(self):\n",
    "            \n",
    "            for file in self.files:\n",
    "                print(\"Open the file:\",file)\n",
    "\n",
    "                _matrix = sparse.load_npz(file).todense()\n",
    "                yield _matrix\n",
    "\n",
    "                del _matrix\n",
    "                #print(\"Force garbage collector\",gc.collect())\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.members)\n",
    "        \n",
    "        \n",
    "    def build_document_representation(self):\n",
    "        \n",
    "        iter_generator = iter(self.Bag_of_Trigram_Generator())\n",
    "        \n",
    "        self.collection_representation = []\n",
    "        \n",
    "        for data in iter_generator:\n",
    "            self.collection_representation.append(self.doc_sub_model.predict(data, batch_size = 2048, verbose=1))\n",
    "            del data\n",
    "            print(\"Force garbage collector\",gc.collect())\n",
    "            \n",
    "        self.collection_representation = np.vstack(self.collection_representation)\n",
    "        \"\"\"\n",
    "\n",
    "        gen = self.Bag_of_Trigram_Generator()\n",
    "        \n",
    "        def clean_up(batch,logs={}):\n",
    "            del batch\n",
    "            print(\"Force garbage collector\",gc.collect())\n",
    "            \n",
    "        cleanup_callback = LambdaCallback(on_batch_end=clean_up)\n",
    "        \n",
    "        self.collection_representation = self.doc_sub_model.predict_generator(\n",
    "                                                        iter(gen), \n",
    "                                                        steps=len(gen),\n",
    "                                                        verbose=1,\n",
    "                                                        callbacks=[cleanup_callback])\n",
    "        \"\"\"\n",
    "        \n",
    "    def _predict_process(self, queries):\n",
    "        pass\n",
    "        \n",
    "    @staticmethod\n",
    "    def load(f_name, path = '/backup/saved_models/'):\n",
    "        file_name = os.path.join(path, f_name)\n",
    "        \n",
    "        print(\"Load metadata\")\n",
    "        with open(file_name+\"_dssm_metadata.p\",\"rb\") as file:\n",
    "            metadata = pickle.load(file) \n",
    "        \n",
    "        dssm = DSSM(metadata[\"TRIGRAM_VOC\"])\n",
    "        dssm.same_q_d_model = metadata[\"same_q_d_model\"]\n",
    "        print(\"Load document representation\")\n",
    "        dssm.collection_representation = np.load(file_name+\"_dssm_doc_emb.npy\")\n",
    "        print(\"Load model\")\n",
    "        dssm.dssm_model = load_model(file_name+\"_dssm_model.h5\")\n",
    "        if dssm.same_q_d_model:\n",
    "            dssm.doc_sub_model = dssm.dssm_model[\"doc_sub_model\"]\n",
    "            dssm.query_sub_model = dssm.doc_sub_model\n",
    "        else:\n",
    "            dssm.doc_sub_model = dssm.dssm_model[\"doc_sub_model\"]\n",
    "            dssm.query_sub_model = dssm.dssm_model[\"query_sub_model\"]\n",
    "        \n",
    "    def save(self, **kwargs):\n",
    "        \n",
    "        if \"f_name\" in kwargs:\n",
    "            f_name = kwargs.pop(\"f_name\")\n",
    "        else:\n",
    "            raise TypeError(\"f_name must be provided\")\n",
    "        \n",
    "        if kwargs:\n",
    "            raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n",
    "            \n",
    "        file_name = os.path.join(self.saved_models_path, f_name)\n",
    "        \n",
    "        #save the document representation\n",
    "        np.save(file_name+\"_dssm_doc_emb\",self.collection_representation)\n",
    "        \n",
    "        #save the neural model\n",
    "        self.dssm_model.save(file_name+\"_dssm_model.h5\")\n",
    "        \n",
    "        #save some metadata\n",
    "        with open(file_name+\"_dssm_metadata.p\",\"wb\") as file:\n",
    "            pickle.dump({\"TRIGRAM_VOC\":self.TRIGRAM_SIZE,\"same_q_d_model\":self.same_q_d_model},file)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dssm = DSSM(TRIGRAM_VOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_000_title_abs_pubmed.p.npz\n",
      "231363/231363 [==============================] - 71s 307us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_001_title_abs_pubmed.p.npz\n",
      "231363/231363 [==============================] - 71s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_002_title_abs_pubmed.p.npz\n",
      "231363/231363 [==============================] - 70s 304us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_003_title_abs_pubmed.p.npz\n",
      "231363/231363 [==============================] - 70s 304us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_004_title_abs_pubmed.p.npz\n",
      "231363/231363 [==============================] - 70s 304us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_005_title_abs_pubmed.p.npz\n",
      "231363/231363 [==============================] - 71s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_006_title_abs_pubmed.p.npz\n",
      "231363/231363 [==============================] - 70s 304us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_007_title_abs_pubmed.p.npz\n",
      "231363/231363 [==============================] - 71s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_008_title_abs_pubmed.p.npz\n",
      "231363/231363 [==============================] - 70s 304us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_009_title_abs_pubmed.p.npz\n",
      "231363/231363 [==============================] - 71s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_010_title_abs_pubmed.p.npz\n",
      "231363/231363 [==============================] - 70s 304us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_011_title_abs_pubmed.p.npz\n",
      "231370/231370 [==============================] - 71s 307us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_012_title_abs_pubmed.p.npz\n",
      "228633/228633 [==============================] - 70s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_013_title_abs_pubmed.p.npz\n",
      "228633/228633 [==============================] - 70s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_014_title_abs_pubmed.p.npz\n",
      "228633/228633 [==============================] - 70s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_015_title_abs_pubmed.p.npz\n",
      "228633/228633 [==============================] - 70s 307us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_016_title_abs_pubmed.p.npz\n",
      "228633/228633 [==============================] - 70s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_017_title_abs_pubmed.p.npz\n",
      "228633/228633 [==============================] - 70s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_018_title_abs_pubmed.p.npz\n",
      "228633/228633 [==============================] - 70s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_019_title_abs_pubmed.p.npz\n",
      "228633/228633 [==============================] - 70s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_020_title_abs_pubmed.p.npz\n",
      "228633/228633 [==============================] - 70s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_021_title_abs_pubmed.p.npz\n",
      "228633/228633 [==============================] - 70s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_022_title_abs_pubmed.p.npz\n",
      "228633/228633 [==============================] - 70s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_023_title_abs_pubmed.p.npz\n",
      "228643/228643 [==============================] - 69s 303us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_024_title_abs_pubmed.p.npz\n",
      "226758/226758 [==============================] - 69s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_025_title_abs_pubmed.p.npz\n",
      "226758/226758 [==============================] - 69s 304us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_026_title_abs_pubmed.p.npz\n",
      " 94208/226758 [===========>..................] - ETA: 40s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240270/240270 [==============================] - 73s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_042_title_abs_pubmed.p.npz\n",
      "240270/240270 [==============================] - 73s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_043_title_abs_pubmed.p.npz\n",
      "240270/240270 [==============================] - 73s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_044_title_abs_pubmed.p.npz\n",
      "240270/240270 [==============================] - 74s 308us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_045_title_abs_pubmed.p.npz\n",
      "240270/240270 [==============================] - 74s 307us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_046_title_abs_pubmed.p.npz\n",
      "240270/240270 [==============================] - 74s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_047_title_abs_pubmed.p.npz\n",
      "240272/240272 [==============================] - 73s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_048_title_abs_pubmed.p.npz\n",
      "239375/239375 [==============================] - 73s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_049_title_abs_pubmed.p.npz\n",
      "239375/239375 [==============================] - 73s 304us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_050_title_abs_pubmed.p.npz\n",
      "239375/239375 [==============================] - 73s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_051_title_abs_pubmed.p.npz\n",
      "239375/239375 [==============================] - 74s 307us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_052_title_abs_pubmed.p.npz\n",
      "239375/239375 [==============================] - 73s 303us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_053_title_abs_pubmed.p.npz\n",
      "239375/239375 [==============================] - 74s 308us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_054_title_abs_pubmed.p.npz\n",
      "239375/239375 [==============================] - 73s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_055_title_abs_pubmed.p.npz\n",
      "239375/239375 [==============================] - 73s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_056_title_abs_pubmed.p.npz\n",
      "239375/239375 [==============================] - 73s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_057_title_abs_pubmed.p.npz\n",
      "239375/239375 [==============================] - 73s 304us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_058_title_abs_pubmed.p.npz\n",
      "239375/239375 [==============================] - 73s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_059_title_abs_pubmed.p.npz\n",
      "239377/239377 [==============================] - 73s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_060_title_abs_pubmed.p.npz\n",
      "241376/241376 [==============================] - 74s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_061_title_abs_pubmed.p.npz\n",
      "241376/241376 [==============================] - 74s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_062_title_abs_pubmed.p.npz\n",
      "241376/241376 [==============================] - 74s 305us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_063_title_abs_pubmed.p.npz\n",
      "241376/241376 [==============================] - 74s 306us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_064_title_abs_pubmed.p.npz\n",
      "241376/241376 [==============================] - 74s 307us/step\n",
      "Force garbage collector 15\n",
      "Open the file: /backup/pubmed_archive_tokenized/bag_of_trigrams/bag_of_trigrams_file_065_title_abs_pubmed.p.npz\n",
      "215040/241376 [=========================>....] - ETA: 8s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#articles_generator = ph.create_pubmed_collection_generator()()\n",
    "\n",
    "#Build document representation\n",
    "#articles = next(articles_generator)\n",
    "gc.collect()\n",
    "dssm.build_document_representation()\n",
    "\n",
    "#gen = DSSM.Bag_of_Trigram_Generator()\n",
    "#iter_gen = iter(gen)\n",
    "#data = next(iter_gen)\n",
    "\n",
    "#print(\"DSSM predict\")\n",
    "#r_1 = dssm.doc_sub_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "gc.collect()\n",
    "data = next(iter_gen)\n",
    "\n",
    "print(\"DSSM predict\")\n",
    "r_2 = dssm.doc_sub_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch size of 1000000\n"
     ]
    }
   ],
   "source": [
    "# SAVE BAG OF TRIGRAM\n",
    "\n",
    "gen = DSSM.Collection_Generator(articles, lambda x:x[\"title\"]+\" \"+x[\"abstract\"],batch_size=int(1e6))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"/backup/saved_models/pmid_index_mapping.p\",\"rb\") as f:\n",
    "#    pmid_document_map = pickle.load(f)\n",
    "    \n",
    "#Load training and test data\n",
    "bioASQ_data_path = \"/backup/BioASQ-training7b/\"\n",
    "bioASQ_data_train = json.load(open(os.path.join(bioASQ_data_path,\"7b_train_split.json\")))\n",
    "bioASQ_data_test = json.load(open(os.path.join(bioASQ_data_path,\"7b_test_split.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open /backup/pubmed_archive_json/pubmed_ready.tar.gz\n",
      "Returning: 2721103 articles\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_08241072_to_11124313\n",
      "Returning: 2883242 articles\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_11124314_to_13996815\n",
      "Returning: 2872502 articles\n",
      "Force garbage collector 0\n",
      "Open the file: pubmed_ready_13996816_to_18824354\n",
      "Returning: 4827539 articles\n",
      "Force garbage collector 0\n"
     ]
    }
   ],
   "source": [
    "#load the articles to the memory\n",
    "articles_generator = ph.create_pubmed_collection_generator()\n",
    "articles = []\n",
    "for docs in articles_generator():\n",
    "    articles.extend(docs)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_steps: 47\n",
      "Test_steps: 12\n",
      "Start dssm training\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 581s 12s/step - loss: 1.3901 - acc: 0.5352 - val_loss: 1.2782 - val_acc: 0.5828\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 519s 11s/step - loss: 1.2195 - acc: 0.6257 - val_loss: 1.2036 - val_acc: 0.6292\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 524s 11s/step - loss: 1.1489 - acc: 0.6670 - val_loss: 1.1852 - val_acc: 0.6622\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 514s 11s/step - loss: 1.1196 - acc: 0.6884 - val_loss: 1.1667 - val_acc: 0.6558\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 515s 11s/step - loss: 1.0815 - acc: 0.7133 - val_loss: 1.1113 - val_acc: 0.6814\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 514s 11s/step - loss: 1.0722 - acc: 0.7305 - val_loss: 1.1205 - val_acc: 0.6999\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 519s 11s/step - loss: 1.0469 - acc: 0.7442 - val_loss: 1.1040 - val_acc: 0.6983\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 524s 11s/step - loss: 1.0371 - acc: 0.7532 - val_loss: 1.0851 - val_acc: 0.7081\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 510s 11s/step - loss: 1.0223 - acc: 0.7640 - val_loss: 1.0827 - val_acc: 0.7131\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 522s 11s/step - loss: 1.0150 - acc: 0.7712 - val_loss: 1.0734 - val_acc: 0.7077\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 519s 11s/step - loss: 1.0093 - acc: 0.7830 - val_loss: 1.0955 - val_acc: 0.7053\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 514s 11s/step - loss: 0.9990 - acc: 0.7830 - val_loss: 1.0883 - val_acc: 0.7202\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 522s 11s/step - loss: 0.9908 - acc: 0.7913 - val_loss: 1.0589 - val_acc: 0.7237\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 516s 11s/step - loss: 0.9877 - acc: 0.7948 - val_loss: 1.0630 - val_acc: 0.7307\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 518s 11s/step - loss: 0.9835 - acc: 0.8005 - val_loss: 1.0544 - val_acc: 0.7350\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 525s 11s/step - loss: 0.9752 - acc: 0.8056 - val_loss: 1.0782 - val_acc: 0.7275\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 522s 11s/step - loss: 0.9743 - acc: 0.8126 - val_loss: 1.0763 - val_acc: 0.7340\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 517s 11s/step - loss: 0.9629 - acc: 0.8097 - val_loss: 1.0516 - val_acc: 0.7403\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 521s 11s/step - loss: 0.9618 - acc: 0.8171 - val_loss: 1.0642 - val_acc: 0.7442\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 511s 11s/step - loss: 0.9533 - acc: 0.8203 - val_loss: 1.0407 - val_acc: 0.7452\n"
     ]
    }
   ],
   "source": [
    "## Train dssm\n",
    "\n",
    "dssm.train(articles,\n",
    "           epoach = 20,\n",
    "           batch = 512,\n",
    "           training_data=bioASQ_data_train,\n",
    "           validation_data=bioASQ_data_test\n",
    "           \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dssm.dssm_model.save(\"model-20-epoach-regex-abs-title-final.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def negative_random_index(low, high, selection, exclude):\n",
    "    \n",
    "    #bad approach! but the selection exclude is a lot small that the num articles...\n",
    "    neg_random_indexs = np.random.randint(0,len(articles),(selection,))\n",
    "    \n",
    "    while any([i in exclude for i in neg_random_indexs]):\n",
    "        neg_random_indexs = np.random.randint(0,len(articles),(selection,))\n",
    "    \n",
    "    return neg_random_indexs\n",
    "    \n",
    "\n",
    "                \n",
    "def training_generator(type_of_data,batch=128,neg_examples=4,only_title=False):\n",
    "    \n",
    "    BATCH = batch #approx number of queries to return per batch\n",
    "    if only_title:\n",
    "        transform_article = lambda x:x[\"title\"]\n",
    "    else:\n",
    "        transform_article = lambda x:x[\"title\"]+ \" \" +x[\"abstract\"]\n",
    "    q_pos_neg_doc = []\n",
    "    \n",
    "    max_article_index = len(articles)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        for query_data in type_of_data:\n",
    "\n",
    "            if len(q_pos_neg_doc)>=BATCH:\n",
    "                b_tri = np.array(bag_of_trigram_list_of_list(q_pos_neg_doc))\n",
    "\n",
    "                q = b_tri[:,0,:]\n",
    "                pos_doc = b_tri[:,1,:]\n",
    "                neg_doc = [b_tri[:,i,:] for i in range(2,2+neg_examples)]\n",
    "                X = [q,pos_doc]+neg_doc\n",
    "\n",
    "                Y = np.array([[1]+[0]*neg_examples]*len(q_pos_neg_doc))\n",
    "\n",
    "                yield (X,Y)\n",
    "                q_pos_neg_doc = []\n",
    "            else:\n",
    "                pos_doc_set = {pmid_document_map[document_pmid] for document_pmid in query_data[\"documents\"]}\n",
    "                \n",
    "                \n",
    "                for index_article in pos_doc_set:\n",
    "                    row=[]\n",
    "                    row.append(query_data[\"body\"])\n",
    "                    row.append(transform_article(articles[index_article]))\n",
    "                    \n",
    "                    neg_random_indexs = negative_random_index(0, max_article_index, neg_examples, pos_doc_set)\n",
    "                    row.extend([ transform_article(articles[neg_index]) for neg_index in neg_random_indexs])\n",
    "                    q_pos_neg_doc.append(row) \n",
    "                 \n",
    "                    \n",
    "                    \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'query_input:0' shape=(?, 77301) dtype=float32>, <tf.Tensor 'pos_doc_input:0' shape=(?, 77301) dtype=float32>, <tf.Tensor 'neg_doc_input_0:0' shape=(?, 77301) dtype=float32>, <tf.Tensor 'neg_doc_input_1:0' shape=(?, 77301) dtype=float32>, <tf.Tensor 'neg_doc_input_2:0' shape=(?, 77301) dtype=float32>, <tf.Tensor 'neg_doc_input_3:0' shape=(?, 77301) dtype=float32>]\n",
      "[<tf.Tensor 'activation/Softmax:0' shape=(?, 5) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print(dssm.dssm_model.inputs)\n",
    "print(dssm.dssm_model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 188\n",
      "test_steps: 48\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 184s 978ms/step - loss: 1.4359 - acc: 0.5104 - val_loss: 1.2423 - val_acc: 0.6298\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 183s 972ms/step - loss: 1.0949 - acc: 0.7564 - val_loss: 1.1284 - val_acc: 0.7160\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 178s 946ms/step - loss: 1.0563 - acc: 0.7861 - val_loss: 1.1247 - val_acc: 0.7107\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 178s 947ms/step - loss: 1.0317 - acc: 0.8080 - val_loss: 1.1089 - val_acc: 0.7333\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 178s 947ms/step - loss: 1.0174 - acc: 0.8187 - val_loss: 1.1055 - val_acc: 0.7365\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 178s 945ms/step - loss: 0.9988 - acc: 0.8292 - val_loss: 1.0928 - val_acc: 0.7426\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 177s 944ms/step - loss: 0.9930 - acc: 0.8354 - val_loss: 1.0792 - val_acc: 0.7489\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 178s 945ms/step - loss: 0.9797 - acc: 0.8427 - val_loss: 1.0909 - val_acc: 0.7279\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 177s 942ms/step - loss: 0.9721 - acc: 0.8451 - val_loss: 1.0864 - val_acc: 0.7374\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 177s 944ms/step - loss: 0.9637 - acc: 0.8498 - val_loss: 1.0897 - val_acc: 0.7304\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 178s 945ms/step - loss: 0.9587 - acc: 0.8561 - val_loss: 1.1020 - val_acc: 0.7742\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 178s 944ms/step - loss: 0.9539 - acc: 0.8536 - val_loss: 1.0949 - val_acc: 0.7523\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 177s 942ms/step - loss: 0.9510 - acc: 0.8596 - val_loss: 1.0836 - val_acc: 0.7270\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 178s 945ms/step - loss: 0.9436 - acc: 0.8622 - val_loss: 1.0843 - val_acc: 0.7545\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 178s 947ms/step - loss: 0.9394 - acc: 0.8620 - val_loss: 1.0822 - val_acc: 0.7468\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 177s 943ms/step - loss: 0.9356 - acc: 0.8663 - val_loss: 1.0705 - val_acc: 0.7539\n",
      "Epoch 18/20\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.9325 - acc: 0.8690"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "BATCH = 128\n",
    "\n",
    "training_samples = sum([ len(q[\"documents\"]) for q in bioASQ_data_train])\n",
    "train_steps = training_samples//BATCH\n",
    "print(\"train_steps:\",train_steps)\n",
    "\n",
    "test_samples = sum([ len(q[\"documents\"]) for q in bioASQ_data_test])\n",
    "test_steps = test_samples//BATCH\n",
    "print(\"test_steps:\",test_steps)\n",
    "\n",
    "#callback\n",
    "callback = ModelCheckpoint('model-title-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5', monitor='val_acc', verbose=0, save_best_only=True)\n",
    "\n",
    "dssm.dssm_model.fit_generator(training_generator(bioASQ_data_train,batch=BATCH,only_title=True), \n",
    "                              epochs=20, \n",
    "                              steps_per_epoch=train_steps,\n",
    "                              shuffle=True,\n",
    "                              callbacks = [callback],\n",
    "                              verbose=1, \n",
    "                              validation_data=training_generator(bioASQ_data_test,batch=BATCH,only_title=True),\n",
    "                              validation_steps=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dssm.dssm_model.save(\"model-title-final.h5\")\n",
    "\n",
    "#verify the generator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24356"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#num of docs per query\n",
    "\n",
    "sum([len(docs[\"documents\"]) for docs in bioASQ_data_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9655078"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open the pubmed tar.gz\n",
      "Creating generator\n",
      "Open the file: 0\n",
      "Returning: 2776363 articles\n",
      "Force garbage collector 0\n",
      "Open the file: 1\n",
      "Returning: 2743606 articles\n",
      "Force garbage collector 0\n",
      "Open the file: 2\n",
      "Returning: 2721103 articles\n",
      "Force garbage collector 0\n",
      "Open the file: 3\n",
      "Returning: 2883242 articles\n",
      "Force garbage collector 0\n",
      "Open the file: 4\n",
      "Returning: 2872502 articles\n",
      "Force garbage collector 0\n",
      "Open the file: 5\n",
      "Returning: 4827539 articles\n",
      "Force garbage collector 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE THAT I WILL NOT USE PROBABLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "\n",
    "    \n",
    "def processJob(process_ID, queries):\n",
    "    print(\"Start:\",process_ID)\n",
    "    q_pos_neg_doc = []\n",
    "    max_article_index = len(articles)\n",
    "    neg_examples=4\n",
    "    \n",
    "    for query_data in queries:\n",
    "\n",
    "        pos_doc_set = {pmid_document_map[document_pmid] for document_pmid in query_data[\"documents\"]}\n",
    "\n",
    "        for index_article in pos_doc_set:\n",
    "            row=[]\n",
    "            row.append(query_data[\"body\"])\n",
    "            row.append(transform_article(articles[index_article]))\n",
    "\n",
    "            neg_random_indexs = negative_random_index(0, max_article_index, neg_examples, pos_doc_set)\n",
    "            row.extend([ transform_article(articles[neg_index]) for neg_index in neg_random_indexs])\n",
    "            q_pos_neg_doc.append(row) \n",
    "        \n",
    "        \n",
    "    print(\"build bag of trigram:\",process_ID,\"len data\",len(q_pos_neg_doc))\n",
    "    b_tri = np.array(bag_of_trigram_list_of_list(q_pos_neg_doc))\n",
    "\n",
    "    q = b_tri[:,0,:]\n",
    "    pos_doc = b_tri[:,1,:]\n",
    "    neg_doc = [b_tri[:,i,:] for i in range(2,2+neg_examples)]\n",
    "    X = [q,pos_doc]+neg_doc\n",
    "    print(len(X))\n",
    "    Y = np.array([[1]+[0]*neg_examples]*len(q_pos_neg_doc))\n",
    "    print(len(Y),Y.shape)\n",
    "    \n",
    "    \n",
    "    file_name = \"bioASQ_bag_of_trigram_{0:03}.p\".format(process_ID)\n",
    "    print(\"save:\",file_name)\n",
    "    with open(\"/backup/BioASQ-training7b/\"+file_name,\"wb\") as f:\n",
    "        pickle.dump(X,f)\n",
    "\n",
    "\n",
    "num_queries_per_itter = 200\n",
    "\n",
    "num_divissions = len(bioASQ_data_train)//num_queries_per_itter\n",
    "\n",
    "batch = range(0,len(bioASQ_data_train),num_queries_per_itter)\n",
    "\n",
    "for i,j in enumerate(batch):\n",
    "    data = bioASQ_data_train[j:j+num_queries_per_itter]\n",
    "\n",
    "    n_process = 20\n",
    "    n_queries = len(data)\n",
    "    n_queries_per_process = n_queries//n_process\n",
    "\n",
    "    batch = list(range(0,n_queries,n_queries_per_process))\n",
    "\n",
    "    if len(batch)==n_process:\n",
    "        batch.append(n_queries)\n",
    "    else:\n",
    "        batch[n_process] = n_queries\n",
    "\n",
    "    process = []\n",
    "\n",
    "    for k in range(n_process):\n",
    "        process.append(Process(target=processJob, args=(i*n_process + k, data[batch[k]:batch[k+1]],)))\n",
    "\n",
    "    print(\"Start\",n_process,\"working threads\")\n",
    "    for p in process:\n",
    "        p.start()\n",
    "\n",
    "    print(\"Wait\",n_process,\"working threads\")\n",
    "    for p in process:\n",
    "        p.join()\n",
    "\n",
    "    print(gc.collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
